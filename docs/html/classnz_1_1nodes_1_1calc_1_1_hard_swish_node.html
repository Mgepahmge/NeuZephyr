<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NeuZephyr: nz::nodes::calc::HardSwishNode Class Reference</title>
<link rel="icon" href="NZ_logo2.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="NZ_logo2.png"/></td>
  <td id="projectalign">
   <div id="projectname">NeuZephyr
   </div>
   <div id="projectbrief">Simple DL Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="inherits.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>nz</b></li><li class="navelem"><a class="el" href="namespacenz_1_1nodes.html">nodes</a></li><li class="navelem"><a class="el" href="namespacenz_1_1nodes_1_1calc.html">calc</a></li><li class="navelem"><a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html">HardSwishNode</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">nz::nodes::calc::HardSwishNode Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Represents a Hard Swish activation function node in a computational graph.  
 <a href="#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for nz::nodes::calc::HardSwishNode:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1nodes_1_1calc_1_1_hard_swish_node__inherit__graph.png" border="0" usemap="#anz_1_1nodes_1_1calc_1_1_hard_swish_node_inherit__map" alt="Inheritance graph"/></div>
<map name="anz_1_1nodes_1_1calc_1_1_hard_swish_node_inherit__map" id="anz_1_1nodes_1_1calc_1_1_hard_swish_node_inherit__map">
<area shape="rect" title="Represents a Hard Swish activation function node in a computational graph." alt="" coords="5,80,219,107"/>
<area shape="rect" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph." alt="" coords="53,5,171,32"/>
<area shape="poly" title=" " alt="" coords="115,48,115,80,109,80,109,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for nz::nodes::calc::HardSwishNode:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1nodes_1_1calc_1_1_hard_swish_node__coll__graph.png" border="0" usemap="#anz_1_1nodes_1_1calc_1_1_hard_swish_node_coll__map" alt="Collaboration graph"/></div>
<map name="anz_1_1nodes_1_1calc_1_1_hard_swish_node_coll__map" id="anz_1_1nodes_1_1calc_1_1_hard_swish_node_coll__map">
<area shape="rect" title="Represents a Hard Swish activation function node in a computational graph." alt="" coords="5,80,219,107"/>
<area shape="rect" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph." alt="" coords="53,5,171,32"/>
<area shape="poly" title=" " alt="" coords="115,48,115,80,109,80,109,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a014a289c458494bf3bf02f266c129205" id="r_a014a289c458494bf3bf02f266c129205"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a014a289c458494bf3bf02f266c129205">HardSwishNode</a> (<a class="el" href="classnz_1_1nodes_1_1_node.html">Node</a> *input, Tensor::value_type alpha=1.0f, Tensor::value_type beta=0.5f)</td></tr>
<tr class="memdesc:a014a289c458494bf3bf02f266c129205"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructor to initialize a <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a></code> for applying the Hard Swish activation function.  <br /></td></tr>
<tr class="separator:a014a289c458494bf3bf02f266c129205"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afd48643b49c76a9c38b0bfc0c4a502f1" id="r_afd48643b49c76a9c38b0bfc0c4a502f1"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#afd48643b49c76a9c38b0bfc0c4a502f1">forward</a> () override</td></tr>
<tr class="memdesc:afd48643b49c76a9c38b0bfc0c4a502f1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Forward pass for the <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a></code> to apply the Hard Swish activation function.  <br /></td></tr>
<tr class="separator:afd48643b49c76a9c38b0bfc0c4a502f1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace2cc92220449c5425a75d8e7f35ff42" id="r_ace2cc92220449c5425a75d8e7f35ff42"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ace2cc92220449c5425a75d8e7f35ff42">backward</a> () override</td></tr>
<tr class="memdesc:ace2cc92220449c5425a75d8e7f35ff42"><td class="mdescLeft">&#160;</td><td class="mdescRight">Backward pass for the <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a></code> to compute gradients.  <br /></td></tr>
<tr class="separator:ace2cc92220449c5425a75d8e7f35ff42"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classnz_1_1nodes_1_1_node"><td colspan="2" onclick="javascript:dynsection.toggleInherit('pub_methods_classnz_1_1nodes_1_1_node')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classnz_1_1nodes_1_1_node.html">nz::nodes::Node</a></td></tr>
<tr class="memitem:a687ee9c34eb61f8f28caa201ca42696e inherit pub_methods_classnz_1_1nodes_1_1_node" id="r_a687ee9c34eb61f8f28caa201ca42696e"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1nodes_1_1_node.html#a687ee9c34eb61f8f28caa201ca42696e">print</a> (std::ostream &amp;os) const</td></tr>
<tr class="memdesc:a687ee9c34eb61f8f28caa201ca42696e inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="mdescLeft">&#160;</td><td class="mdescRight">Prints the type, data, and gradient of the node.  <br /></td></tr>
<tr class="separator:a687ee9c34eb61f8f28caa201ca42696e inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d2c36560f68c28e655ca242da5b2097 inherit pub_methods_classnz_1_1nodes_1_1_node" id="r_a7d2c36560f68c28e655ca242da5b2097"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1nodes_1_1_node.html#a7d2c36560f68c28e655ca242da5b2097">dataInject</a> (const Tensor::value_type *data, bool grad=false) const</td></tr>
<tr class="memdesc:a7d2c36560f68c28e655ca242da5b2097 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="mdescLeft">&#160;</td><td class="mdescRight">Injects data into a relevant tensor object, optionally setting its gradient requirement.  <br /></td></tr>
<tr class="separator:a7d2c36560f68c28e655ca242da5b2097 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a609f1730085dd1d31e0ddcbbae48a065 inherit pub_methods_classnz_1_1nodes_1_1_node" id="r_a609f1730085dd1d31e0ddcbbae48a065"><td class="memTemplParams" colspan="2">template&lt;typename Iterator &gt; </td></tr>
<tr class="memitem:a609f1730085dd1d31e0ddcbbae48a065 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classnz_1_1nodes_1_1_node.html#a609f1730085dd1d31e0ddcbbae48a065">dataInject</a> (Iterator begin, Iterator end, const bool grad=false) const</td></tr>
<tr class="memdesc:a609f1730085dd1d31e0ddcbbae48a065 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="mdescLeft">&#160;</td><td class="mdescRight">Injects data from an iterator range into the output tensor of the InputNode, optionally setting its gradient requirement.  <br /></td></tr>
<tr class="separator:a609f1730085dd1d31e0ddcbbae48a065 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8b4bab3271df92ca1f0914f7a97b1e8 inherit pub_methods_classnz_1_1nodes_1_1_node" id="r_af8b4bab3271df92ca1f0914f7a97b1e8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1nodes_1_1_node.html#af8b4bab3271df92ca1f0914f7a97b1e8">dataInject</a> (const std::initializer_list&lt; Tensor::value_type &gt; &amp;data, bool grad=false) const</td></tr>
<tr class="memdesc:af8b4bab3271df92ca1f0914f7a97b1e8 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="mdescLeft">&#160;</td><td class="mdescRight">Injects data from a std::initializer_list into the output tensor of the <a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a>, optionally setting its gradient requirement.  <br /></td></tr>
<tr class="separator:af8b4bab3271df92ca1f0914f7a97b1e8 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Represents a Hard Swish activation function node in a computational graph. </p>
<p>The <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a></code> class applies the Hard Swish activation function to the input tensor. The Hard Swish function is a computationally efficient approximation of the Swish function and is defined as: </p><div class="fragment"><div class="line"><a class="code hl_function" href="namespacenz_1_1krnl.html#a6c18d8a2b6ddeeeb83cea014829864ad">HardSwish</a>(x) = x * max(0, min(1, alpha * x + beta))</div>
<div class="ttc" id="anamespacenz_1_1krnl_html_a6c18d8a2b6ddeeeb83cea014829864ad"><div class="ttname"><a href="namespacenz_1_1krnl.html#a6c18d8a2b6ddeeeb83cea014829864ad">nz::krnl::HardSwish</a></div><div class="ttdeci">void HardSwish(const dim3 gridDim, const dim3 blockDim, float *out, const float *in, unsigned long long n, float alpha=0.2f, float beta=0.5f)</div><div class="ttdoc">Kernel function to apply the Hard Swish activation function on the GPU.</div><div class="ttdef"><b>Definition</b> <a href="_operation_kernels_8cu_source.html#l00384">OperationKernels.cu:384</a></div></div>
</div><!-- fragment --><p> where <code>alpha</code> and <code>beta</code> control the slope and offset of the linear part of the function.</p>
<p>Key features:</p><ul>
<li><b>Forward Pass</b>: Applies the Hard Swish activation function element-wise to the input tensor, blending the input with a clipped linear function.</li>
<li><b>Backward Pass</b>: Computes the gradient of the loss with respect to the input tensor, handling linear and non-linear regions separately.</li>
<li><b>Shape Preservation</b>: The output tensor has the same shape as the input tensor.</li>
<li><b>Gradient Management</b>: Automatically tracks gradients if required by the input tensor.</li>
</ul>
<p>This class is part of the <code><a class="el" href="namespacenz_1_1nodes.html" title="Contains classes and functionality for nodes in a neural network or computational graph.">nz::nodes</a></code> namespace and is used in models to improve performance while maintaining computational efficiency.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The <code>alpha</code> and <code>beta</code> parameters default to <code>1.0</code> and <code>0.5</code>, respectively, but can be customized during construction.</li>
<li>Efficient GPU computations are performed for both forward and backward passes.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="autotoc_md59"></a>
Usage Example:</h3>
<div class="fragment"><div class="line"><span class="comment">// Example: Using HardSwishNode in a computational graph</span></div>
<div class="line">InputNode input({3, 3}, <span class="keyword">true</span>);  <span class="comment">// Create an input node with shape {3, 3}</span></div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">float</span> data[] = {-1.0f, 0.0f, 1.0f, 2.0f, -2.0f, 3.0f, -3.0f, 4.0f, -4.0f};  <span class="comment">// Sample input values</span></div>
<div class="line">input.output-&gt;dataInject(data);  <span class="comment">// Copy data to the input tensor</span></div>
<div class="line"> </div>
<div class="line"><a class="code hl_function" href="#a014a289c458494bf3bf02f266c129205">HardSwishNode</a> hard_swish_node(&amp;input, 1.0f, 0.5f);  <span class="comment">// Apply Hard Swish activation</span></div>
<div class="line">hard_swish_node.forward();  <span class="comment">// Perform the forward pass</span></div>
<div class="line">hard_swish_node.backward();  <span class="comment">// Propagate gradients in the backward pass</span></div>
<div class="line"> </div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Output: &quot;</span> &lt;&lt; *hard_swish_node.output &lt;&lt; std::endl;  <span class="comment">// Print the result</span></div>
<div class="ttc" id="aclassnz_1_1nodes_1_1calc_1_1_hard_swish_node_html_a014a289c458494bf3bf02f266c129205"><div class="ttname"><a href="#a014a289c458494bf3bf02f266c129205">nz::nodes::calc::HardSwishNode::HardSwishNode</a></div><div class="ttdeci">HardSwishNode(Node *input, Tensor::value_type alpha=1.0f, Tensor::value_type beta=0.5f)</div><div class="ttdoc">Constructor to initialize a HardSwishNode for applying the Hard Swish activation function.</div><div class="ttdef"><b>Definition</b> <a href="_nodes_8cu_source.html#l00437">Nodes.cu:437</a></div></div>
</div><!-- fragment --><dl class="section see"><dt>See also</dt><dd><a class="el" href="#afd48643b49c76a9c38b0bfc0c4a502f1" title="Forward pass for the HardSwishNode to apply the Hard Swish activation function.">forward()</a> for the Hard <a class="el" href="namespacenz_1_1krnl.html#a5d16dbfa0fdad676c5eecd502aa312a3" title="Kernel function to apply the Swish activation function on the GPU.">Swish</a> computation in the <a class="el" href="#afd48643b49c76a9c38b0bfc0c4a502f1" title="Forward pass for the HardSwishNode to apply the Hard Swish activation function.">forward</a> pass. </dd>
<dd>
<a class="el" href="#ace2cc92220449c5425a75d8e7f35ff42" title="Backward pass for the HardSwishNode to compute gradients.">backward()</a> for gradient computation in the <a class="el" href="#ace2cc92220449c5425a75d8e7f35ff42" title="Backward pass for the HardSwishNode to compute gradients.">backward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_nodes_8cuh_source.html#l02954">2954</a> of file <a class="el" href="_nodes_8cuh_source.html">Nodes.cuh</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a014a289c458494bf3bf02f266c129205" name="a014a289c458494bf3bf02f266c129205"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a014a289c458494bf3bf02f266c129205">&#9670;&#160;</a></span>HardSwishNode()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">nz::nodes::calc::HardSwishNode::HardSwishNode </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classnz_1_1nodes_1_1_node.html">Node</a> *</td>          <td class="paramname"><span class="paramname"><em>input</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">1.0f</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>beta</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.5f</span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">explicit</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Constructor to initialize a <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a></code> for applying the Hard Swish activation function. </p>
<p>The constructor initializes a <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a></code>, which applies the Hard Swish activation function to an input tensor. It establishes a connection to the input node, initializes the output tensor, and sets the <code>alpha</code> and <code>beta</code> parameters as well as the node type.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>A pointer to the input node. Its <code>output</code> tensor will have the Hard Swish activation applied. </td></tr>
    <tr><td class="paramname">alpha</td><td>The slope parameter for the Hard Swish function. Controls the steepness of the curve. </td></tr>
    <tr><td class="paramname">beta</td><td>The offset parameter for the Hard Swish function. Shifts the function horizontally.</td></tr>
  </table>
  </dd>
</dl>
<p>The Hard Swish activation function is defined as: </p><div class="fragment"><div class="line"><a class="code hl_function" href="namespacenz_1_1krnl.html#a6c18d8a2b6ddeeeb83cea014829864ad">HardSwish</a>(x) = x * max(0, min(1, alpha * x + beta))</div>
</div><!-- fragment --><p>Key operations performed by the constructor:</p><ul>
<li>Adds the input node to the <code>inputs</code> vector, establishing the connection in the computational graph.</li>
<li>Determines if gradient tracking is required based on the input tensor's <code>requiresGrad</code> property.</li>
<li>Initializes the <code>output</code> tensor with the same shape as the input tensor and appropriate gradient tracking.</li>
<li>Sets the <code>alpha</code> and <code>beta</code> parameters, which control the shape of the Hard Swish function.</li>
<li>Sets the node type to "HardSwish" for identification in the computational graph.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The Hard Swish function is a smooth approximation of the ReLU activation, combining properties of ReLU and Swish activations.</li>
<li>The <code>alpha</code> and <code>beta</code> parameters allow for customization of the activation function's behavior.</li>
<li>Gradient tracking for the output tensor is automatically set based on the input tensor's requirements.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#afd48643b49c76a9c38b0bfc0c4a502f1" title="Forward pass for the HardSwishNode to apply the Hard Swish activation function.">forward()</a> for the implementation of the <a class="el" href="#afd48643b49c76a9c38b0bfc0c4a502f1" title="Forward pass for the HardSwishNode to apply the Hard Swish activation function.">forward</a> pass using these parameters. </dd>
<dd>
<a class="el" href="#ace2cc92220449c5425a75d8e7f35ff42" title="Backward pass for the HardSwishNode to compute gradients.">backward()</a> for the gradient computation in the <a class="el" href="#ace2cc92220449c5425a75d8e7f35ff42" title="Backward pass for the HardSwishNode to compute gradients.">backward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00437">437</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="ace2cc92220449c5425a75d8e7f35ff42" name="ace2cc92220449c5425a75d8e7f35ff42"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ace2cc92220449c5425a75d8e7f35ff42">&#9670;&#160;</a></span>backward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nz::nodes::calc::HardSwishNode::backward </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Backward pass for the <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a></code> to compute gradients. </p>
<p>This method implements the backward pass of the Hard Swish activation function. It computes the gradient of the loss with respect to the input by applying the derivative of the Hard Swish function to the incoming gradient.</p>
<p>The derivative of the Hard Swish function is: </p><div class="fragment"><div class="line"><a class="code hl_function" href="namespacenz_1_1krnl.html#a6c18d8a2b6ddeeeb83cea014829864ad">HardSwish</a><span class="stringliteral">&#39;(x) = (2 * alpha * x + beta) * min(max(alpha * x + beta, 0), 1) +</span></div>
<div class="line"><span class="stringliteral">                max(0, min(1, alpha * x + beta)) +</span></div>
<div class="line"><span class="stringliteral">                x * (alpha * (x &gt; -beta/alpha) * (x &lt; (1-beta)/alpha))</span></div>
</div><!-- fragment --><p> where <code>alpha</code> and <code>beta</code> are the parameters that control the shape of the function.</p>
<p>Key operations:</p><ul>
<li>Checks if the input tensor requires gradient computation.</li>
<li>If gradients are required:<ul>
<li>Configures CUDA execution parameters (grid and block dimensions) for parallel processing.</li>
<li>Launches a CUDA kernel (<code>HardSwishBackward</code>) to compute gradients on the GPU.</li>
<li>Processes all elements of the input tensor in parallel.</li>
</ul>
</li>
</ul>
<p>CUDA kernel configuration:</p><ul>
<li>Block size: 256 threads per block.</li>
<li>Grid size: Calculated to ensure coverage of all elements in the input tensor.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This method is only executed if the input tensor requires gradient computation.</li>
<li>The method assumes that the CUDA kernel <code>HardSwishBackward</code> is defined elsewhere and correctly implements the derivative of the Hard Swish function.</li>
<li>The gradient computation leverages GPU parallelism for efficiency, especially for large tensors.</li>
<li>The computed gradients are accumulated in the input tensor's gradient buffer.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#afd48643b49c76a9c38b0bfc0c4a502f1" title="Forward pass for the HardSwishNode to apply the Hard Swish activation function.">forward()</a> for the corresponding <a class="el" href="#afd48643b49c76a9c38b0bfc0c4a502f1" title="Forward pass for the HardSwishNode to apply the Hard Swish activation function.">forward</a> pass implementation. </dd>
<dd>
<a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a> constructor for the initialization of <code>alpha</code> and <code>beta</code> parameters.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/5 </dd></dl>

<p>Implements <a class="el" href="classnz_1_1nodes_1_1_node.html#a0a9ecbaa3d790ba38e8218aca7837fd0">nz::nodes::Node</a>.</p>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00452">452</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1nodes_1_1calc_1_1_hard_swish_node_ace2cc92220449c5425a75d8e7f35ff42_cgraph.png" border="0" usemap="#aclassnz_1_1nodes_1_1calc_1_1_hard_swish_node_ace2cc92220449c5425a75d8e7f35ff42_cgraph" alt=""/></div>
<map name="aclassnz_1_1nodes_1_1calc_1_1_hard_swish_node_ace2cc92220449c5425a75d8e7f35ff42_cgraph" id="aclassnz_1_1nodes_1_1calc_1_1_hard_swish_node_ace2cc92220449c5425a75d8e7f35ff42_cgraph">
<area shape="rect" title="Backward pass for the HardSwishNode to compute gradients." alt="" coords="5,5,188,48"/>
<area shape="rect" href="namespacenz_1_1krnl.html#a293918c6c59bd44f5f0c94b5791202f7" title="Kernel function to compute the gradient of the Hard Swish activation during backpropagation." alt="" coords="236,13,430,40"/>
<area shape="poly" title=" " alt="" coords="188,24,220,24,220,29,188,29"/>
</map>
</div>

</div>
</div>
<a id="afd48643b49c76a9c38b0bfc0c4a502f1" name="afd48643b49c76a9c38b0bfc0c4a502f1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#afd48643b49c76a9c38b0bfc0c4a502f1">&#9670;&#160;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nz::nodes::calc::HardSwishNode::forward </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Forward pass for the <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a></code> to apply the Hard Swish activation function. </p>
<p>This method implements the forward pass of the Hard Swish activation function. It applies the Hard Swish operation element-wise to the input tensor and stores the result in the output tensor.</p>
<p>The Hard Swish function is defined as: </p><div class="fragment"><div class="line"><a class="code hl_function" href="namespacenz_1_1krnl.html#a6c18d8a2b6ddeeeb83cea014829864ad">HardSwish</a>(x) = x * max(0, min(1, alpha * x + beta))</div>
</div><!-- fragment --><p> where <code>alpha</code> and <code>beta</code> are parameters that control the shape of the function.</p>
<p>Key operations:</p><ul>
<li>Configures CUDA execution parameters (grid and block dimensions) for parallel processing.</li>
<li>Launches a CUDA kernel (<code>HardSwish</code>) to perform the Hard Swish computation on the GPU.</li>
<li>Processes all elements of the input tensor in parallel.</li>
</ul>
<p>CUDA kernel configuration:</p><ul>
<li>Block size: 256 threads per block.</li>
<li>Grid size: Calculated to ensure coverage of all elements in the output tensor.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This method assumes that the CUDA kernel <code>HardSwish</code> is defined elsewhere and properly implements the Hard Swish function.</li>
<li>The output tensor is assumed to have the same shape as the input tensor.</li>
<li>This implementation leverages GPU parallelism for efficient computation, especially for large tensors.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#ace2cc92220449c5425a75d8e7f35ff42" title="Backward pass for the HardSwishNode to compute gradients.">backward()</a> for the corresponding <a class="el" href="#ace2cc92220449c5425a75d8e7f35ff42" title="Backward pass for the HardSwishNode to compute gradients.">backward</a> pass implementation. </dd>
<dd>
<a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph.">HardSwishNode</a> constructor for the initialization of <code>alpha</code> and <code>beta</code> parameters.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/5 </dd></dl>

<p>Implements <a class="el" href="classnz_1_1nodes_1_1_node.html#a8a828c2e91a4aa2a9ab7b94554e4685b">nz::nodes::Node</a>.</p>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00446">446</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1nodes_1_1calc_1_1_hard_swish_node_afd48643b49c76a9c38b0bfc0c4a502f1_cgraph.png" border="0" usemap="#aclassnz_1_1nodes_1_1calc_1_1_hard_swish_node_afd48643b49c76a9c38b0bfc0c4a502f1_cgraph" alt=""/></div>
<map name="aclassnz_1_1nodes_1_1calc_1_1_hard_swish_node_afd48643b49c76a9c38b0bfc0c4a502f1_cgraph" id="aclassnz_1_1nodes_1_1calc_1_1_hard_swish_node_afd48643b49c76a9c38b0bfc0c4a502f1_cgraph">
<area shape="rect" title="Forward pass for the HardSwishNode to apply the Hard Swish activation function." alt="" coords="5,5,188,48"/>
<area shape="rect" href="namespacenz_1_1krnl.html#a6c18d8a2b6ddeeeb83cea014829864ad" title="Kernel function to apply the Hard Swish activation function on the GPU." alt="" coords="236,13,372,40"/>
<area shape="poly" title=" " alt="" coords="188,24,220,24,220,29,188,29"/>
</map>
</div>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/include/NeuZephyr/<a class="el" href="_nodes_8cuh_source.html">Nodes.cuh</a></li>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/src/<a class="el" href="_nodes_8cu_source.html">Nodes.cu</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
