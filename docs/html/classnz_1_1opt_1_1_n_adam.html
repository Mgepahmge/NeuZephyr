<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NeuZephyr: nz::opt::NAdam Class Reference</title>
<link rel="icon" href="NZ_logo2.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="NZ_logo2.png"/></td>
  <td id="projectalign">
   <div id="projectname">NeuZephyr
   </div>
   <div id="projectbrief">Simple DL Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="inherits.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>nz</b></li><li class="navelem"><a class="el" href="namespacenz_1_1opt.html">opt</a></li><li class="navelem"><a class="el" href="classnz_1_1opt_1_1_n_adam.html">NAdam</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classnz_1_1opt_1_1_n_adam-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">nz::opt::NAdam Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p><a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> optimizer for deep learning models.  
 <a href="#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for nz::opt::NAdam:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1opt_1_1_n_adam__inherit__graph.png" border="0" usemap="#anz_1_1opt_1_1_n_adam_inherit__map" alt="Inheritance graph"/></div>
<map name="anz_1_1opt_1_1_n_adam_inherit__map" id="anz_1_1opt_1_1_n_adam_inherit__map">
<area shape="rect" title="NAdam optimizer for deep learning models." alt="" coords="11,80,125,107"/>
<area shape="rect" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning." alt="" coords="5,5,131,32"/>
<area shape="poly" title=" " alt="" coords="71,48,71,80,65,80,65,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for nz::opt::NAdam:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1opt_1_1_n_adam__coll__graph.png" border="0" usemap="#anz_1_1opt_1_1_n_adam_coll__map" alt="Collaboration graph"/></div>
<map name="anz_1_1opt_1_1_n_adam_coll__map" id="anz_1_1opt_1_1_n_adam_coll__map">
<area shape="rect" title="NAdam optimizer for deep learning models." alt="" coords="11,80,125,107"/>
<area shape="rect" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning." alt="" coords="5,5,131,32"/>
<area shape="poly" title=" " alt="" coords="71,48,71,80,65,80,65,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a2a13f32181ab30524b0fdf4ca555805f" id="r_a2a13f32181ab30524b0fdf4ca555805f"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2a13f32181ab30524b0fdf4ca555805f">NAdam</a> (Tensor::value_type learning_rate, Tensor::value_type beta1, Tensor::value_type beta2)</td></tr>
<tr class="memdesc:a2a13f32181ab30524b0fdf4ca555805f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructs a <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> optimizer with specified hyperparameters.  <br /></td></tr>
<tr class="separator:a2a13f32181ab30524b0fdf4ca555805f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:add5c94bdc1b012f035b51339f92e7a49" id="r_add5c94bdc1b012f035b51339f92e7a49"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#add5c94bdc1b012f035b51339f92e7a49">step</a> (<a class="el" href="classnz_1_1nodes_1_1_node.html">Node</a> *input) override</td></tr>
<tr class="memdesc:add5c94bdc1b012f035b51339f92e7a49"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs a single optimization step using the <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> algorithm.  <br /></td></tr>
<tr class="separator:add5c94bdc1b012f035b51339f92e7a49"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classnz_1_1opt_1_1_optimizer"><td colspan="2" onclick="javascript:dynsection.toggleInherit('pub_methods_classnz_1_1opt_1_1_optimizer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classnz_1_1opt_1_1_optimizer.html">nz::opt::Optimizer</a></td></tr>
<tr class="memitem:aaf8d92566a815254dbb0ace9af9cb1ae inherit pub_methods_classnz_1_1opt_1_1_optimizer" id="r_aaf8d92566a815254dbb0ace9af9cb1ae"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1opt_1_1_optimizer.html#aaf8d92566a815254dbb0ace9af9cb1ae">Optimizer</a> ()=default</td></tr>
<tr class="memdesc:aaf8d92566a815254dbb0ace9af9cb1ae inherit pub_methods_classnz_1_1opt_1_1_optimizer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Default constructor for the <a class="el" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning.">Optimizer</a> class.  <br /></td></tr>
<tr class="separator:aaf8d92566a815254dbb0ace9af9cb1ae inherit pub_methods_classnz_1_1opt_1_1_optimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9262983ef3bd11e6f548862b2f58e1d inherit pub_methods_classnz_1_1opt_1_1_optimizer" id="r_ab9262983ef3bd11e6f548862b2f58e1d"><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1opt_1_1_optimizer.html#ab9262983ef3bd11e6f548862b2f58e1d">~Optimizer</a> ()=default</td></tr>
<tr class="memdesc:ab9262983ef3bd11e6f548862b2f58e1d inherit pub_methods_classnz_1_1opt_1_1_optimizer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Default destructor for the <a class="el" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning.">Optimizer</a> class.  <br /></td></tr>
<tr class="separator:ab9262983ef3bd11e6f548862b2f58e1d inherit pub_methods_classnz_1_1opt_1_1_optimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p><a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> optimizer for deep learning models. </p>
<p>The <code><a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a></code> class implements the Nesterov-accelerated Adaptive Moment Estimation (<a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a>) optimization algorithm, which combines the benefits of the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimizer with Nesterov momentum. <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> improves upon <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> by incorporating Nesterov momentum into the first moment estimation, which can lead to faster convergence in some scenarios.</p>
<p>This class extends the <code><a class="el" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning.">Optimizer</a></code> base class and provides a concrete implementation of the <code>step</code> method, which updates the model's parameters (represented as <code>Node</code> objects) using the <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> algorithm.</p>
<ul>
<li>The optimizer maintains three tensors for each parameter (<code>Node</code>):<ul>
<li>( m_t ): The first moment estimate, which is the exponentially decaying average of past gradients.</li>
<li>( m_t' ): The modified first moment estimate, incorporating Nesterov momentum.</li>
<li>( v_t ): The second moment estimate, which is the exponentially decaying average of past squared gradients.</li>
</ul>
</li>
<li>The moment estimates are updated using the following formulas: [ m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t ] [ m_t' = \beta_1 m_t + (1 - \beta_1) g_t ] [ v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 ] where ( g_t ) is the current gradient, ( \beta_1 ) and ( \beta_2 ) are the decay rates for the first and second moments.</li>
<li>The model parameters are then updated using the bias-corrected moment estimates: [ \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{m}_t' = \frac{m_t'}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t} ] [ \theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t'}{\sqrt{\hat{v}_t} + \epsilon} ] where ( \eta ) is the learning rate, ( \epsilon ) is a small constant to prevent division by zero.</li>
<li>The optimizer uses GPU-accelerated computations through CUDA to efficiently update parameters, making it suitable for large-scale models.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The optimizer assumes that the model parameters are represented by <code>Node</code> objects, and each node must have associated gradients.</li>
<li>The first moment estimate (<code>m</code>), modified first moment estimate (<code>m_modified</code>), and second moment estimate (<code>v</code>) are stored per <code>Node</code> object. If a <code>Node</code> does not have existing moments, they are initialized to zero tensors.</li>
<li>The optimizer utilizes GPU memory for moment storage and gradient computation, requiring CUDA support.</li>
<li>Ensure that the model parameters have been properly initialized, and gradients are computed before calling this method.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="autotoc_md61"></a>
Usage Example:</h3>
<div class="fragment"><div class="line"><a class="code hl_class" href="classnz_1_1opt_1_1_n_adam.html">NAdam</a> optimizer(0.001, 0.9, 0.999);</div>
<div class="line">graph.update(&amp;optimizer); <span class="comment">// Suppose &quot;graph&quot; is a computation graph waiting for gradient updates.</span></div>
<div class="ttc" id="aclassnz_1_1opt_1_1_n_adam_html"><div class="ttname"><a href="classnz_1_1opt_1_1_n_adam.html">nz::opt::NAdam</a></div><div class="ttdoc">NAdam optimizer for deep learning models.</div><div class="ttdef"><b>Definition</b> <a href="_optimizer_8cuh_source.html#l00844">Optimizer.cuh:844</a></div></div>
</div><!-- fragment --><dl class="section see"><dt>See also</dt><dd><a class="el" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning.">Optimizer</a> for the base class that defines the interface for all optimizers. </dd>
<dd>
Nodes::Node for the class representing model parameters.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/07 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_optimizer_8cuh_source.html#l00844">844</a> of file <a class="el" href="_optimizer_8cuh_source.html">Optimizer.cuh</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a2a13f32181ab30524b0fdf4ca555805f" name="a2a13f32181ab30524b0fdf4ca555805f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2a13f32181ab30524b0fdf4ca555805f">&#9670;&#160;</a></span>NAdam()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">nz::opt::NAdam::NAdam </td>
          <td>(</td>
          <td class="paramtype">Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>learning_rate</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>beta1</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>beta2</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">explicit</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Constructs a <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> optimizer with specified hyperparameters. </p>
<p>Initializes the <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> (Nesterov-accelerated Adaptive Moment Estimation) optimizer with user-defined learning rate and momentum parameters. <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> combines the benefits of Nesterov accelerated gradient and <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimization techniques, providing adaptive learning rates for each parameter while incorporating momentum.</p>
<p>The constructor sets up the initial state of the optimizer, including the learning rate, exponential decay rates for moment estimates, and initializes the iteration counter to zero. This prepares the optimizer for the first optimization step in the training process.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">learning_rate</td><td>The base learning rate that controls the step size during optimization. A smaller value leads to more conservative updates, while a larger value allows for more aggressive parameter adjustments.</td></tr>
    <tr><td class="paramname">beta1</td><td>The exponential decay rate for the first moment estimate (moving average of gradients). Typically set close to 1 (e.g., 0.9) to control the influence of past gradients on the current update.</td></tr>
    <tr><td class="paramname">beta2</td><td>The exponential decay rate for the second moment estimate (moving average of squared gradients). Typically set close to 1 (e.g., 0.999) to adapt the learning rate for each parameter based on its historical gradient information.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The iteration counter <code>it</code> is initialized to 0, which is critical for the first bias correction step in the <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> algorithm.</li>
<li>Recommended default values are learning_rate = 0.001, beta1 = 0.9, beta2 = 0.999.</li>
<li>The hyperparameters significantly impact the optimization process and may require tuning based on the specific machine learning task.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a>, <a class="el" href="classnz_1_1opt_1_1_r_m_sprop.html" title="RMSprop optimizer for deep learning models.">RMSprop</a> Optimization algorithms with similar adaptive learning rate strategies</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/07 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_optimizer_8cu_source.html#l00101">101</a> of file <a class="el" href="_optimizer_8cu_source.html">Optimizer.cu</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="add5c94bdc1b012f035b51339f92e7a49" name="add5c94bdc1b012f035b51339f92e7a49"></a>
<h2 class="memtitle"><span class="permalink"><a href="#add5c94bdc1b012f035b51339f92e7a49">&#9670;&#160;</a></span>step()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nz::opt::NAdam::step </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classnz_1_1nodes_1_1_node.html">Node</a> *</td>          <td class="paramname"><span class="paramname"><em>input</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Performs a single optimization step using the <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> algorithm. </p>
<p>This method updates the model parameters for a given input node using the Nesterov-accelerated Adaptive Moment Estimation (<a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a>) optimization algorithm. It manages the adaptive learning rates and momentum for individual parameters by maintaining and updating first and second moment estimates.</p>
<p>The method performs several key operations:</p><ol type="1">
<li>Increments the iteration counter</li>
<li>Initializes moment and modified moment tensors if they don't exist for the input node</li>
<li>Prepares CUDA grid and block configurations for parallel parameter updates</li>
<li>Invokes a CUDA kernel to apply the <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> update rule</li>
</ol>
<p>The initialization of moment tensors ensures that each parameter has its own adaptive learning rate and momentum, allowing for more flexible and efficient optimization across different model parameters.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>A pointer to the <code>Node</code> object representing the model parameter to be updated. The node must have a valid output tensor and its gradient already computed.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This method assumes the input node has a valid gradient stored in its output object.</li>
<li>Moment tensors are created lazily (on-demand) for each unique input node.</li>
<li>The method uses CUDA for parallel computation of parameter updates.</li>
<li>The iteration counter is crucial for bias correction in the <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> algorithm.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#a2a13f32181ab30524b0fdf4ca555805f" title="Constructs a NAdam optimizer with specified hyperparameters.">NAdam::NAdam()</a> Constructor for initializing optimizer parameters </dd>
<dd>
krnl::NAdam CUDA kernel implementing the <a class="el" href="classnz_1_1opt_1_1_n_adam.html" title="NAdam optimizer for deep learning models.">NAdam</a> update rule</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/07 </dd></dl>

<p>Implements <a class="el" href="classnz_1_1opt_1_1_optimizer.html#a826381abaaf29dbebade7cfd38b266e4">nz::opt::Optimizer</a>.</p>

<p class="definition">Definition at line <a class="el" href="_optimizer_8cu_source.html#l00108">108</a> of file <a class="el" href="_optimizer_8cu_source.html">Optimizer.cu</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1opt_1_1_n_adam_add5c94bdc1b012f035b51339f92e7a49_cgraph.png" border="0" usemap="#aclassnz_1_1opt_1_1_n_adam_add5c94bdc1b012f035b51339f92e7a49_cgraph" alt=""/></div>
<map name="aclassnz_1_1opt_1_1_n_adam_add5c94bdc1b012f035b51339f92e7a49_cgraph" id="aclassnz_1_1opt_1_1_n_adam_add5c94bdc1b012f035b51339f92e7a49_cgraph">
<area shape="rect" title="Performs a single optimization step using the NAdam algorithm." alt="" coords="5,5,152,32"/>
<area shape="rect" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e" title="Fills the tensor&#39;s data with a specified value." alt="" coords="200,5,336,32"/>
<area shape="poly" title=" " alt="" coords="152,16,184,16,184,21,152,21"/>
<area shape="rect" href="classnz_1_1data_1_1_tensor.html#a38ba233ef49f34620297f96edd962c55" title="Retrieves a pointer to the tensor&#39;s data stored in GPU memory." alt="" coords="384,5,532,32"/>
<area shape="poly" title=" " alt="" coords="336,16,369,16,369,21,336,21"/>
</map>
</div>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/include/NeuZephyr/<a class="el" href="_optimizer_8cuh_source.html">Optimizer.cuh</a></li>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/src/<a class="el" href="_optimizer_8cu_source.html">Optimizer.cu</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
