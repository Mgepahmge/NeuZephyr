<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NeuZephyr: nz::data Namespace Reference</title>
<link rel="icon" href="NZ_logo2.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="NZ_logo2.png"/></td>
  <td id="projectalign">
   <div id="projectname">NeuZephyr
   </div>
   <div id="projectbrief">Simple DL Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li class="current"><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="namespaces.html"><span>Namespace&#160;List</span></a></li>
      <li><a href="namespacemembers.html"><span>Namespace&#160;Members</span></a></li>
    </ul>
  </div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>nz</b></li><li class="navelem"><a class="el" href="namespacenz_1_1data.html">data</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">nz::data Namespace Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Contains data structures and utilities for tensor operations in machine learning workflows.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible container-like interfaces.  <a href="classnz_1_1data_1_1_mapped_tensor.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.  <a href="classnz_1_1data_1_1_tensor.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:af967fb10a908c374d8378ac7ef22779c" id="r_af967fb10a908c374d8378ac7ef22779c"><td class="memItemLeft" align="right" valign="top">std::ostream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af967fb10a908c374d8378ac7ef22779c">operator&lt;&lt;</a> (std::ostream &amp;os, const <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:af967fb10a908c374d8378ac7ef22779c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overload the &lt;&lt; operator to print a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object to an output stream.  <br /></td></tr>
<tr class="separator:af967fb10a908c374d8378ac7ef22779c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4ea5e60f987ab3853b4d0af44453a9e2" id="r_a4ea5e60f987ab3853b4d0af44453a9e2"><td class="memItemLeft" align="right" valign="top">std::istream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a4ea5e60f987ab3853b4d0af44453a9e2">operator&gt;&gt;</a> (std::istream &amp;is, <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a4ea5e60f987ab3853b4d0af44453a9e2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overload the &gt;&gt; operator to read data from an input stream into a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object.  <br /></td></tr>
<tr class="separator:a4ea5e60f987ab3853b4d0af44453a9e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2907370af84a6c5bdc4b72803c9edc68" id="r_a2907370af84a6c5bdc4b72803c9edc68"><td class="memItemLeft" align="right" valign="top">std::ostream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2907370af84a6c5bdc4b72803c9edc68">operator&lt;&lt;</a> (std::ostream &amp;os, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a2907370af84a6c5bdc4b72803c9edc68"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overloads the <code>&lt;&lt;</code> operator to print the tensor's data to an output stream.  <br /></td></tr>
<tr class="separator:a2907370af84a6c5bdc4b72803c9edc68"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a40134aba93013e1b0d43c6fd5158d400" id="r_a40134aba93013e1b0d43c6fd5158d400"><td class="memItemLeft" align="right" valign="top">std::istream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a40134aba93013e1b0d43c6fd5158d400">operator&gt;&gt;</a> (std::istream &amp;is, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a40134aba93013e1b0d43c6fd5158d400"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overloads the <code>&gt;&gt;</code> operator to read a tensor's data from an input stream.  <br /></td></tr>
<tr class="separator:a40134aba93013e1b0d43c6fd5158d400"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac65cd4ed45d91da297a90d4381411064" id="r_ac65cd4ed45d91da297a90d4381411064"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ac65cd4ed45d91da297a90d4381411064">operator*</a> (const Tensor::value_type lhs, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;rhs)</td></tr>
<tr class="memdesc:ac65cd4ed45d91da297a90d4381411064"><td class="mdescLeft">&#160;</td><td class="mdescRight">Multiplies a tensor by a scalar (element-wise multiplication).  <br /></td></tr>
<tr class="separator:ac65cd4ed45d91da297a90d4381411064"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab7386d5f0c567a1bed5a78eab4bf2d6b" id="r_ab7386d5f0c567a1bed5a78eab4bf2d6b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab7386d5f0c567a1bed5a78eab4bf2d6b">operator*</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:ab7386d5f0c567a1bed5a78eab4bf2d6b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Multiplies a tensor by a scalar (element-wise multiplication).  <br /></td></tr>
<tr class="separator:ab7386d5f0c567a1bed5a78eab4bf2d6b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3b044ce5a34b4724830f073efb954743" id="r_a3b044ce5a34b4724830f073efb954743"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a3b044ce5a34b4724830f073efb954743">operator/</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:a3b044ce5a34b4724830f073efb954743"><td class="mdescLeft">&#160;</td><td class="mdescRight">Divides a tensor by a scalar (element-wise division).  <br /></td></tr>
<tr class="separator:a3b044ce5a34b4724830f073efb954743"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5c3243e28d8132d9514c8d37d597a7f" id="r_ab5c3243e28d8132d9514c8d37d597a7f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab5c3243e28d8132d9514c8d37d597a7f">operator+</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:ab5c3243e28d8132d9514c8d37d597a7f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a scalar to a tensor (element-wise addition).  <br /></td></tr>
<tr class="separator:ab5c3243e28d8132d9514c8d37d597a7f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0f75427702f8732bc1833b272baa7f6" id="r_aa0f75427702f8732bc1833b272baa7f6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aa0f75427702f8732bc1833b272baa7f6">operator+</a> (const Tensor::value_type lhs, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;rhs)</td></tr>
<tr class="memdesc:aa0f75427702f8732bc1833b272baa7f6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a scalar to a tensor (element-wise addition).  <br /></td></tr>
<tr class="separator:aa0f75427702f8732bc1833b272baa7f6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a402c3fdfb410d9351a43087834982bbe" id="r_a402c3fdfb410d9351a43087834982bbe"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a402c3fdfb410d9351a43087834982bbe">operator-</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:a402c3fdfb410d9351a43087834982bbe"><td class="mdescLeft">&#160;</td><td class="mdescRight">Subtracts a scalar from a tensor (element-wise subtraction).  <br /></td></tr>
<tr class="separator:a402c3fdfb410d9351a43087834982bbe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a00d2b2ae594fa907461c12045de8f470" id="r_a00d2b2ae594fa907461c12045de8f470"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a00d2b2ae594fa907461c12045de8f470">operator-</a> (const Tensor::value_type lhs, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;rhs)</td></tr>
<tr class="memdesc:a00d2b2ae594fa907461c12045de8f470"><td class="mdescLeft">&#160;</td><td class="mdescRight">Subtracts a tensor from a scalar (element-wise subtraction).  <br /></td></tr>
<tr class="separator:a00d2b2ae594fa907461c12045de8f470"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2cd35db5c2bc0fc68cb0461d2f600089" id="r_a2cd35db5c2bc0fc68cb0461d2f600089"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2cd35db5c2bc0fc68cb0461d2f600089">ReLU</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a2cd35db5c2bc0fc68cb0461d2f600089"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Rectified Linear Unit (ReLU) activation function to a tensor.  <br /></td></tr>
<tr class="separator:a2cd35db5c2bc0fc68cb0461d2f600089"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a370bede45854c04c8222e9971cedc075" id="r_a370bede45854c04c8222e9971cedc075"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a370bede45854c04c8222e9971cedc075">Sigmoid</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a370bede45854c04c8222e9971cedc075"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Sigmoid activation function to a tensor.  <br /></td></tr>
<tr class="separator:a370bede45854c04c8222e9971cedc075"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2037c2f13444847ea57d7110d8c48a51" id="r_a2037c2f13444847ea57d7110d8c48a51"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2037c2f13444847ea57d7110d8c48a51">Tanh</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a2037c2f13444847ea57d7110d8c48a51"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Tanh (hyperbolic tangent) activation function to a tensor.  <br /></td></tr>
<tr class="separator:a2037c2f13444847ea57d7110d8c48a51"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae638e4a70745936b4542263a5fef9b9d" id="r_ae638e4a70745936b4542263a5fef9b9d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ae638e4a70745936b4542263a5fef9b9d">LeakyReLU</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor, float alpha=0.01f)</td></tr>
<tr class="memdesc:ae638e4a70745936b4542263a5fef9b9d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Tanh (hyperbolic tangent) activation function to a tensor.  <br /></td></tr>
<tr class="separator:ae638e4a70745936b4542263a5fef9b9d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a271473f387bde9059ad5afa03851184a" id="r_a271473f387bde9059ad5afa03851184a"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a271473f387bde9059ad5afa03851184a">Swish</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a271473f387bde9059ad5afa03851184a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Swish activation function to a tensor.  <br /></td></tr>
<tr class="separator:a271473f387bde9059ad5afa03851184a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a463a6acd4af10f53a8f57e30fba2e660" id="r_a463a6acd4af10f53a8f57e30fba2e660"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a463a6acd4af10f53a8f57e30fba2e660">ELU</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor, float alpha=1.0f)</td></tr>
<tr class="memdesc:a463a6acd4af10f53a8f57e30fba2e660"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Exponential Linear Unit (ELU) activation function to a tensor.  <br /></td></tr>
<tr class="separator:a463a6acd4af10f53a8f57e30fba2e660"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9821ac15d218a07161a211b04db5528" id="r_ab9821ac15d218a07161a211b04db5528"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab9821ac15d218a07161a211b04db5528">HardSigmoid</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor, float alpha=0.2f, float beta=0.5f)</td></tr>
<tr class="memdesc:ab9821ac15d218a07161a211b04db5528"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Hard Sigmoid activation function to a tensor.  <br /></td></tr>
<tr class="separator:ab9821ac15d218a07161a211b04db5528"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a68859b30e6833b954f27f89eb8592235" id="r_a68859b30e6833b954f27f89eb8592235"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a68859b30e6833b954f27f89eb8592235">HardSwish</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor, float alpha=0.2f, float beta=0.5f)</td></tr>
<tr class="memdesc:a68859b30e6833b954f27f89eb8592235"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Hard Swish activation function to a tensor.  <br /></td></tr>
<tr class="separator:a68859b30e6833b954f27f89eb8592235"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a09849b685c23664bf02b0f824006f695" id="r_a09849b685c23664bf02b0f824006f695"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a09849b685c23664bf02b0f824006f695">Softmax</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a09849b685c23664bf02b0f824006f695"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Softmax activation function to a tensor.  <br /></td></tr>
<tr class="separator:a09849b685c23664bf02b0f824006f695"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Contains data structures and utilities for tensor operations in machine learning workflows. </p>
<p>The <code><a class="el" href="namespacenz_1_1data.html" title="Contains data structures and utilities for tensor operations in machine learning workflows.">nz::data</a></code> namespace provides foundational classes and functions for managing and manipulating tensors in GPU-based computations. It is designed for use in deep learning frameworks and other numerical computing applications.</p>
<p>Key components within this namespace include:</p><ul>
<li><b><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></b>: A class representing multidimensional arrays (tensors) stored in GPU memory.</li>
<li><b>Utilities</b>: Functions and operators for performing mathematical operations, memory management, and activation functions.</li>
</ul>
<p>The namespace is intended to encapsulate all tensor-related functionality to ensure modularity and maintainability in the larger nz project.</p>
<dl class="section note"><dt>Note</dt><dd>The components in this namespace rely on CUDA for GPU-based operations. Ensure that CUDA-compatible hardware and software are properly configured.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge(<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/11/29 </dd></dl>
</div><h2 class="groupheader">Function Documentation</h2>
<a id="a463a6acd4af10f53a8f57e30fba2e660" name="a463a6acd4af10f53a8f57e30fba2e660"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a463a6acd4af10f53a8f57e30fba2e660">&#9670;&#160;</a></span>ELU()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::ELU </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">1.0f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Exponential Linear Unit (ELU) activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the ELU activation function element-wise to the given tensor. ELU is a smooth, differentiable activation function defined as: </p><pre class="fragment">ELU(x) = x, if x &gt; 0
ELU(x) = alpha * (exp(x) - 1), if x &lt;= 0
</pre><p>where <code>alpha</code> is a hyperparameter that controls the slope of the negative part of the function. ELU has been shown to perform better than ReLU in certain situations, especially in deep networks.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the ELU activation function will be applied. </td></tr>
    <tr><td class="paramname">alpha</td><td>A parameter that controls the value for negative inputs. It determines the slope of the negative part. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the ELU activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>ExponentialLinearUnit</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise ELU activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(-1.0f);  <span class="comment">// Fill the tensor with negative values</span></div>
<div class="line"><span class="keywordtype">float</span> alpha = 1.0f;</div>
<div class="line">Tensor result = <a class="code hl_function" href="#a463a6acd4af10f53a8f57e30fba2e660">ELU</a>(tensor, alpha);  <span class="comment">// Apply the ELU activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="aclassnz_1_1data_1_1_tensor_html"><div class="ttname"><a href="classnz_1_1data_1_1_tensor.html">nz::data::Tensor</a></div><div class="ttdoc">A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cuh_source.html#l00133">Tensor.cuh:133</a></div></div>
<div class="ttc" id="aclassnz_1_1data_1_1_tensor_html_aaba025895b2c8b69de8d36c73c74929e"><div class="ttname"><a href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">nz::data::Tensor::fill</a></div><div class="ttdeci">void fill(value_type value) const</div><div class="ttdoc">Fills the tensor's data with a specified value.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00935">Tensor.cu:935</a></div></div>
<div class="ttc" id="anamespacenz_1_1data_html_a463a6acd4af10f53a8f57e30fba2e660"><div class="ttname"><a href="#a463a6acd4af10f53a8f57e30fba2e660">nz::data::ELU</a></div><div class="ttdeci">Tensor ELU(const Tensor &amp;tensor, float alpha)</div><div class="ttdoc">Applies the Exponential Linear Unit (ELU) activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00594">Tensor.cu:594</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00594">594</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ab9821ac15d218a07161a211b04db5528" name="ab9821ac15d218a07161a211b04db5528"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab9821ac15d218a07161a211b04db5528">&#9670;&#160;</a></span>HardSigmoid()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::HardSigmoid </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.2f</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.5f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Hard Sigmoid activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Hard Sigmoid activation function element-wise to the given tensor. Hard Sigmoid is a piecewise linear approximation of the Sigmoid function, defined as: </p><pre class="fragment">HardSigmoid(x) = alpha * x + beta, if x is between -2.5 and 2.5
HardSigmoid(x) = 0, if x &lt; -2.5
HardSigmoid(x) = 1, if x &gt; 2.5
</pre><p>where <code>alpha</code> and <code>beta</code> are parameters that control the slope and the shift of the function. Hard Sigmoid is often used in neural networks as a computationally efficient approximation to Sigmoid.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Hard Sigmoid activation function will be applied. </td></tr>
    <tr><td class="paramname">alpha</td><td>A parameter that controls the slope of the activation function. </td></tr>
    <tr><td class="paramname">beta</td><td>A parameter that controls the shift of the activation function. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Hard Sigmoid activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>HardSigmoid</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Hard Sigmoid activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line"><span class="keywordtype">float</span> alpha = 0.2f;</div>
<div class="line"><span class="keywordtype">float</span> beta = 0.5f;</div>
<div class="line">Tensor result = <a class="code hl_function" href="#ab9821ac15d218a07161a211b04db5528">HardSigmoid</a>(tensor, alpha, beta);  <span class="comment">// Apply the Hard Sigmoid activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_ab9821ac15d218a07161a211b04db5528"><div class="ttname"><a href="#ab9821ac15d218a07161a211b04db5528">nz::data::HardSigmoid</a></div><div class="ttdeci">Tensor HardSigmoid(const Tensor &amp;tensor, float alpha, float beta)</div><div class="ttdoc">Applies the Hard Sigmoid activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00640">Tensor.cu:640</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00640">640</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a68859b30e6833b954f27f89eb8592235" name="a68859b30e6833b954f27f89eb8592235"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a68859b30e6833b954f27f89eb8592235">&#9670;&#160;</a></span>HardSwish()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::HardSwish </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.2f</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.5f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Hard Swish activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Hard Swish activation function element-wise to the given tensor. Hard Swish is a variant of the Swish activation function, which is defined as: </p><pre class="fragment">HardSwish(x) = x * (ReLU6(x + 3) / 6)
</pre><p>where <code>ReLU6(x)</code> is the Rectified Linear Unit function with a maximum value of 6. The Hard Swish function is computationally more efficient than the original Swish function while maintaining similar properties.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Hard Swish activation function will be applied. </td></tr>
    <tr><td class="paramname">alpha</td><td>A parameter that controls the scaling of the activation. Typically, this is set to 1.0. </td></tr>
    <tr><td class="paramname">beta</td><td>A parameter that controls the shift of the activation. Typically, this is set to 3.0. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Hard Swish activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>HardSwish</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Hard Swish activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line"><span class="keywordtype">float</span> alpha = 1.0f;</div>
<div class="line"><span class="keywordtype">float</span> beta = 3.0f;</div>
<div class="line">Tensor result = <a class="code hl_function" href="#a68859b30e6833b954f27f89eb8592235">HardSwish</a>(tensor, alpha, beta);  <span class="comment">// Apply the Hard Swish activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a68859b30e6833b954f27f89eb8592235"><div class="ttname"><a href="#a68859b30e6833b954f27f89eb8592235">nz::data::HardSwish</a></div><div class="ttdeci">Tensor HardSwish(const Tensor &amp;tensor, float alpha, float beta)</div><div class="ttdoc">Applies the Hard Swish activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00684">Tensor.cu:684</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00684">684</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ae638e4a70745936b4542263a5fef9b9d" name="ae638e4a70745936b4542263a5fef9b9d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae638e4a70745936b4542263a5fef9b9d">&#9670;&#160;</a></span>LeakyReLU()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::LeakyReLU </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.01f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Tanh (hyperbolic tangent) activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Tanh activation function element-wise to the given tensor. The Tanh function squashes each element of the tensor to a value between -1 and 1. The Tanh function is often used in neural networks and is similar to the Sigmoid function but has a wider output range.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Tanh activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Tanh activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>Tanh</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Tanh activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(0.5f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a2037c2f13444847ea57d7110d8c48a51">Tanh</a>(tensor);  <span class="comment">// Apply the Tanh activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a2037c2f13444847ea57d7110d8c48a51"><div class="ttname"><a href="#a2037c2f13444847ea57d7110d8c48a51">nz::data::Tanh</a></div><div class="ttdeci">Tensor Tanh(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Tanh (hyperbolic tangent) activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00476">Tensor.cu:476</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00512">512</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ab7386d5f0c567a1bed5a78eab4bf2d6b" name="ab7386d5f0c567a1bed5a78eab4bf2d6b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab7386d5f0c567a1bed5a78eab4bf2d6b">&#9670;&#160;</a></span>operator*() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator* </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Multiplies a tensor by a scalar (element-wise multiplication). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the multiplication operator (<code>*</code>) to multiply each element of the tensor by a scalar value. It performs element-wise multiplication, where every element in the tensor is multiplied by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will be multiplied by the scalar. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value to multiply each element of the tensor by. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise multiplication.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarMul</code>) to perform the element-wise multiplication in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise multiplication.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor * scalar;  <span class="comment">// Multiply each element of the tensor by 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00174">174</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ac65cd4ed45d91da297a90d4381411064" name="ac65cd4ed45d91da297a90d4381411064"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac65cd4ed45d91da297a90d4381411064">&#9670;&#160;</a></span>operator*() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator* </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Multiplies a tensor by a scalar (element-wise multiplication). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the multiplication operator (<code>*</code>) to multiply each element of the tensor by a scalar value. It performs element-wise multiplication, where every element in the tensor is multiplied by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The scalar value to multiply each element of the tensor by. </td></tr>
    <tr><td class="paramname">rhs</td><td>The tensor whose elements will be multiplied by the scalar. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise multiplication.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarMul</code>) to perform the element-wise multiplication in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise multiplication.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = scalar * tensor;  <span class="comment">// Multiply each element of the tensor by 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00135">135</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ab5c3243e28d8132d9514c8d37d597a7f" name="ab5c3243e28d8132d9514c8d37d597a7f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab5c3243e28d8132d9514c8d37d597a7f">&#9670;&#160;</a></span>operator+() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator+ </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds a scalar to a tensor (element-wise addition). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the addition operator (<code>+</code>) to add a scalar value to each element of the tensor. It performs element-wise addition, where every element in the tensor is increased by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will be added by the scalar. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value to add to each element of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise addition.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise addition in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise addition.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor + scalar;  <span class="comment">// Add 2.0f to each element of the tensor</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00251">251</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="aa0f75427702f8732bc1833b272baa7f6" name="aa0f75427702f8732bc1833b272baa7f6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa0f75427702f8732bc1833b272baa7f6">&#9670;&#160;</a></span>operator+() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator+ </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds a scalar to a tensor (element-wise addition). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the addition operator (<code>+</code>) to add a scalar value to each element of the tensor. It performs element-wise addition, where every element in the tensor is increased by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The scalar value to add to each element of the tensor. </td></tr>
    <tr><td class="paramname">rhs</td><td>The tensor whose elements will be added by the scalar. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise addition.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise addition in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise addition.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = scalar + tensor;  <span class="comment">// Add 2.0f to each element of the tensor</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00289">289</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a402c3fdfb410d9351a43087834982bbe" name="a402c3fdfb410d9351a43087834982bbe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a402c3fdfb410d9351a43087834982bbe">&#9670;&#160;</a></span>operator-() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator- </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Subtracts a scalar from a tensor (element-wise subtraction). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the subtraction operator (<code>-</code>) to subtract a scalar value from each element of the tensor. It performs element-wise subtraction, where every element in the tensor is decreased by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will have the scalar subtracted from them. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value to subtract from each element of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise subtraction.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise subtraction in parallel on the GPU. The result is stored in a new tensor, which is returned. The scalar is negated during the operation to achieve subtraction.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise subtraction.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(10.0f);  <span class="comment">// Fill the tensor with 10.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor - scalar;  <span class="comment">// Subtract 2.0f from each element of the tensor</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00328">328</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a00d2b2ae594fa907461c12045de8f470" name="a00d2b2ae594fa907461c12045de8f470"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00d2b2ae594fa907461c12045de8f470">&#9670;&#160;</a></span>operator-() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator- </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Subtracts a tensor from a scalar (element-wise subtraction). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the subtraction operator (<code>-</code>) to subtract each element of the tensor from a scalar value. It performs element-wise subtraction, where every element in the tensor is subtracted from the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The scalar value from which each element of the tensor will be subtracted. </td></tr>
    <tr><td class="paramname">rhs</td><td>The tensor whose elements will be subtracted from the scalar. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise subtraction.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise subtraction in parallel on the GPU. The result is stored in a new tensor, which is returned. The scalar is negated during the operation to achieve subtraction.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise subtraction.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
<li>The scalar is negated to perform subtraction, which results in <code>lhs - rhs</code> for each element in the tensor.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(10.0f);  <span class="comment">// Fill the tensor with 10.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = scalar - tensor;  <span class="comment">// Subtract each element of the tensor from 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00368">368</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a3b044ce5a34b4724830f073efb954743" name="a3b044ce5a34b4724830f073efb954743"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3b044ce5a34b4724830f073efb954743">&#9670;&#160;</a></span>operator/()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator/ </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Divides a tensor by a scalar (element-wise division). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the division operator (<code>/</code>) to divide each element of the tensor by a scalar value. It performs element-wise division, where every element in the tensor is divided by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will be divided by the scalar. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value by which each element of the tensor will be divided. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise division.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarDiv</code>) to perform the element-wise division in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise division.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
<li>Division by zero should be handled appropriately, and input tensors should be checked to ensure no element is zero.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(10.0f);  <span class="comment">// Fill the tensor with 10.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor / scalar;  <span class="comment">// Divide each element of the tensor by 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00213">213</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="af967fb10a908c374d8378ac7ef22779c" name="af967fb10a908c374d8378ac7ef22779c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af967fb10a908c374d8378ac7ef22779c">&#9670;&#160;</a></span>operator&lt;&lt;() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::ostream &amp; nz::data::operator&lt;&lt; </td>
          <td>(</td>
          <td class="paramtype">std::ostream &amp;</td>          <td class="paramname"><span class="paramname"><em>os</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overload the &lt;&lt; operator to print a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object to an output stream. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">os</td><td>An output stream (host-to-host) where the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> data and gradient will be printed. </td></tr>
    <tr><td class="paramname">tensor</td><td>A constant reference (host-to-host) to the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object to be printed.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A reference to the output stream <code>os</code> after printing the tensor data and possibly its gradient.</dd></dl>
<p>This function provides a convenient way to print a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object using the &lt;&lt; operator. It first calls the <code>print</code> method of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> to print the tensor's data. If the tensor requires gradients, it then prints a header "Gradient: " followed by the gradient data using the <code>printGrad</code> method.</p>
<p>Memory management: The function does not allocate or deallocate any memory. It relies on the <code>print</code> and <code>printGrad</code> methods of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a>, which also do not perform memory allocation. Exception handling: If the tensor requires gradients and an exception occurs during the <code>printGrad</code> call (e.g., due to an invalid state of the output stream or incorrect internal data), the exception will be propagated. If the tensor does not require gradients, the <code>printGrad</code> call is skipped, and no exception related to gradient printing will be thrown. Relationship with other components: This function is related to the data presentation component of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a>. It integrates the <code>print</code> and <code>printGrad</code> methods to provide a unified way of printing the tensor and its gradient.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">std::invalid_argument</td><td>Propagated from the <code>printGrad</code> method if the tensor requires gradients and there is an issue with gradient printing.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The overall time complexity of this function is O(m * n) if the tensor does not require gradients and O(2 * m * n) if it does, where m is the number of rows (<code>_shape[0]</code>) and n is the number of columns (<code>_shape[1]</code>) of the tensor, as it iterates over the tensor data and possibly the gradient data.</li>
<li>Ensure that the output stream <code>os</code> is in a valid state before calling this function.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line">nz::data::MappedTensor::shape_type shape = {2, 3};</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_mapped_tensor.html">nz::data::MappedTensor</a> tensor(shape, <span class="keyword">true</span>);</div>
<div class="line">tensor.dataInject({1, 2, 3, 4, 5, 6}, <span class="keyword">false</span>);</div>
<div class="line">tensor.dataInject({7, 8, 9, 10, 11, 12}, <span class="keyword">true</span>);</div>
<div class="line">std::cout &lt;&lt; tensor;</div>
<div class="line">```</div>
<div class="ttc" id="aclassnz_1_1data_1_1_mapped_tensor_html"><div class="ttname"><a href="classnz_1_1data_1_1_mapped_tensor.html">nz::data::MappedTensor</a></div><div class="ttdoc">A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...</div><div class="ttdef"><b>Definition</b> <a href="_mapped_tensor_8cuh_source.html#l00065">MappedTensor.cuh:65</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_mapped_tensor_8cu_source.html#l00043">43</a> of file <a class="el" href="_mapped_tensor_8cu_source.html">MappedTensor.cu</a>.</p>

</div>
</div>
<a id="a2907370af84a6c5bdc4b72803c9edc68" name="a2907370af84a6c5bdc4b72803c9edc68"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2907370af84a6c5bdc4b72803c9edc68">&#9670;&#160;</a></span>operator&lt;&lt;() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::ostream &amp; nz::data::operator&lt;&lt; </td>
          <td>(</td>
          <td class="paramtype">std::ostream &amp;</td>          <td class="paramname"><span class="paramname"><em>os</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overloads the <code>&lt;&lt;</code> operator to print the tensor's data to an output stream. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the output stream operator (<code>&lt;&lt;</code>) to print the contents of a tensor to the specified output stream (e.g., <code>std::cout</code> or a file stream).</p>
<p>The tensor's data is first copied from GPU memory to host memory for printing, and then the data is printed in a 2D matrix format. Each row of the tensor is printed on a new line, and each element in a row is separated by a space. Each row is enclosed in square brackets.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">os</td><td>The output stream to which the tensor will be printed. </td></tr>
    <tr><td class="paramname">tensor</td><td>The tensor whose contents will be printed. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The output stream (<code>os</code>) after the tensor has been printed, allowing for chaining of operations.</dd></dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator works by accessing the tensor's private data members (e.g., <code>_data</code>) directly.</li>
<li>The tensor's data is assumed to be in a valid state (i.e., properly allocated in GPU memory) before printing.</li>
<li>The function copies the tensor's data from device (GPU) memory to host (CPU) memory using <code>cudaMemcpy</code>, which may introduce performance overhead for large tensors.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line">std::cout &lt;&lt; tensor &lt;&lt; std::endl;  <span class="comment">// Prints the tensor to standard output in matrix format</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00037">37</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a40134aba93013e1b0d43c6fd5158d400" name="a40134aba93013e1b0d43c6fd5158d400"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a40134aba93013e1b0d43c6fd5158d400">&#9670;&#160;</a></span>operator&gt;&gt;() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::istream &amp; nz::data::operator&gt;&gt; </td>
          <td>(</td>
          <td class="paramtype">std::istream &amp;</td>          <td class="paramname"><span class="paramname"><em>is</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overloads the <code>&gt;&gt;</code> operator to read a tensor's data from an input stream. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the input stream operator (<code>&gt;&gt;</code>) to read the contents of a tensor from the specified input stream (e.g., <code>std::cin</code> or a file stream).</p>
<p>The function reads the tensor's data element by element from the input stream and stores the values in a temporary buffer. Once all the data has been read, it is copied from the host memory back into the tensor's GPU memory using <code>cudaMemcpy</code>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">is</td><td>The input stream from which the tensor's data will be read. </td></tr>
    <tr><td class="paramname">tensor</td><td>The tensor to which the data will be read. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The input stream (<code>is</code>) after reading the tensor's data, allowing for chaining of operations.</dd></dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator works by reading data from the input stream and storing it in a temporary buffer on the host.</li>
<li>The function assumes that the input data matches the size of the tensor. If the data is malformed or does not match, the behavior may be undefined.</li>
<li>After reading, the data is copied from host memory back into the tensor's GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">std::cin &gt;&gt; tensor;  <span class="comment">// Reads the tensor&#39;s data from standard input</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00095">95</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a4ea5e60f987ab3853b4d0af44453a9e2" name="a4ea5e60f987ab3853b4d0af44453a9e2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4ea5e60f987ab3853b4d0af44453a9e2">&#9670;&#160;</a></span>operator&gt;&gt;() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::istream &amp; nz::data::operator&gt;&gt; </td>
          <td>(</td>
          <td class="paramtype">std::istream &amp;</td>          <td class="paramname"><span class="paramname"><em>is</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overload the &gt;&gt; operator to read data from an input stream into a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">is</td><td>An input stream (host-to-host) from which the data will be read. </td></tr>
    <tr><td class="paramname">tensor</td><td>A reference (host-to-host) to the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object where the data will be stored.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A reference to the input stream <code>is</code> after the reading operation.</dd></dl>
<p>This function provides a convenient way to populate a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object with data from an input stream. It iterates through the elements of the tensor and reads values from the input stream one by one, until either all elements of the tensor have been filled or the input stream fails to provide more data.</p>
<p>Memory management: The function does not allocate or deallocate any memory. It assumes that the <code>_data</code> array of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> has already been allocated with the appropriate size (<code>_size</code>). Exception handling: If the input stream fails to provide data (e.g., due to end-of-file or an invalid input format), the loop will terminate, and the function will return the input stream in its current state. No exceptions are thrown by this function itself, but the <code>&gt;&gt;</code> operator on the input stream may throw exceptions depending on its implementation. Relationship with other components: This function is related to the data input component of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a>. It integrates with the standard input stream to allow easy data population.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the size of the tensor (<code>_size</code>), as it iterates through each element of the tensor once.</li>
<li>Ensure that the input stream contains valid data in the correct format to avoid unexpected behavior.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line">nz::data::MappedTensor::shape_type shape = {2, 3};</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_mapped_tensor.html">nz::data::MappedTensor</a> tensor(shape, <span class="keyword">false</span>);</div>
<div class="line">std::istringstream iss(<span class="stringliteral">&quot;1 2 3 4 5 6&quot;</span>);</div>
<div class="line">iss &gt;&gt; tensor;</div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_mapped_tensor_8cu_source.html#l00079">79</a> of file <a class="el" href="_mapped_tensor_8cu_source.html">MappedTensor.cu</a>.</p>

</div>
</div>
<a id="a2cd35db5c2bc0fc68cb0461d2f600089" name="a2cd35db5c2bc0fc68cb0461d2f600089"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2cd35db5c2bc0fc68cb0461d2f600089">&#9670;&#160;</a></span>ReLU()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::ReLU </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Rectified Linear Unit (ReLU) activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the ReLU activation function element-wise to the given tensor. ReLU is a popular activation function in neural networks that replaces all negative values in the tensor with zero, leaving positive values unchanged.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the ReLU activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the ReLU activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>RectifiedLinearUnit</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise ReLU activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(-1.0f);  <span class="comment">// Fill the tensor with negative values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a2cd35db5c2bc0fc68cb0461d2f600089">ReLU</a>(tensor);  <span class="comment">// Apply the ReLU activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a2cd35db5c2bc0fc68cb0461d2f600089"><div class="ttname"><a href="#a2cd35db5c2bc0fc68cb0461d2f600089">nz::data::ReLU</a></div><div class="ttdeci">Tensor ReLU(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Rectified Linear Unit (ReLU) activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00404">Tensor.cu:404</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00404">404</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a370bede45854c04c8222e9971cedc075" name="a370bede45854c04c8222e9971cedc075"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a370bede45854c04c8222e9971cedc075">&#9670;&#160;</a></span>Sigmoid()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::Sigmoid </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Sigmoid activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Sigmoid activation function element-wise to the given tensor. The Sigmoid function squashes each element of the tensor to a value between 0 and 1. The Sigmoid function is commonly used in neural networks, especially for binary classification tasks.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Sigmoid activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Sigmoid activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>Sigmoid</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Sigmoid activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(0.5f);  <span class="comment">// Fill the tensor with values between 0 and 1</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a370bede45854c04c8222e9971cedc075">Sigmoid</a>(tensor);  <span class="comment">// Apply the Sigmoid activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a370bede45854c04c8222e9971cedc075"><div class="ttname"><a href="#a370bede45854c04c8222e9971cedc075">nz::data::Sigmoid</a></div><div class="ttdeci">Tensor Sigmoid(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Sigmoid activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00440">Tensor.cu:440</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00440">440</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a09849b685c23664bf02b0f824006f695" name="a09849b685c23664bf02b0f824006f695"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a09849b685c23664bf02b0f824006f695">&#9670;&#160;</a></span>Softmax()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::Softmax </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Softmax activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Softmax activation function element-wise to the given tensor. The Softmax function converts the tensor into a probability distribution, where each element is transformed into a value between 0 and 1, and the sum of all elements in the tensor equals 1. The Softmax function is commonly used in the output layer of neural networks for multi-class classification tasks.</p>
<p>The Softmax function for each element <code>x_i</code> in the tensor is computed as: </p><pre class="fragment">Softmax(x_i) = exp(x_i) / sum(exp(x_j) for all j)
</pre><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Softmax activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The input tensor with the Softmax function applied element-wise.</dd></dl>
<p>This function uses a CUDA kernel (<code>SummationExp</code>) to compute the sum of the exponentiated values in parallel on the GPU, and then another kernel (<code>Softmax</code>) to apply the Softmax transformation. The result is stored in the original tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator modifies the original tensor by applying the Softmax transformation in-place.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
<li>The Softmax computation is performed in two stages: first by calculating the exponentiated values' sum, then applying the Softmax transformation.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a09849b685c23664bf02b0f824006f695">Softmax</a>(tensor);  <span class="comment">// Apply the Softmax activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a09849b685c23664bf02b0f824006f695"><div class="ttname"><a href="#a09849b685c23664bf02b0f824006f695">nz::data::Softmax</a></div><div class="ttdeci">Tensor Softmax(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Softmax activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00727">Tensor.cu:727</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00727">727</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a271473f387bde9059ad5afa03851184a" name="a271473f387bde9059ad5afa03851184a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a271473f387bde9059ad5afa03851184a">&#9670;&#160;</a></span>Swish()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::Swish </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Swish activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Swish activation function element-wise to the given tensor. Swish is a smooth, non-monotonic activation function defined as: </p><pre class="fragment">Swish(x) = x / (1 + exp(-x))
</pre><p>Swish has been shown to work well in deep neural networks, offering benefits over ReLU in certain tasks.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Swish activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Swish activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>Swish</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Swish activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(0.5f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a271473f387bde9059ad5afa03851184a">Swish</a>(tensor);  <span class="comment">// Apply the Swish activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a271473f387bde9059ad5afa03851184a"><div class="ttname"><a href="#a271473f387bde9059ad5afa03851184a">nz::data::Swish</a></div><div class="ttdeci">Tensor Swish(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Swish activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00551">Tensor.cu:551</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00551">551</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a2037c2f13444847ea57d7110d8c48a51" name="a2037c2f13444847ea57d7110d8c48a51"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2037c2f13444847ea57d7110d8c48a51">&#9670;&#160;</a></span>Tanh()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::Tanh </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Tanh (hyperbolic tangent) activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Tanh activation function element-wise to the given tensor. The Tanh function squashes each element of the tensor to a value between -1 and 1. The Tanh function is often used in neural networks and is similar to the Sigmoid function but has a wider output range.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Tanh activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Tanh activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>Tanh</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Tanh activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(0.5f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a2037c2f13444847ea57d7110d8c48a51">Tanh</a>(tensor);  <span class="comment">// Apply the Tanh activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00476">476</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
