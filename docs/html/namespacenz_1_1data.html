<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NeuZephyr: nz::data Namespace Reference</title>
<link rel="icon" href="NZ_logo2.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="NZ_logo2.png"/></td>
  <td id="projectalign">
   <div id="projectname">NeuZephyr
   </div>
   <div id="projectbrief">Simple DL Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li class="current"><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="namespaces.html"><span>Namespace&#160;List</span></a></li>
      <li><a href="namespacemembers.html"><span>Namespace&#160;Members</span></a></li>
    </ul>
  </div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>nz</b></li><li class="navelem"><a class="el" href="namespacenz_1_1data.html">data</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">nz::data Namespace Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Contains data structures and utilities for tensor operations in machine learning workflows.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible container-like interfaces.  <a href="classnz_1_1data_1_1_mapped_tensor.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.  <a href="classnz_1_1data_1_1_tensor.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a9bcee9a75db3d824b92ed17a59711428" id="r_a9bcee9a75db3d824b92ed17a59711428"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a9bcee9a75db3d824b92ed17a59711428"><td class="memTemplItemLeft" align="right" valign="top">T&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a9bcee9a75db3d824b92ed17a59711428">ReLU</a> (T input)</td></tr>
<tr class="memdesc:a9bcee9a75db3d824b92ed17a59711428"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply the Rectified Linear Unit (ReLU) activation function element-wise to an input tensor.  <br /></td></tr>
<tr class="separator:a9bcee9a75db3d824b92ed17a59711428"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0740e60918114b0a4d7c88d09aeb8f88" id="r_a0740e60918114b0a4d7c88d09aeb8f88"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a0740e60918114b0a4d7c88d09aeb8f88"><td class="memTemplItemLeft" align="right" valign="top">T&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a0740e60918114b0a4d7c88d09aeb8f88">Sigmoid</a> (T input)</td></tr>
<tr class="memdesc:a0740e60918114b0a4d7c88d09aeb8f88"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply the sigmoid activation function element-wise to an input tensor.  <br /></td></tr>
<tr class="separator:a0740e60918114b0a4d7c88d09aeb8f88"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a56c9ff66e41ae568a2fec83708c4dd85" id="r_a56c9ff66e41ae568a2fec83708c4dd85"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a56c9ff66e41ae568a2fec83708c4dd85"><td class="memTemplItemLeft" align="right" valign="top">T&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a56c9ff66e41ae568a2fec83708c4dd85">Tanh</a> (T input)</td></tr>
<tr class="memdesc:a56c9ff66e41ae568a2fec83708c4dd85"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply the hyperbolic tangent (tanh) activation function element-wise to an input tensor.  <br /></td></tr>
<tr class="separator:a56c9ff66e41ae568a2fec83708c4dd85"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a65ebd7e32d0d942bffdcd1665def151e" id="r_a65ebd7e32d0d942bffdcd1665def151e"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a65ebd7e32d0d942bffdcd1665def151e"><td class="memTemplItemLeft" align="right" valign="top">T&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a65ebd7e32d0d942bffdcd1665def151e">LeakyReLU</a> (T input, const float alpha=0.01f)</td></tr>
<tr class="memdesc:a65ebd7e32d0d942bffdcd1665def151e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply the Leaky Rectified Linear Unit (Leaky ReLU) activation function element-wise to an input tensor.  <br /></td></tr>
<tr class="separator:a65ebd7e32d0d942bffdcd1665def151e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a42cbe5a414e4212ee7a26b0a66ce08a4" id="r_a42cbe5a414e4212ee7a26b0a66ce08a4"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a42cbe5a414e4212ee7a26b0a66ce08a4"><td class="memTemplItemLeft" align="right" valign="top">T&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a42cbe5a414e4212ee7a26b0a66ce08a4">Swish</a> (T input)</td></tr>
<tr class="memdesc:a42cbe5a414e4212ee7a26b0a66ce08a4"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply the Swish activation function element-wise to an input tensor.  <br /></td></tr>
<tr class="separator:a42cbe5a414e4212ee7a26b0a66ce08a4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5e6656db9844fdb33add8afa7c0ceb8a" id="r_a5e6656db9844fdb33add8afa7c0ceb8a"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a5e6656db9844fdb33add8afa7c0ceb8a"><td class="memTemplItemLeft" align="right" valign="top">T&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a5e6656db9844fdb33add8afa7c0ceb8a">ELU</a> (T input, const float alpha=1.0f)</td></tr>
<tr class="memdesc:a5e6656db9844fdb33add8afa7c0ceb8a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply the Exponential Linear Unit (ELU) activation function element-wise to an input tensor.  <br /></td></tr>
<tr class="separator:a5e6656db9844fdb33add8afa7c0ceb8a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a836b8388aba110fd9130757ef743597a" id="r_a836b8388aba110fd9130757ef743597a"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a836b8388aba110fd9130757ef743597a"><td class="memTemplItemLeft" align="right" valign="top">T&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a836b8388aba110fd9130757ef743597a">HardSigmoid</a> (T input, const float alpha=0.2f, const float beta=0.5f)</td></tr>
<tr class="memdesc:a836b8388aba110fd9130757ef743597a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply the Hard Sigmoid activation function element-wise to an input tensor.  <br /></td></tr>
<tr class="separator:a836b8388aba110fd9130757ef743597a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a07ab6e69dbefd62eae7058c9136aea2a" id="r_a07ab6e69dbefd62eae7058c9136aea2a"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:a07ab6e69dbefd62eae7058c9136aea2a"><td class="memTemplItemLeft" align="right" valign="top">T&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a07ab6e69dbefd62eae7058c9136aea2a">HardSwish</a> (T input, const float alpha=0.5f, const float beta=0.5f)</td></tr>
<tr class="memdesc:a07ab6e69dbefd62eae7058c9136aea2a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Apply the Hard Swish activation function element-wise to an input tensor.  <br /></td></tr>
<tr class="separator:a07ab6e69dbefd62eae7058c9136aea2a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad6092c1138e5093c07ec00c12c72314a" id="r_ad6092c1138e5093c07ec00c12c72314a"><td class="memTemplParams" colspan="2">template&lt;typename T &gt; </td></tr>
<tr class="memitem:ad6092c1138e5093c07ec00c12c72314a"><td class="memTemplItemLeft" align="right" valign="top">T&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#ad6092c1138e5093c07ec00c12c72314a">Softmax</a> (T input)</td></tr>
<tr class="memdesc:ad6092c1138e5093c07ec00c12c72314a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the softmax function for a given input of type T.  <br /></td></tr>
<tr class="separator:ad6092c1138e5093c07ec00c12c72314a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af967fb10a908c374d8378ac7ef22779c" id="r_af967fb10a908c374d8378ac7ef22779c"><td class="memItemLeft" align="right" valign="top">std::ostream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af967fb10a908c374d8378ac7ef22779c">operator&lt;&lt;</a> (std::ostream &amp;os, const <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:af967fb10a908c374d8378ac7ef22779c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overload the &lt;&lt; operator to print a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object to an output stream.  <br /></td></tr>
<tr class="separator:af967fb10a908c374d8378ac7ef22779c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4ea5e60f987ab3853b4d0af44453a9e2" id="r_a4ea5e60f987ab3853b4d0af44453a9e2"><td class="memItemLeft" align="right" valign="top">std::istream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a4ea5e60f987ab3853b4d0af44453a9e2">operator&gt;&gt;</a> (std::istream &amp;is, <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a4ea5e60f987ab3853b4d0af44453a9e2"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overload the &gt;&gt; operator to read data from an input stream into a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object.  <br /></td></tr>
<tr class="separator:a4ea5e60f987ab3853b4d0af44453a9e2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2907370af84a6c5bdc4b72803c9edc68" id="r_a2907370af84a6c5bdc4b72803c9edc68"><td class="memItemLeft" align="right" valign="top">std::ostream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2907370af84a6c5bdc4b72803c9edc68">operator&lt;&lt;</a> (std::ostream &amp;os, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a2907370af84a6c5bdc4b72803c9edc68"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overloads the <code>&lt;&lt;</code> operator to print the tensor's data to an output stream.  <br /></td></tr>
<tr class="separator:a2907370af84a6c5bdc4b72803c9edc68"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a40134aba93013e1b0d43c6fd5158d400" id="r_a40134aba93013e1b0d43c6fd5158d400"><td class="memItemLeft" align="right" valign="top">std::istream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a40134aba93013e1b0d43c6fd5158d400">operator&gt;&gt;</a> (std::istream &amp;is, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a40134aba93013e1b0d43c6fd5158d400"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overloads the <code>&gt;&gt;</code> operator to read a tensor's data from an input stream.  <br /></td></tr>
<tr class="separator:a40134aba93013e1b0d43c6fd5158d400"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac65cd4ed45d91da297a90d4381411064" id="r_ac65cd4ed45d91da297a90d4381411064"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ac65cd4ed45d91da297a90d4381411064">operator*</a> (const Tensor::value_type lhs, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;rhs)</td></tr>
<tr class="memdesc:ac65cd4ed45d91da297a90d4381411064"><td class="mdescLeft">&#160;</td><td class="mdescRight">Multiplies a tensor by a scalar (element-wise multiplication).  <br /></td></tr>
<tr class="separator:ac65cd4ed45d91da297a90d4381411064"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab7386d5f0c567a1bed5a78eab4bf2d6b" id="r_ab7386d5f0c567a1bed5a78eab4bf2d6b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab7386d5f0c567a1bed5a78eab4bf2d6b">operator*</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:ab7386d5f0c567a1bed5a78eab4bf2d6b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Multiplies a tensor by a scalar (element-wise multiplication).  <br /></td></tr>
<tr class="separator:ab7386d5f0c567a1bed5a78eab4bf2d6b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3b044ce5a34b4724830f073efb954743" id="r_a3b044ce5a34b4724830f073efb954743"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a3b044ce5a34b4724830f073efb954743">operator/</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:a3b044ce5a34b4724830f073efb954743"><td class="mdescLeft">&#160;</td><td class="mdescRight">Divides a tensor by a scalar (element-wise division).  <br /></td></tr>
<tr class="separator:a3b044ce5a34b4724830f073efb954743"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5c3243e28d8132d9514c8d37d597a7f" id="r_ab5c3243e28d8132d9514c8d37d597a7f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab5c3243e28d8132d9514c8d37d597a7f">operator+</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:ab5c3243e28d8132d9514c8d37d597a7f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a scalar to a tensor (element-wise addition).  <br /></td></tr>
<tr class="separator:ab5c3243e28d8132d9514c8d37d597a7f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0f75427702f8732bc1833b272baa7f6" id="r_aa0f75427702f8732bc1833b272baa7f6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aa0f75427702f8732bc1833b272baa7f6">operator+</a> (const Tensor::value_type lhs, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;rhs)</td></tr>
<tr class="memdesc:aa0f75427702f8732bc1833b272baa7f6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a scalar to a tensor (element-wise addition).  <br /></td></tr>
<tr class="separator:aa0f75427702f8732bc1833b272baa7f6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a402c3fdfb410d9351a43087834982bbe" id="r_a402c3fdfb410d9351a43087834982bbe"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a402c3fdfb410d9351a43087834982bbe">operator-</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:a402c3fdfb410d9351a43087834982bbe"><td class="mdescLeft">&#160;</td><td class="mdescRight">Subtracts a scalar from a tensor (element-wise subtraction).  <br /></td></tr>
<tr class="separator:a402c3fdfb410d9351a43087834982bbe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a00d2b2ae594fa907461c12045de8f470" id="r_a00d2b2ae594fa907461c12045de8f470"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a00d2b2ae594fa907461c12045de8f470">operator-</a> (const Tensor::value_type lhs, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;rhs)</td></tr>
<tr class="memdesc:a00d2b2ae594fa907461c12045de8f470"><td class="mdescLeft">&#160;</td><td class="mdescRight">Subtracts a tensor from a scalar (element-wise subtraction).  <br /></td></tr>
<tr class="separator:a00d2b2ae594fa907461c12045de8f470"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a09849b685c23664bf02b0f824006f695" id="r_a09849b685c23664bf02b0f824006f695"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a09849b685c23664bf02b0f824006f695">Softmax</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a09849b685c23664bf02b0f824006f695"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Softmax activation function to a tensor.  <br /></td></tr>
<tr class="separator:a09849b685c23664bf02b0f824006f695"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Contains data structures and utilities for tensor operations in machine learning workflows. </p>
<p>The <code><a class="el" href="namespacenz_1_1data.html" title="Contains data structures and utilities for tensor operations in machine learning workflows.">nz::data</a></code> namespace provides foundational classes and functions for managing and manipulating tensors in GPU-based computations. It is designed for use in deep learning frameworks and other numerical computing applications.</p>
<p>Key components within this namespace include:</p><ul>
<li><b><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></b>: A class representing multidimensional arrays (tensors) stored in GPU memory.</li>
<li><b>Utilities</b>: Functions and operators for performing mathematical operations, memory management, and activation functions.</li>
</ul>
<p>The namespace is intended to encapsulate all tensor-related functionality to ensure modularity and maintainability in the larger nz project.</p>
<dl class="section note"><dt>Note</dt><dd>The components in this namespace rely on CUDA for GPU-based operations. Ensure that CUDA-compatible hardware and software are properly configured.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge(<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/11/29 </dd></dl>
</div><h2 class="groupheader">Function Documentation</h2>
<a id="a5e6656db9844fdb33add8afa7c0ceb8a" name="a5e6656db9844fdb33add8afa7c0ceb8a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a5e6656db9844fdb33add8afa7c0ceb8a">&#9670;&#160;</a></span>ELU()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">T nz::data::ELU </td>
          <td>(</td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>input</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">1.0f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Apply the Exponential Linear Unit (ELU) activation function element-wise to an input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input tensor (either <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) to which the ELU function will be applied (device-to-device). </td></tr>
    <tr><td class="paramname">alpha</td><td>The alpha value for the ELU function. It controls the value to which the function saturates for negative inputs. The default value is 1.0f.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor (of the same type as the input: <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) with the ELU function applied element-wise.</dd></dl>
<p>This function applies the ELU activation function, defined as ( f(x) = \begin{cases} x &amp; \text{if } x \geq 0 \ \alpha (e^{x}- 1) &amp; \text{if } x &lt; 0 \end{cases} ), to each element of the input tensor. It first creates a new tensor <code>result</code> with the same shape and gradient requirement as the input tensor. Then, it calls the <code>iELU</code> function to perform the actual ELU operation on the data of the input tensor and store the results in the <code>result</code> tensor. Finally, the <code>result</code> tensor is returned.</p>
<p>Memory management: A new tensor <code>result</code> is created, and its memory is managed by the tensor's own class (<code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>). The memory of the input tensor remains unchanged. Exception handling: There is no explicit exception handling in this function. However, if the <code>iELU</code> function or the tensor constructors throw exceptions, they will propagate up. Relationship with other components: This function depends on the <code>iELU</code> function to perform the ELU operation and the tensor's constructor to create a new tensor.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">[Exception</td><td>type thrown by iELU or tensor constructors] If there are issues during the operation, such as memory allocation failures or incorrect input data.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the number of elements in the input tensor (<code>input.size()</code>), as it needs to apply the ELU function to each element.</li>
<li>A positive <code>alpha</code> value is recommended for better performance and to avoid the vanishing gradient problem.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="comment">// Assume T is either Tensor or MappedTensor</span></div>
<div class="line">nz::data::T::shape_type shape = {2, 3};</div>
<div class="line">nz::data::T input(shape, <span class="keyword">true</span>);</div>
<div class="line">nz::data::T output = <a class="code hl_function" href="#a5e6656db9844fdb33add8afa7c0ceb8a">ELU</a>(input, 0.5f);</div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a5e6656db9844fdb33add8afa7c0ceb8a"><div class="ttname"><a href="#a5e6656db9844fdb33add8afa7c0ceb8a">nz::data::ELU</a></div><div class="ttdeci">T ELU(T input, const float alpha=1.0f)</div><div class="ttdoc">Apply the Exponential Linear Unit (ELU) activation function element-wise to an input tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00223">TensorOperations.cuh:223</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_operations_8cuh_source.html#l00223">223</a> of file <a class="el" href="_tensor_operations_8cuh_source.html">TensorOperations.cuh</a>.</p>

</div>
</div>
<a id="a836b8388aba110fd9130757ef743597a" name="a836b8388aba110fd9130757ef743597a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a836b8388aba110fd9130757ef743597a">&#9670;&#160;</a></span>HardSigmoid()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">T nz::data::HardSigmoid </td>
          <td>(</td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>input</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.2f</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.5f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Apply the Hard Sigmoid activation function element-wise to an input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input tensor (either <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) to which the Hard Sigmoid function will be applied (device-to-device). </td></tr>
    <tr><td class="paramname">alpha</td><td>The alpha value for the Hard Sigmoid function, controlling the slope of the linear part. The default value is 0.2f. </td></tr>
    <tr><td class="paramname">beta</td><td>The beta value for the Hard Sigmoid function, controlling the bias of the linear part. The default value is 0.5f.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor (of the same type as the input: <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) with the Hard Sigmoid function applied element-wise.</dd></dl>
<p>This function applies the Hard Sigmoid activation function, typically defined as ( f(x) = \max(0, \min(1, \alpha x + \beta)) ), to each element of the input tensor. It first creates a new tensor <code>result</code> with the same shape and gradient requirement as the input tensor. Then, it calls the <code>iHardSigmoid</code> function to perform the actual Hard Sigmoid operation on the data of the input tensor and store the results in the <code>result</code> tensor. Finally, the <code>result</code> tensor is returned.</p>
<p>Memory management: A new tensor <code>result</code> is created, and its memory is managed by the tensor's own class (<code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>). The memory of the input tensor remains unchanged. Exception handling: There is no explicit exception handling in this function. However, if the <code>iHardSigmoid</code> function or the tensor constructors throw exceptions, they will propagate up. Relationship with other components: This function depends on the <code>iHardSigmoid</code> function to perform the Hard Sigmoid operation and the tensor's constructor to create a new tensor.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">[Exception</td><td>type thrown by iHardSigmoid or tensor constructors] If there are issues during the operation, such as memory allocation failures or incorrect input data.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the number of elements in the input tensor (<code>input.size()</code>), as it needs to apply the Hard Sigmoid function to each element.</li>
<li>The choice of <code>alpha</code> and <code>beta</code> values can significantly affect the behavior of the Hard Sigmoid function.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="comment">// Assume T is either Tensor or MappedTensor</span></div>
<div class="line">nz::data::T::shape_type shape = {2, 3};</div>
<div class="line">nz::data::T input(shape, <span class="keyword">true</span>);</div>
<div class="line">nz::data::T output = <a class="code hl_function" href="#a836b8388aba110fd9130757ef743597a">HardSigmoid</a>(input, 0.3f, 0.6f);</div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a836b8388aba110fd9130757ef743597a"><div class="ttname"><a href="#a836b8388aba110fd9130757ef743597a">nz::data::HardSigmoid</a></div><div class="ttdeci">T HardSigmoid(T input, const float alpha=0.2f, const float beta=0.5f)</div><div class="ttdoc">Apply the Hard Sigmoid activation function element-wise to an input tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00262">TensorOperations.cuh:262</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_operations_8cuh_source.html#l00262">262</a> of file <a class="el" href="_tensor_operations_8cuh_source.html">TensorOperations.cuh</a>.</p>

</div>
</div>
<a id="a07ab6e69dbefd62eae7058c9136aea2a" name="a07ab6e69dbefd62eae7058c9136aea2a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a07ab6e69dbefd62eae7058c9136aea2a">&#9670;&#160;</a></span>HardSwish()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">T nz::data::HardSwish </td>
          <td>(</td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>input</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.5f</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.5f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Apply the Hard Swish activation function element-wise to an input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input tensor (either <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) to which the Hard Swish function will be applied (device-to-device). </td></tr>
    <tr><td class="paramname">alpha</td><td>The alpha value for the Hard Swish function, used to scale the input. The default value is 0.5f. </td></tr>
    <tr><td class="paramname">beta</td><td>The beta value for the Hard Swish function, used as an offset. The default value is 0.5f.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor (of the same type as the input: <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) with the Hard Swish function applied element-wise.</dd></dl>
<p>This function applies the Hard Swish activation function to each element of the input tensor. The Hard Swish function is often defined as ( f(x)=x \cdot \max(0, \min(1, \alpha x+\beta)) ). It first creates a new tensor <code>result</code> with the same shape and gradient requirement as the input tensor. Then, it calls the <code>iHardSwish</code> function to perform the actual Hard Swish operation on the data of the input tensor and store the results in the <code>result</code> tensor. Finally, the <code>result</code> tensor is returned.</p>
<p>Memory management: A new tensor <code>result</code> is created, and its memory is managed by the tensor's own class (<code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>). The memory of the input tensor remains unchanged. Exception handling: There is no explicit exception handling in this function. However, if the <code>iHardSwish</code> function or the tensor constructors throw exceptions, they will propagate up. Relationship with other components: This function depends on the <code>iHardSwish</code> function to perform the Hard Swish operation and the tensor's constructor to create a new tensor.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">[Exception</td><td>type thrown by iHardSwish or tensor constructors] If there are issues during the operation, such as memory allocation failures or incorrect input data.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the number of elements in the input tensor (<code>input.size()</code>), as it needs to apply the Hard Swish function to each element.</li>
<li>The values of <code>alpha</code> and <code>beta</code> can be adjusted to fine - tune the behavior of the Hard Swish function.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="comment">// Assume T is either Tensor or MappedTensor</span></div>
<div class="line">nz::data::T::shape_type shape = {2, 3};</div>
<div class="line">nz::data::T input(shape, <span class="keyword">true</span>);</div>
<div class="line">nz::data::T output = <a class="code hl_function" href="#a07ab6e69dbefd62eae7058c9136aea2a">HardSwish</a>(input, 0.4f, 0.7f);</div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a07ab6e69dbefd62eae7058c9136aea2a"><div class="ttname"><a href="#a07ab6e69dbefd62eae7058c9136aea2a">nz::data::HardSwish</a></div><div class="ttdeci">T HardSwish(T input, const float alpha=0.5f, const float beta=0.5f)</div><div class="ttdoc">Apply the Hard Swish activation function element-wise to an input tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00301">TensorOperations.cuh:301</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_operations_8cuh_source.html#l00301">301</a> of file <a class="el" href="_tensor_operations_8cuh_source.html">TensorOperations.cuh</a>.</p>

</div>
</div>
<a id="a65ebd7e32d0d942bffdcd1665def151e" name="a65ebd7e32d0d942bffdcd1665def151e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a65ebd7e32d0d942bffdcd1665def151e">&#9670;&#160;</a></span>LeakyReLU()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">T nz::data::LeakyReLU </td>
          <td>(</td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>input</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.01f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Apply the Leaky Rectified Linear Unit (Leaky ReLU) activation function element-wise to an input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input tensor (either <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) to which the Leaky ReLU function will be applied (device-to-device). </td></tr>
    <tr><td class="paramname">alpha</td><td>The slope coefficient for negative values. It has a default value of 0.01f.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor (of the same type as the input: <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) with the Leaky ReLU function applied element-wise.</dd></dl>
<p>This function applies the Leaky ReLU activation function, defined as ( f(x) = \begin{cases} x &amp; \text{if } x \geq 0 \ \alpha x &amp; \text{if } x &lt; 0 \end{cases} ), to each element of the input tensor. It first creates a new tensor <code>result</code> with the same shape and gradient requirement as the input tensor. Then, it calls the <code>iLeakyReLU</code> function to perform the actual Leaky ReLU operation on the data of the input tensor and store the results in the <code>result</code> tensor. Finally, the <code>result</code> tensor is returned.</p>
<p>Memory management: A new tensor <code>result</code> is created, and its memory is managed by the tensor's own class (<code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>). The memory of the input tensor remains unchanged. Exception handling: There is no explicit exception handling in this function. However, if the <code>iLeakyReLU</code> function or the tensor constructors throw exceptions, they will propagate up. Relationship with other components: This function depends on the <code>iLeakyReLU</code> function to perform the Leaky ReLU operation and the tensor's constructor to create a new tensor.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">[Exception</td><td>type thrown by iLeakyReLU or tensor constructors] If there are issues during the operation, such as memory allocation failures or incorrect input data.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the number of elements in the input tensor (<code>input.size()</code>), as it needs to apply the Leaky ReLU function to each element.</li>
<li>The value of <code>alpha</code> should be a small positive number to avoid vanishing gradient problem for negative inputs.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="comment">// Assume T is either Tensor or MappedTensor</span></div>
<div class="line">nz::data::T::shape_type shape = {2, 3};</div>
<div class="line">nz::data::T input(shape, <span class="keyword">true</span>);</div>
<div class="line">nz::data::T output = <a class="code hl_function" href="#a65ebd7e32d0d942bffdcd1665def151e">LeakyReLU</a>(input, 0.02f);</div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a65ebd7e32d0d942bffdcd1665def151e"><div class="ttname"><a href="#a65ebd7e32d0d942bffdcd1665def151e">nz::data::LeakyReLU</a></div><div class="ttdeci">T LeakyReLU(T input, const float alpha=0.01f)</div><div class="ttdoc">Apply the Leaky Rectified Linear Unit (Leaky ReLU) activation function element-wise to an input tenso...</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00149">TensorOperations.cuh:149</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_operations_8cuh_source.html#l00149">149</a> of file <a class="el" href="_tensor_operations_8cuh_source.html">TensorOperations.cuh</a>.</p>

</div>
</div>
<a id="ab7386d5f0c567a1bed5a78eab4bf2d6b" name="ab7386d5f0c567a1bed5a78eab4bf2d6b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab7386d5f0c567a1bed5a78eab4bf2d6b">&#9670;&#160;</a></span>operator*() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator* </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Multiplies a tensor by a scalar (element-wise multiplication). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the multiplication operator (<code>*</code>) to multiply each element of the tensor by a scalar value. It performs element-wise multiplication, where every element in the tensor is multiplied by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will be multiplied by the scalar. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value to multiply each element of the tensor by. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise multiplication.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarMul</code>) to perform the element-wise multiplication in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise multiplication.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor * scalar;  <span class="comment">// Multiply each element of the tensor by 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="aclassnz_1_1data_1_1_tensor_html"><div class="ttname"><a href="classnz_1_1data_1_1_tensor.html">nz::data::Tensor</a></div><div class="ttdoc">A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cuh_source.html#l00133">Tensor.cuh:133</a></div></div>
<div class="ttc" id="aclassnz_1_1data_1_1_tensor_html_aaba025895b2c8b69de8d36c73c74929e"><div class="ttname"><a href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">nz::data::Tensor::fill</a></div><div class="ttdeci">void fill(value_type value) const</div><div class="ttdoc">Fills the tensor's data with a specified value.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00619">Tensor.cu:619</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00174">174</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ac65cd4ed45d91da297a90d4381411064" name="ac65cd4ed45d91da297a90d4381411064"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac65cd4ed45d91da297a90d4381411064">&#9670;&#160;</a></span>operator*() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator* </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Multiplies a tensor by a scalar (element-wise multiplication). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the multiplication operator (<code>*</code>) to multiply each element of the tensor by a scalar value. It performs element-wise multiplication, where every element in the tensor is multiplied by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The scalar value to multiply each element of the tensor by. </td></tr>
    <tr><td class="paramname">rhs</td><td>The tensor whose elements will be multiplied by the scalar. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise multiplication.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarMul</code>) to perform the element-wise multiplication in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise multiplication.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = scalar * tensor;  <span class="comment">// Multiply each element of the tensor by 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00135">135</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ab5c3243e28d8132d9514c8d37d597a7f" name="ab5c3243e28d8132d9514c8d37d597a7f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab5c3243e28d8132d9514c8d37d597a7f">&#9670;&#160;</a></span>operator+() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator+ </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds a scalar to a tensor (element-wise addition). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the addition operator (<code>+</code>) to add a scalar value to each element of the tensor. It performs element-wise addition, where every element in the tensor is increased by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will be added by the scalar. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value to add to each element of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise addition.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise addition in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise addition.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor + scalar;  <span class="comment">// Add 2.0f to each element of the tensor</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00251">251</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="aa0f75427702f8732bc1833b272baa7f6" name="aa0f75427702f8732bc1833b272baa7f6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa0f75427702f8732bc1833b272baa7f6">&#9670;&#160;</a></span>operator+() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator+ </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds a scalar to a tensor (element-wise addition). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the addition operator (<code>+</code>) to add a scalar value to each element of the tensor. It performs element-wise addition, where every element in the tensor is increased by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The scalar value to add to each element of the tensor. </td></tr>
    <tr><td class="paramname">rhs</td><td>The tensor whose elements will be added by the scalar. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise addition.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise addition in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise addition.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = scalar + tensor;  <span class="comment">// Add 2.0f to each element of the tensor</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00289">289</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a402c3fdfb410d9351a43087834982bbe" name="a402c3fdfb410d9351a43087834982bbe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a402c3fdfb410d9351a43087834982bbe">&#9670;&#160;</a></span>operator-() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator- </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Subtracts a scalar from a tensor (element-wise subtraction). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the subtraction operator (<code>-</code>) to subtract a scalar value from each element of the tensor. It performs element-wise subtraction, where every element in the tensor is decreased by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will have the scalar subtracted from them. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value to subtract from each element of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise subtraction.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise subtraction in parallel on the GPU. The result is stored in a new tensor, which is returned. The scalar is negated during the operation to achieve subtraction.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise subtraction.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(10.0f);  <span class="comment">// Fill the tensor with 10.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor - scalar;  <span class="comment">// Subtract 2.0f from each element of the tensor</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00328">328</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a00d2b2ae594fa907461c12045de8f470" name="a00d2b2ae594fa907461c12045de8f470"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00d2b2ae594fa907461c12045de8f470">&#9670;&#160;</a></span>operator-() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator- </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Subtracts a tensor from a scalar (element-wise subtraction). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the subtraction operator (<code>-</code>) to subtract each element of the tensor from a scalar value. It performs element-wise subtraction, where every element in the tensor is subtracted from the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The scalar value from which each element of the tensor will be subtracted. </td></tr>
    <tr><td class="paramname">rhs</td><td>The tensor whose elements will be subtracted from the scalar. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise subtraction.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise subtraction in parallel on the GPU. The result is stored in a new tensor, which is returned. The scalar is negated during the operation to achieve subtraction.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise subtraction.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
<li>The scalar is negated to perform subtraction, which results in <code>lhs - rhs</code> for each element in the tensor.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(10.0f);  <span class="comment">// Fill the tensor with 10.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = scalar - tensor;  <span class="comment">// Subtract each element of the tensor from 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00368">368</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a3b044ce5a34b4724830f073efb954743" name="a3b044ce5a34b4724830f073efb954743"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3b044ce5a34b4724830f073efb954743">&#9670;&#160;</a></span>operator/()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator/ </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Divides a tensor by a scalar (element-wise division). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the division operator (<code>/</code>) to divide each element of the tensor by a scalar value. It performs element-wise division, where every element in the tensor is divided by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will be divided by the scalar. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value by which each element of the tensor will be divided. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise division.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarDiv</code>) to perform the element-wise division in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise division.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
<li>Division by zero should be handled appropriately, and input tensors should be checked to ensure no element is zero.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(10.0f);  <span class="comment">// Fill the tensor with 10.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor / scalar;  <span class="comment">// Divide each element of the tensor by 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00213">213</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="af967fb10a908c374d8378ac7ef22779c" name="af967fb10a908c374d8378ac7ef22779c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af967fb10a908c374d8378ac7ef22779c">&#9670;&#160;</a></span>operator&lt;&lt;() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::ostream &amp; nz::data::operator&lt;&lt; </td>
          <td>(</td>
          <td class="paramtype">std::ostream &amp;</td>          <td class="paramname"><span class="paramname"><em>os</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overload the &lt;&lt; operator to print a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object to an output stream. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">os</td><td>An output stream (host-to-host) where the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> data and gradient will be printed. </td></tr>
    <tr><td class="paramname">tensor</td><td>A constant reference (host-to-host) to the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object to be printed.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A reference to the output stream <code>os</code> after printing the tensor data and possibly its gradient.</dd></dl>
<p>This function provides a convenient way to print a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object using the &lt;&lt; operator. It first calls the <code>print</code> method of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> to print the tensor's data. If the tensor requires gradients, it then prints a header "Gradient: " followed by the gradient data using the <code>printGrad</code> method.</p>
<p>Memory management: The function does not allocate or deallocate any memory. It relies on the <code>print</code> and <code>printGrad</code> methods of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a>, which also do not perform memory allocation. Exception handling: If the tensor requires gradients and an exception occurs during the <code>printGrad</code> call (e.g., due to an invalid state of the output stream or incorrect internal data), the exception will be propagated. If the tensor does not require gradients, the <code>printGrad</code> call is skipped, and no exception related to gradient printing will be thrown. Relationship with other components: This function is related to the data presentation component of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a>. It integrates the <code>print</code> and <code>printGrad</code> methods to provide a unified way of printing the tensor and its gradient.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">std::invalid_argument</td><td>Propagated from the <code>printGrad</code> method if the tensor requires gradients and there is an issue with gradient printing.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The overall time complexity of this function is O(m * n) if the tensor does not require gradients and O(2 * m * n) if it does, where m is the number of rows (<code>_shape[0]</code>) and n is the number of columns (<code>_shape[1]</code>) of the tensor, as it iterates over the tensor data and possibly the gradient data.</li>
<li>Ensure that the output stream <code>os</code> is in a valid state before calling this function.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line">nz::data::MappedTensor::shape_type shape = {2, 3};</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_mapped_tensor.html">nz::data::MappedTensor</a> tensor(shape, <span class="keyword">true</span>);</div>
<div class="line">tensor.dataInject({1, 2, 3, 4, 5, 6}, <span class="keyword">false</span>);</div>
<div class="line">tensor.dataInject({7, 8, 9, 10, 11, 12}, <span class="keyword">true</span>);</div>
<div class="line">std::cout &lt;&lt; tensor;</div>
<div class="line">```</div>
<div class="ttc" id="aclassnz_1_1data_1_1_mapped_tensor_html"><div class="ttname"><a href="classnz_1_1data_1_1_mapped_tensor.html">nz::data::MappedTensor</a></div><div class="ttdoc">A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...</div><div class="ttdef"><b>Definition</b> <a href="_mapped_tensor_8cuh_source.html#l00065">MappedTensor.cuh:65</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_mapped_tensor_8cu_source.html#l00043">43</a> of file <a class="el" href="_mapped_tensor_8cu_source.html">MappedTensor.cu</a>.</p>

</div>
</div>
<a id="a2907370af84a6c5bdc4b72803c9edc68" name="a2907370af84a6c5bdc4b72803c9edc68"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2907370af84a6c5bdc4b72803c9edc68">&#9670;&#160;</a></span>operator&lt;&lt;() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::ostream &amp; nz::data::operator&lt;&lt; </td>
          <td>(</td>
          <td class="paramtype">std::ostream &amp;</td>          <td class="paramname"><span class="paramname"><em>os</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overloads the <code>&lt;&lt;</code> operator to print the tensor's data to an output stream. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the output stream operator (<code>&lt;&lt;</code>) to print the contents of a tensor to the specified output stream (e.g., <code>std::cout</code> or a file stream).</p>
<p>The tensor's data is first copied from GPU memory to host memory for printing, and then the data is printed in a 2D matrix format. Each row of the tensor is printed on a new line, and each element in a row is separated by a space. Each row is enclosed in square brackets.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">os</td><td>The output stream to which the tensor will be printed. </td></tr>
    <tr><td class="paramname">tensor</td><td>The tensor whose contents will be printed. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The output stream (<code>os</code>) after the tensor has been printed, allowing for chaining of operations.</dd></dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator works by accessing the tensor's private data members (e.g., <code>_data</code>) directly.</li>
<li>The tensor's data is assumed to be in a valid state (i.e., properly allocated in GPU memory) before printing.</li>
<li>The function copies the tensor's data from device (GPU) memory to host (CPU) memory using <code>cudaMemcpy</code>, which may introduce performance overhead for large tensors.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line">std::cout &lt;&lt; tensor &lt;&lt; std::endl;  <span class="comment">// Prints the tensor to standard output in matrix format</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00037">37</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a40134aba93013e1b0d43c6fd5158d400" name="a40134aba93013e1b0d43c6fd5158d400"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a40134aba93013e1b0d43c6fd5158d400">&#9670;&#160;</a></span>operator&gt;&gt;() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::istream &amp; nz::data::operator&gt;&gt; </td>
          <td>(</td>
          <td class="paramtype">std::istream &amp;</td>          <td class="paramname"><span class="paramname"><em>is</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overloads the <code>&gt;&gt;</code> operator to read a tensor's data from an input stream. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the input stream operator (<code>&gt;&gt;</code>) to read the contents of a tensor from the specified input stream (e.g., <code>std::cin</code> or a file stream).</p>
<p>The function reads the tensor's data element by element from the input stream and stores the values in a temporary buffer. Once all the data has been read, it is copied from the host memory back into the tensor's GPU memory using <code>cudaMemcpy</code>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">is</td><td>The input stream from which the tensor's data will be read. </td></tr>
    <tr><td class="paramname">tensor</td><td>The tensor to which the data will be read. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The input stream (<code>is</code>) after reading the tensor's data, allowing for chaining of operations.</dd></dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator works by reading data from the input stream and storing it in a temporary buffer on the host.</li>
<li>The function assumes that the input data matches the size of the tensor. If the data is malformed or does not match, the behavior may be undefined.</li>
<li>After reading, the data is copied from host memory back into the tensor's GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">std::cin &gt;&gt; tensor;  <span class="comment">// Reads the tensor&#39;s data from standard input</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00095">95</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a4ea5e60f987ab3853b4d0af44453a9e2" name="a4ea5e60f987ab3853b4d0af44453a9e2"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4ea5e60f987ab3853b4d0af44453a9e2">&#9670;&#160;</a></span>operator&gt;&gt;() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::istream &amp; nz::data::operator&gt;&gt; </td>
          <td>(</td>
          <td class="paramtype">std::istream &amp;</td>          <td class="paramname"><span class="paramname"><em>is</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html">MappedTensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overload the &gt;&gt; operator to read data from an input stream into a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">is</td><td>An input stream (host-to-host) from which the data will be read. </td></tr>
    <tr><td class="paramname">tensor</td><td>A reference (host-to-host) to the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object where the data will be stored.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A reference to the input stream <code>is</code> after the reading operation.</dd></dl>
<p>This function provides a convenient way to populate a <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> object with data from an input stream. It iterates through the elements of the tensor and reads values from the input stream one by one, until either all elements of the tensor have been filled or the input stream fails to provide more data.</p>
<p>Memory management: The function does not allocate or deallocate any memory. It assumes that the <code>_data</code> array of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a> has already been allocated with the appropriate size (<code>_size</code>). Exception handling: If the input stream fails to provide data (e.g., due to end-of-file or an invalid input format), the loop will terminate, and the function will return the input stream in its current state. No exceptions are thrown by this function itself, but the <code>&gt;&gt;</code> operator on the input stream may throw exceptions depending on its implementation. Relationship with other components: This function is related to the data input component of the <a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a>. It integrates with the standard input stream to allow easy data population.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the size of the tensor (<code>_size</code>), as it iterates through each element of the tensor once.</li>
<li>Ensure that the input stream contains valid data in the correct format to avoid unexpected behavior.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line">nz::data::MappedTensor::shape_type shape = {2, 3};</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_mapped_tensor.html">nz::data::MappedTensor</a> tensor(shape, <span class="keyword">false</span>);</div>
<div class="line">std::istringstream iss(<span class="stringliteral">&quot;1 2 3 4 5 6&quot;</span>);</div>
<div class="line">iss &gt;&gt; tensor;</div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_mapped_tensor_8cu_source.html#l00079">79</a> of file <a class="el" href="_mapped_tensor_8cu_source.html">MappedTensor.cu</a>.</p>

</div>
</div>
<a id="a9bcee9a75db3d824b92ed17a59711428" name="a9bcee9a75db3d824b92ed17a59711428"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a9bcee9a75db3d824b92ed17a59711428">&#9670;&#160;</a></span>ReLU()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">T nz::data::ReLU </td>
          <td>(</td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>input</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Apply the Rectified Linear Unit (ReLU) activation function element-wise to an input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input tensor (either <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) to which the ReLU function will be applied (device to device).</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor (of the same type as the input: <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) with the ReLU function applied element-wise.</dd></dl>
<p>This function applies the ReLU activation function, defined as ( f(x) = \max(0, x) ), to each element of the input tensor. It first creates a new tensor <code>result</code> with the same shape and gradient requirement as the input tensor. Then, it calls the <code>iRELU</code> function to perform the actual ReLU operation on the data of the input tensor and store the results in the <code>result</code> tensor. Finally, the <code>result</code> tensor is returned.</p>
<p>Memory management: A new tensor <code>result</code> is created, and its memory is managed by the tensor's own class (<code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>). The memory of the input tensor remains unchanged. Exception handling: There is no explicit exception handling in this function. However, if the <code>iRELU</code> function or the tensor constructors throw exceptions, they will propagate up. Relationship with other components: This function depends on the <code>iRELU</code> function to perform the ReLU operation and the tensor's constructor to create a new tensor.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">[Exception</td><td>type thrown by iRELU or tensor constructors] If there are issues during the operation, such as memory allocation failures or incorrect input data.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the number of elements in the input tensor (<code>input.size()</code>), as it needs to apply the ReLU function to each element.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="comment">// Assume T is either Tensor or MappedTensor</span></div>
<div class="line">nz::data::T::shape_type shape = {2, 3};</div>
<div class="line">nz::data::T input(shape, <span class="keyword">true</span>);</div>
<div class="line">nz::data::T output = <a class="code hl_function" href="#a9bcee9a75db3d824b92ed17a59711428">ReLU</a>(input);</div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a9bcee9a75db3d824b92ed17a59711428"><div class="ttname"><a href="#a9bcee9a75db3d824b92ed17a59711428">nz::data::ReLU</a></div><div class="ttdeci">T ReLU(T input)</div><div class="ttdoc">Apply the Rectified Linear Unit (ReLU) activation function element-wise to an input tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00037">TensorOperations.cuh:37</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_operations_8cuh_source.html#l00037">37</a> of file <a class="el" href="_tensor_operations_8cuh_source.html">TensorOperations.cuh</a>.</p>

</div>
</div>
<a id="a0740e60918114b0a4d7c88d09aeb8f88" name="a0740e60918114b0a4d7c88d09aeb8f88"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0740e60918114b0a4d7c88d09aeb8f88">&#9670;&#160;</a></span>Sigmoid()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">T nz::data::Sigmoid </td>
          <td>(</td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>input</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Apply the sigmoid activation function element-wise to an input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input tensor (either <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) to which the sigmoid function will be applied (device-to-device).</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor (of the same type as the input: <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) with the sigmoid function applied element-wise.</dd></dl>
<p>This function applies the sigmoid activation function, defined as ( f(x)=\frac{1}{1 + e^{-x}} ), to each element of the input tensor. It first creates a new tensor <code>result</code> with the same shape and gradient requirement as the input tensor. Then, it calls the <code>iSigmoid</code> function to perform the actual sigmoid operation on the data of the input tensor and store the results in the <code>result</code> tensor. Finally, the <code>result</code> tensor is returned.</p>
<p>Memory management: A new tensor <code>result</code> is created, and its memory is managed by the tensor's own class (<code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>). The memory of the input tensor remains unchanged. Exception handling: There is no explicit exception handling in this function. However, if the <code>iSigmoid</code> function or the tensor constructors throw exceptions, they will propagate up. Relationship with other components: This function depends on the <code>iSigmoid</code> function to perform the sigmoid operation and the tensor's constructor to create a new tensor.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">[Exception</td><td>type thrown by iSigmoid or tensor constructors] If there are issues during the operation, such as memory allocation failures or incorrect input data.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the number of elements in the input tensor (<code>input.size()</code>), as it needs to apply the sigmoid function to each element.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="comment">// Assume T is either Tensor or MappedTensor</span></div>
<div class="line">nz::data::T::shape_type shape = {2, 3};</div>
<div class="line">nz::data::T input(shape, <span class="keyword">true</span>);</div>
<div class="line">nz::data::T output = <a class="code hl_function" href="#a0740e60918114b0a4d7c88d09aeb8f88">Sigmoid</a>(input);</div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a0740e60918114b0a4d7c88d09aeb8f88"><div class="ttname"><a href="#a0740e60918114b0a4d7c88d09aeb8f88">nz::data::Sigmoid</a></div><div class="ttdeci">T Sigmoid(T input)</div><div class="ttdoc">Apply the sigmoid activation function element-wise to an input tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00074">TensorOperations.cuh:74</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_operations_8cuh_source.html#l00074">74</a> of file <a class="el" href="_tensor_operations_8cuh_source.html">TensorOperations.cuh</a>.</p>

</div>
</div>
<a id="a09849b685c23664bf02b0f824006f695" name="a09849b685c23664bf02b0f824006f695"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a09849b685c23664bf02b0f824006f695">&#9670;&#160;</a></span>Softmax() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::Softmax </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Softmax activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Softmax activation function element-wise to the given tensor. The Softmax function converts the tensor into a probability distribution, where each element is transformed into a value between 0 and 1, and the sum of all elements in the tensor equals 1. The Softmax function is commonly used in the output layer of neural networks for multi-class classification tasks.</p>
<p>The Softmax function for each element <code>x_i</code> in the tensor is computed as: </p><pre class="fragment">Softmax(x_i) = exp(x_i) / sum(exp(x_j) for all j)
</pre><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Softmax activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The input tensor with the Softmax function applied element-wise.</dd></dl>
<p>This function uses a CUDA kernel (<code>SummationExp</code>) to compute the sum of the exponentiated values in parallel on the GPU, and then another kernel (<code>Softmax</code>) to apply the Softmax transformation. The result is stored in the original tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator modifies the original tensor by applying the Softmax transformation in-place.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
<li>The Softmax computation is performed in two stages: first by calculating the exponentiated values' sum, then applying the Softmax transformation.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#aaba025895b2c8b69de8d36c73c74929e">fill</a>(1.0f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#ad6092c1138e5093c07ec00c12c72314a">Softmax</a>(tensor);  <span class="comment">// Apply the Softmax activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_ad6092c1138e5093c07ec00c12c72314a"><div class="ttname"><a href="#ad6092c1138e5093c07ec00c12c72314a">nz::data::Softmax</a></div><div class="ttdeci">T Softmax(T input)</div><div class="ttdoc">Compute the softmax function for a given input of type T.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00342">TensorOperations.cuh:342</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00411">411</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ad6092c1138e5093c07ec00c12c72314a" name="ad6092c1138e5093c07ec00c12c72314a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad6092c1138e5093c07ec00c12c72314a">&#9670;&#160;</a></span>Softmax() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">T nz::data::Softmax </td>
          <td>(</td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>input</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the softmax function for a given input of type T. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input object of type T for which the softmax function will be computed. The input is passed by value, so a copy of the input is made inside the function.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>An object of type T representing the result of the softmax function applied to the input.</dd></dl>
<p>This function computes the softmax function for the given input. It first creates a new object <code>result</code> with the same shape and gradient requirement as the input. Then, it calls the <code>iSoftmax</code> function to perform the actual softmax computation. The <code>iSoftmax</code> function takes the data pointers of the result and input, the exponential sum of the input, and the size of the input as parameters. Finally, the computed result is returned.</p>
<p>Memory management:</p><ul>
<li>A new object <code>result</code> is created inside the function, which may allocate memory depending on the implementation of the constructor of type T. The memory for the result will be managed by the destructor of the object when it goes out of scope.</li>
</ul>
<p>Exception handling:</p><ul>
<li>There is no explicit exception handling in this function. However, if the <code>iSoftmax</code> function or the constructor of type T throws an exception, it will propagate up to the caller.</li>
</ul>
<p>Relationship with other components:</p><ul>
<li>This function depends on the <code>iSoftmax</code> function to perform the actual softmax computation.</li>
<li>It also depends on the <code>shape()</code>, <code>requiresGrad()</code>, <code>expSum()</code>, and <code>size()</code> member functions of type T.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function depends on the implementation of the <code>iSoftmax</code> function. If the <code>iSoftmax</code> function has a time complexity of O(n), where n is the size of the input, then the overall time complexity of this function is also O(n).</li>
<li>Ensure that the input object <code>input</code> has valid shape, gradient requirement, exponential sum, and size information.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="comment">// Assume Tensor is a valid type with shape(), requiresGrad(), expSum(), and size() member functions</span></div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">nz::data::Tensor</a> input({2, 3}, <span class="keyword">true</span>);</div>
<div class="line"><span class="comment">// Assume input is filled with some values</span></div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">nz::data::Tensor</a> result = <a class="code hl_function" href="#ad6092c1138e5093c07ec00c12c72314a">Softmax</a>(input);</div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_operations_8cuh_source.html#l00342">342</a> of file <a class="el" href="_tensor_operations_8cuh_source.html">TensorOperations.cuh</a>.</p>

</div>
</div>
<a id="a42cbe5a414e4212ee7a26b0a66ce08a4" name="a42cbe5a414e4212ee7a26b0a66ce08a4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a42cbe5a414e4212ee7a26b0a66ce08a4">&#9670;&#160;</a></span>Swish()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">T nz::data::Swish </td>
          <td>(</td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>input</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Apply the Swish activation function element-wise to an input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input tensor (either <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) to which the Swish function will be applied (device-to-device).</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor (of the same type as the input: <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) with the Swish function applied element-wise.</dd></dl>
<p>This function applies the Swish activation function, defined as ( f(x)=x\cdot\sigma(x) ), where (\sigma(x)=\frac{1}{1 + e^{-x}}) is the sigmoid function, to each element of the input tensor. It first creates a new tensor <code>result</code> with the same shape and gradient requirement as the input tensor. Then, it calls the <code>iSwish</code> function to perform the actual Swish operation on the data of the input tensor and store the results in the <code>result</code> tensor. Finally, the <code>result</code> tensor is returned.</p>
<p>Memory management: A new tensor <code>result</code> is created, and its memory is managed by the tensor's own class (<code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>). The memory of the input tensor remains unchanged. Exception handling: There is no explicit exception handling in this function. However, if the <code>iSwish</code> function or the tensor constructors throw exceptions, they will propagate up. Relationship with other components: This function depends on the <code>iSwish</code> function to perform the Swish operation and the tensor's constructor to create a new tensor.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">[Exception</td><td>type thrown by iSwish or tensor constructors] If there are issues during the operation, such as memory allocation failures or incorrect input data.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the number of elements in the input tensor (<code>input.size()</code>), as it needs to apply the Swish function to each element.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="comment">// Assume T is either Tensor or MappedTensor</span></div>
<div class="line">nz::data::T::shape_type shape = {2, 3};</div>
<div class="line">nz::data::T input(shape, <span class="keyword">true</span>);</div>
<div class="line">nz::data::T output = <a class="code hl_function" href="#a42cbe5a414e4212ee7a26b0a66ce08a4">Swish</a>(input);</div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a42cbe5a414e4212ee7a26b0a66ce08a4"><div class="ttname"><a href="#a42cbe5a414e4212ee7a26b0a66ce08a4">nz::data::Swish</a></div><div class="ttdeci">T Swish(T input)</div><div class="ttdoc">Apply the Swish activation function element-wise to an input tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00185">TensorOperations.cuh:185</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_operations_8cuh_source.html#l00185">185</a> of file <a class="el" href="_tensor_operations_8cuh_source.html">TensorOperations.cuh</a>.</p>

</div>
</div>
<a id="a56c9ff66e41ae568a2fec83708c4dd85" name="a56c9ff66e41ae568a2fec83708c4dd85"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a56c9ff66e41ae568a2fec83708c4dd85">&#9670;&#160;</a></span>Tanh()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">T nz::data::Tanh </td>
          <td>(</td>
          <td class="paramtype">T</td>          <td class="paramname"><span class="paramname"><em>input</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Apply the hyperbolic tangent (tanh) activation function element-wise to an input tensor. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>The input tensor (either <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) to which the tanh function will be applied (device-to-device).</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor (of the same type as the input: <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>) with the tanh function applied element-wise.</dd></dl>
<p>This function applies the hyperbolic tangent activation function, defined as ( f(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} ), to each element of the input tensor. It first creates a new tensor <code>result</code> with the same shape and gradient requirement as the input tensor. Then, it calls the <code>iTanh</code> function to perform the actual tanh operation on the data of the input tensor and store the results in the <code>result</code> tensor. Finally, the <code>result</code> tensor is returned.</p>
<p>Memory management: A new tensor <code>result</code> is created, and its memory is managed by the tensor's own class (<code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> or <code><a class="el" href="classnz_1_1data_1_1_mapped_tensor.html" title="A class for representing multidimensional arrays in CUDA zero-copy memory, providing host-accessible ...">MappedTensor</a></code>). The memory of the input tensor remains unchanged. Exception handling: There is no explicit exception handling in this function. However, if the <code>iTanh</code> function or the tensor constructors throw exceptions, they will propagate up. Relationship with other components: This function depends on the <code>iTanh</code> function to perform the tanh operation and the tensor's constructor to create a new tensor.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">[Exception</td><td>type thrown by iTanh or tensor constructors] If there are issues during the operation, such as memory allocation failures or incorrect input data.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The time complexity of this function is O(n), where n is the number of elements in the input tensor (<code>input.size()</code>), as it needs to apply the tanh function to each element.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="comment">// Assume T is either Tensor or MappedTensor</span></div>
<div class="line">nz::data::T::shape_type shape = {2, 3};</div>
<div class="line">nz::data::T input(shape, <span class="keyword">true</span>);</div>
<div class="line">nz::data::T output = <a class="code hl_function" href="#a56c9ff66e41ae568a2fec83708c4dd85">Tanh</a>(input);</div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a56c9ff66e41ae568a2fec83708c4dd85"><div class="ttname"><a href="#a56c9ff66e41ae568a2fec83708c4dd85">nz::data::Tanh</a></div><div class="ttdeci">T Tanh(T input)</div><div class="ttdoc">Apply the hyperbolic tangent (tanh) activation function element-wise to an input tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00111">TensorOperations.cuh:111</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_operations_8cuh_source.html#l00111">111</a> of file <a class="el" href="_tensor_operations_8cuh_source.html">TensorOperations.cuh</a>.</p>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
