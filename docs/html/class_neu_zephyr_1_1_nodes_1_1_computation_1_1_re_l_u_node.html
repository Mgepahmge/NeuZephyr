<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NeuZephyr: NeuZephyr::Nodes::Computation::ReLUNode Class Reference</title>
<link rel="icon" href="NZ_logo2.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="NZ_logo2.png"/></td>
  <td id="projectalign">
   <div id="projectname">NeuZephyr
   </div>
   <div id="projectbrief">Simple DL Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="inherits.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>NeuZephyr</b></li><li class="navelem"><a class="el" href="namespace_neu_zephyr_1_1_nodes.html">Nodes</a></li><li class="navelem"><a class="el" href="namespace_neu_zephyr_1_1_nodes_1_1_computation.html">Computation</a></li><li class="navelem"><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node.html">ReLUNode</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">NeuZephyr::Nodes::Computation::ReLUNode Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.  
 <a href="#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for NeuZephyr::Nodes::Computation::ReLUNode:</div>
<div class="dyncontent">
<div class="center"><img src="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node__inherit__graph.png" border="0" usemap="#a_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_inherit__map" alt="Inheritance graph"/></div>
<map name="a_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_inherit__map" id="a_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_inherit__map">
<area shape="rect" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph." alt="" coords="5,80,219,123"/>
<area shape="rect" href="class_neu_zephyr_1_1_nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph." alt="" coords="27,5,197,32"/>
<area shape="poly" title=" " alt="" coords="115,48,115,80,109,80,109,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for NeuZephyr::Nodes::Computation::ReLUNode:</div>
<div class="dyncontent">
<div class="center"><img src="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node__coll__graph.png" border="0" usemap="#a_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_coll__map" alt="Collaboration graph"/></div>
<map name="a_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_coll__map" id="a_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_coll__map">
<area shape="rect" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph." alt="" coords="5,80,219,123"/>
<area shape="rect" href="class_neu_zephyr_1_1_nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph." alt="" coords="27,5,197,32"/>
<area shape="poly" title=" " alt="" coords="115,48,115,80,109,80,109,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a8b0685300b15a20fbd7bc0fee2e5e74e" id="r_a8b0685300b15a20fbd7bc0fee2e5e74e"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a8b0685300b15a20fbd7bc0fee2e5e74e">ReLUNode</a> (<a class="el" href="class_neu_zephyr_1_1_nodes_1_1_node.html">Node</a> *input)</td></tr>
<tr class="memdesc:a8b0685300b15a20fbd7bc0fee2e5e74e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructor to initialize a <code><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> for applying the ReLU activation function.  <br /></td></tr>
<tr class="separator:a8b0685300b15a20fbd7bc0fee2e5e74e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a863c16e3b4334c78813e9e5524a609a8" id="r_a863c16e3b4334c78813e9e5524a609a8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a863c16e3b4334c78813e9e5524a609a8">forward</a> () override</td></tr>
<tr class="memdesc:a863c16e3b4334c78813e9e5524a609a8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Forward pass for the <code><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> to apply the ReLU activation function.  <br /></td></tr>
<tr class="separator:a863c16e3b4334c78813e9e5524a609a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a357cecf49efa717a00112f76ac9ec5be" id="r_a357cecf49efa717a00112f76ac9ec5be"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a357cecf49efa717a00112f76ac9ec5be">backward</a> () override</td></tr>
<tr class="memdesc:a357cecf49efa717a00112f76ac9ec5be"><td class="mdescLeft">&#160;</td><td class="mdescRight">Backward pass for the <code><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> to compute gradients.  <br /></td></tr>
<tr class="separator:a357cecf49efa717a00112f76ac9ec5be"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_class_neu_zephyr_1_1_nodes_1_1_node"><td colspan="2" onclick="javascript:dynsection.toggleInherit('pub_methods_class_neu_zephyr_1_1_nodes_1_1_node')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="class_neu_zephyr_1_1_nodes_1_1_node.html">NeuZephyr::Nodes::Node</a></td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Represents a Rectified Linear Unit (ReLU) operation node in a computational graph. </p>
<p>The <code><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> class applies the ReLU activation function to the input tensor. ReLU is a commonly used non-linear activation function in neural networks, defined as <code>ReLU(x) = max(0, x)</code>. It introduces non-linearity and sparsity into the network.</p>
<p>Key features:</p><ul>
<li><b>Forward Pass</b>: Applies the ReLU activation function element-wise to the input tensor. Values less than zero are set to zero, while non-negative values remain unchanged.</li>
<li><b>Backward Pass</b>: Computes the gradient of the loss with respect to the input tensor. Gradients are passed through unchanged for positive input values and set to zero for negative input values.</li>
<li><b>Shape Preservation</b>: The output tensor has the same shape as the input tensor.</li>
<li><b>Gradient Management</b>: Automatically tracks gradients if required by the input tensor.</li>
</ul>
<p>This class is part of the <code><a class="el" href="namespace_neu_zephyr_1_1_nodes.html" title="Contains classes and functionality for nodes in a neural network or computational graph.">NeuZephyr::Nodes</a></code> namespace and is typically used in constructing neural network models to introduce non-linearity between layers.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The input tensor's shape is preserved in the output tensor.</li>
<li>Gradients are efficiently computed using CUDA kernels.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="autotoc_md46"></a>
Usage Example:</h3>
<div class="fragment"><div class="line"><span class="comment">// Example: Using ReLUNode in a computational graph</span></div>
<div class="line">InputNode input({3, 3}, <span class="keyword">true</span>);  <span class="comment">// Create an input node with shape {3, 3}</span></div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">float</span> data[] = {-1.0f, 0.0f, 1.0f, 2.0f, -2.0f, 3.0f, -3.0f, 4.0f, -4.0f};  <span class="comment">// Sample input values</span></div>
<div class="line">input.output-&gt;copyData(data);  <span class="comment">// Copy data to the input tensor</span></div>
<div class="line"> </div>
<div class="line"><a class="code hl_function" href="#a8b0685300b15a20fbd7bc0fee2e5e74e">ReLUNode</a> relu_node(&amp;input);  <span class="comment">// Apply ReLU activation</span></div>
<div class="line">relu_node.forward();  <span class="comment">// Perform the forward pass</span></div>
<div class="line">relu_node.backward();  <span class="comment">// Propagate gradients in the backward pass</span></div>
<div class="line"> </div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Output: &quot;</span> &lt;&lt; *relu_node.output &lt;&lt; std::endl;  <span class="comment">// Print the result</span></div>
<div class="ttc" id="aclass_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_html_a8b0685300b15a20fbd7bc0fee2e5e74e"><div class="ttname"><a href="#a8b0685300b15a20fbd7bc0fee2e5e74e">NeuZephyr::Nodes::Computation::ReLUNode::ReLUNode</a></div><div class="ttdeci">ReLUNode(Node *input)</div><div class="ttdoc">Constructor to initialize a ReLUNode for applying the ReLU activation function.</div><div class="ttdef"><b>Definition</b> <a href="_nodes_8cu_source.html#l00269">Nodes.cu:269</a></div></div>
</div><!-- fragment --><dl class="section see"><dt>See also</dt><dd><a class="el" href="#a863c16e3b4334c78813e9e5524a609a8" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward()</a> for the <a class="el" href="namespace_neu_zephyr_1_1_data.html#ac498882faa00ab9a8fbd7eac6783cee0" title="Applies the Rectified Linear Unit (ReLU) activation function to a tensor.">ReLU</a> activation computation in the <a class="el" href="#a863c16e3b4334c78813e9e5524a609a8" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward</a> pass. </dd>
<dd>
<a class="el" href="#a357cecf49efa717a00112f76ac9ec5be" title="Backward pass for the ReLUNode to compute gradients.">backward()</a> for gradient computation in the <a class="el" href="#a357cecf49efa717a00112f76ac9ec5be" title="Backward pass for the ReLUNode to compute gradients.">backward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_nodes_8cuh_source.html#l01652">1652</a> of file <a class="el" href="_nodes_8cuh_source.html">Nodes.cuh</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a8b0685300b15a20fbd7bc0fee2e5e74e" name="a8b0685300b15a20fbd7bc0fee2e5e74e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8b0685300b15a20fbd7bc0fee2e5e74e">&#9670;&#160;</a></span>ReLUNode()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">NeuZephyr::Nodes::Computation::ReLUNode::ReLUNode </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_node.html">Node</a> *</td>          <td class="paramname"><span class="paramname"><em>input</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">explicit</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Constructor to initialize a <code><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> for applying the ReLU activation function. </p>
<p>The constructor initializes a <code><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code>, which applies the Rectified Linear Unit (ReLU) activation function to an input tensor. It establishes a connection to the input node, initializes the output tensor, and sets the type of the node to "ReLU".</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>A pointer to the input node. Its <code>output</code> tensor will have the ReLU activation applied.</td></tr>
  </table>
  </dd>
</dl>
<ul>
<li>The input node is added to the <code>inputs</code> vector to establish the connection in the computational graph.</li>
<li>The <code>output</code> tensor is initialized with the same shape as the input tensor, and its gradient tracking is determined based on the input tensor's requirements.</li>
<li>The node's type is set to "ReLU" to reflect its operation.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The ReLU activation is defined as <code>ReLU(x) = max(0, x)</code>. This will be applied during the forward pass.</li>
<li>This node supports automatic gradient tracking if the input tensor requires gradients.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#a863c16e3b4334c78813e9e5524a609a8" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward()</a> for the <a class="el" href="#a863c16e3b4334c78813e9e5524a609a8" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward</a> pass implementation. </dd>
<dd>
<a class="el" href="#a357cecf49efa717a00112f76ac9ec5be" title="Backward pass for the ReLUNode to compute gradients.">backward()</a> for gradient propagation in the <a class="el" href="#a357cecf49efa717a00112f76ac9ec5be" title="Backward pass for the ReLUNode to compute gradients.">backward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00269">269</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="a357cecf49efa717a00112f76ac9ec5be" name="a357cecf49efa717a00112f76ac9ec5be"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a357cecf49efa717a00112f76ac9ec5be">&#9670;&#160;</a></span>backward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void NeuZephyr::Nodes::Computation::ReLUNode::backward </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Backward pass for the <code><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> to compute gradients. </p>
<p>The <code><a class="el" href="#a357cecf49efa717a00112f76ac9ec5be" title="Backward pass for the ReLUNode to compute gradients.">backward()</a></code> method computes the gradient of the loss with respect to the input tensor by applying the derivative of the ReLU activation function. Gradients are propagated only for elements where the input tensor values are positive; otherwise, the gradients are set to zero.</p>
<ul>
<li>A CUDA kernel (<code>ReLUBackward</code>) is launched to compute the gradients in parallel on the GPU.</li>
<li>The grid and block dimensions are calculated dynamically based on the size of the <code>output</code> tensor.</li>
<li>The derivative of ReLU is defined as: <div class="fragment"><div class="line"><a class="code hl_function" href="namespace_neu_zephyr_1_1_data.html#ac498882faa00ab9a8fbd7eac6783cee0">ReLU</a><span class="stringliteral">&#39;(x) = 1, if x &gt; 0</span></div>
<div class="line"><span class="stringliteral">          0, if x &lt;= 0</span></div>
<div class="ttc" id="anamespace_neu_zephyr_1_1_data_html_ac498882faa00ab9a8fbd7eac6783cee0"><div class="ttname"><a href="namespace_neu_zephyr_1_1_data.html#ac498882faa00ab9a8fbd7eac6783cee0">NeuZephyr::Data::ReLU</a></div><div class="ttdeci">Tensor ReLU(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Rectified Linear Unit (ReLU) activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00379">Tensor.cu:379</a></div></div>
</div><!-- fragment --> This derivative is applied element-wise to propagate gradients through the ReLU operation.</li>
<li>Gradients are accumulated in the gradient tensor of the input node.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Gradients are only propagated if the input tensor's <code>requiresGrad</code> property is true.</li>
<li>The shape of the gradient tensor matches the shape of the input tensor.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#a863c16e3b4334c78813e9e5524a609a8" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward()</a> for the <a class="el" href="namespace_neu_zephyr_1_1_data.html#ac498882faa00ab9a8fbd7eac6783cee0" title="Applies the Rectified Linear Unit (ReLU) activation function to a tensor.">ReLU</a> activation computation in the <a class="el" href="#a863c16e3b4334c78813e9e5524a609a8" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p>Implements <a class="el" href="class_neu_zephyr_1_1_nodes_1_1_node.html#a41914155871c84330701f9d1649b39f3">NeuZephyr::Nodes::Node</a>.</p>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00282">282</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_a357cecf49efa717a00112f76ac9ec5be_cgraph.png" border="0" usemap="#aclass_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_a357cecf49efa717a00112f76ac9ec5be_cgraph" alt=""/></div>
<map name="aclass_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_a357cecf49efa717a00112f76ac9ec5be_cgraph" id="aclass_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_a357cecf49efa717a00112f76ac9ec5be_cgraph">
<area shape="rect" title="Backward pass for the ReLUNode to compute gradients." alt="" coords="5,5,219,48"/>
<area shape="rect" href="namespace_neu_zephyr_1_1_kernels.html#afe3c35a4b83480aa41e7f550e4176063" title="Kernel function to compute the gradient of the ReLU activation during backpropagation." alt="" coords="267,5,404,48"/>
<area shape="poly" title=" " alt="" coords="219,24,251,24,251,29,219,29"/>
</map>
</div>

</div>
</div>
<a id="a863c16e3b4334c78813e9e5524a609a8" name="a863c16e3b4334c78813e9e5524a609a8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a863c16e3b4334c78813e9e5524a609a8">&#9670;&#160;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void NeuZephyr::Nodes::Computation::ReLUNode::forward </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Forward pass for the <code><a class="el" href="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> to apply the ReLU activation function. </p>
<p>The <code><a class="el" href="#a863c16e3b4334c78813e9e5524a609a8" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward()</a></code> method applies the Rectified Linear Unit (ReLU) activation function element-wise to the input tensor. Values less than zero are set to zero, while non-negative values remain unchanged. The results are stored in the <code>output</code> tensor.</p>
<ul>
<li>A CUDA kernel (<code>RectifiedLinearUnit</code>) is launched to compute the ReLU activation in parallel on the GPU.</li>
<li>The grid and block dimensions are calculated dynamically based on the size of the <code>output</code> tensor to optimize GPU performance.</li>
<li>The output tensor stores the result of applying <code>ReLU(x) = max(0, x)</code> for each element of the input tensor.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The shape of the output tensor matches that of the input tensor.</li>
<li>Ensure the input tensor is properly initialized before calling this method.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#a357cecf49efa717a00112f76ac9ec5be" title="Backward pass for the ReLUNode to compute gradients.">backward()</a> for gradient computation in the <a class="el" href="#a357cecf49efa717a00112f76ac9ec5be" title="Backward pass for the ReLUNode to compute gradients.">backward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p>Implements <a class="el" href="class_neu_zephyr_1_1_nodes_1_1_node.html#a64e42ba40199e35bfe453ef14b2d15c0">NeuZephyr::Nodes::Node</a>.</p>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00276">276</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="class_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_a863c16e3b4334c78813e9e5524a609a8_cgraph.png" border="0" usemap="#aclass_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_a863c16e3b4334c78813e9e5524a609a8_cgraph" alt=""/></div>
<map name="aclass_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_a863c16e3b4334c78813e9e5524a609a8_cgraph" id="aclass_neu_zephyr_1_1_nodes_1_1_computation_1_1_re_l_u_node_a863c16e3b4334c78813e9e5524a609a8_cgraph">
<area shape="rect" title="Forward pass for the ReLUNode to apply the ReLU activation function." alt="" coords="5,5,219,48"/>
<area shape="rect" href="namespace_neu_zephyr_1_1_kernels.html#a0eb24e2329f5d2f9b175e40df5d52d6c" title="Kernel function to apply the Rectified Linear Unit (ReLU) activation on the GPU." alt="" coords="267,5,407,48"/>
<area shape="poly" title=" " alt="" coords="219,24,251,24,251,29,219,29"/>
</map>
</div>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>D:/C Program/Simple-CPP-DL-Framework/include/NeuZephyr/<a class="el" href="_nodes_8cuh_source.html">Nodes.cuh</a></li>
<li>D:/C Program/Simple-CPP-DL-Framework/src/<a class="el" href="_nodes_8cu_source.html">Nodes.cu</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
