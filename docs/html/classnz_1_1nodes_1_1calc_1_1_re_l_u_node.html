<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NeuZephyr: nz::nodes::calc::ReLUNode Class Reference</title>
<link rel="icon" href="NZ_logo2.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="NZ_logo2.png"/></td>
  <td id="projectalign">
   <div id="projectname">NeuZephyr
   </div>
   <div id="projectbrief">Simple DL Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="inherits.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>nz</b></li><li class="navelem"><a class="el" href="namespacenz_1_1nodes.html">nodes</a></li><li class="navelem"><a class="el" href="namespacenz_1_1nodes_1_1calc.html">calc</a></li><li class="navelem"><a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html">ReLUNode</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">nz::nodes::calc::ReLUNode Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.  
 <a href="#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for nz::nodes::calc::ReLUNode:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1nodes_1_1calc_1_1_re_l_u_node__inherit__graph.png" border="0" usemap="#anz_1_1nodes_1_1calc_1_1_re_l_u_node_inherit__map" alt="Inheritance graph"/></div>
<map name="anz_1_1nodes_1_1calc_1_1_re_l_u_node_inherit__map" id="anz_1_1nodes_1_1calc_1_1_re_l_u_node_inherit__map">
<area shape="rect" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph." alt="" coords="5,80,189,107"/>
<area shape="rect" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph." alt="" coords="38,5,156,32"/>
<area shape="poly" title=" " alt="" coords="100,48,100,80,94,80,94,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for nz::nodes::calc::ReLUNode:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1nodes_1_1calc_1_1_re_l_u_node__coll__graph.png" border="0" usemap="#anz_1_1nodes_1_1calc_1_1_re_l_u_node_coll__map" alt="Collaboration graph"/></div>
<map name="anz_1_1nodes_1_1calc_1_1_re_l_u_node_coll__map" id="anz_1_1nodes_1_1calc_1_1_re_l_u_node_coll__map">
<area shape="rect" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph." alt="" coords="5,80,189,107"/>
<area shape="rect" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph." alt="" coords="38,5,156,32"/>
<area shape="poly" title=" " alt="" coords="100,48,100,80,94,80,94,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a78a8e2e5cf13d31956e2367d923efa53" id="r_a78a8e2e5cf13d31956e2367d923efa53"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a78a8e2e5cf13d31956e2367d923efa53">ReLUNode</a> (<a class="el" href="classnz_1_1nodes_1_1_node.html">Node</a> *input)</td></tr>
<tr class="memdesc:a78a8e2e5cf13d31956e2367d923efa53"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructor to initialize a <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> for applying the ReLU activation function.  <br /></td></tr>
<tr class="separator:a78a8e2e5cf13d31956e2367d923efa53"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae8e78ee766f1e4845c6a110464e077f7" id="r_ae8e78ee766f1e4845c6a110464e077f7"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ae8e78ee766f1e4845c6a110464e077f7">forward</a> () override</td></tr>
<tr class="memdesc:ae8e78ee766f1e4845c6a110464e077f7"><td class="mdescLeft">&#160;</td><td class="mdescRight">Forward pass for the <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> to apply the ReLU activation function.  <br /></td></tr>
<tr class="separator:ae8e78ee766f1e4845c6a110464e077f7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af06dd1eadec4b3616c7c9e655820b1b8" id="r_af06dd1eadec4b3616c7c9e655820b1b8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af06dd1eadec4b3616c7c9e655820b1b8">backward</a> () override</td></tr>
<tr class="memdesc:af06dd1eadec4b3616c7c9e655820b1b8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Backward pass for the <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> to compute gradients.  <br /></td></tr>
<tr class="separator:af06dd1eadec4b3616c7c9e655820b1b8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classnz_1_1nodes_1_1_node"><td colspan="2" onclick="javascript:dynsection.toggleInherit('pub_methods_classnz_1_1nodes_1_1_node')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classnz_1_1nodes_1_1_node.html">nz::nodes::Node</a></td></tr>
<tr class="memitem:a687ee9c34eb61f8f28caa201ca42696e inherit pub_methods_classnz_1_1nodes_1_1_node" id="r_a687ee9c34eb61f8f28caa201ca42696e"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1nodes_1_1_node.html#a687ee9c34eb61f8f28caa201ca42696e">print</a> (std::ostream &amp;os) const</td></tr>
<tr class="memdesc:a687ee9c34eb61f8f28caa201ca42696e inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="mdescLeft">&#160;</td><td class="mdescRight">Prints the type, data, and gradient of the node.  <br /></td></tr>
<tr class="separator:a687ee9c34eb61f8f28caa201ca42696e inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9b85913e12422bb4ac2fff483427bb47 inherit pub_methods_classnz_1_1nodes_1_1_node" id="r_a9b85913e12422bb4ac2fff483427bb47"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1nodes_1_1_node.html#a9b85913e12422bb4ac2fff483427bb47">dataInject</a> (Tensor::value_type *data, bool grad=false) const</td></tr>
<tr class="memdesc:a9b85913e12422bb4ac2fff483427bb47 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="mdescLeft">&#160;</td><td class="mdescRight">Injects data into a relevant tensor object, optionally setting its gradient requirement.  <br /></td></tr>
<tr class="separator:a9b85913e12422bb4ac2fff483427bb47 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a609f1730085dd1d31e0ddcbbae48a065 inherit pub_methods_classnz_1_1nodes_1_1_node" id="r_a609f1730085dd1d31e0ddcbbae48a065"><td class="memTemplParams" colspan="2">template&lt;typename Iterator &gt; </td></tr>
<tr class="memitem:a609f1730085dd1d31e0ddcbbae48a065 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="classnz_1_1nodes_1_1_node.html#a609f1730085dd1d31e0ddcbbae48a065">dataInject</a> (Iterator begin, Iterator end, const bool grad=false) const</td></tr>
<tr class="memdesc:a609f1730085dd1d31e0ddcbbae48a065 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="mdescLeft">&#160;</td><td class="mdescRight">Injects data from an iterator range into the output tensor of the InputNode, optionally setting its gradient requirement.  <br /></td></tr>
<tr class="separator:a609f1730085dd1d31e0ddcbbae48a065 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8b4bab3271df92ca1f0914f7a97b1e8 inherit pub_methods_classnz_1_1nodes_1_1_node" id="r_af8b4bab3271df92ca1f0914f7a97b1e8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1nodes_1_1_node.html#af8b4bab3271df92ca1f0914f7a97b1e8">dataInject</a> (const std::initializer_list&lt; Tensor::value_type &gt; &amp;data, bool grad=false) const</td></tr>
<tr class="memdesc:af8b4bab3271df92ca1f0914f7a97b1e8 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="mdescLeft">&#160;</td><td class="mdescRight">Injects data from a std::initializer_list into the output tensor of the <a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a>, optionally setting its gradient requirement.  <br /></td></tr>
<tr class="separator:af8b4bab3271df92ca1f0914f7a97b1e8 inherit pub_methods_classnz_1_1nodes_1_1_node"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Represents a Rectified Linear Unit (ReLU) operation node in a computational graph. </p>
<p>The <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> class applies the ReLU activation function to the input tensor. ReLU is a commonly used non-linear activation function in neural networks, defined as <code>ReLU(x) = max(0, x)</code>. It introduces non-linearity and sparsity into the network.</p>
<p>Key features:</p><ul>
<li><b>Forward Pass</b>: Applies the ReLU activation function element-wise to the input tensor. Values less than zero are set to zero, while non-negative values remain unchanged.</li>
<li><b>Backward Pass</b>: Computes the gradient of the loss with respect to the input tensor. Gradients are passed through unchanged for positive input values and set to zero for negative input values.</li>
<li><b>Shape Preservation</b>: The output tensor has the same shape as the input tensor.</li>
<li><b>Gradient Management</b>: Automatically tracks gradients if required by the input tensor.</li>
</ul>
<p>This class is part of the <code><a class="el" href="namespacenz_1_1nodes.html" title="Contains classes and functionality for nodes in a neural network or computational graph.">nz::nodes</a></code> namespace and is typically used in constructing neural network models to introduce non-linearity between layers.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The input tensor's shape is preserved in the output tensor.</li>
<li>Gradients are efficiently computed using CUDA kernels.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="autotoc_md96"></a>
Usage Example:</h3>
<div class="fragment"><div class="line"><span class="comment">// Example: Using ReLUNode in a computational graph</span></div>
<div class="line">InputNode input({3, 3}, <span class="keyword">true</span>);  <span class="comment">// Create an input node with shape {3, 3}</span></div>
<div class="line"> </div>
<div class="line"><span class="keywordtype">float</span> data[] = {-1.0f, 0.0f, 1.0f, 2.0f, -2.0f, 3.0f, -3.0f, 4.0f, -4.0f};  <span class="comment">// Sample input values</span></div>
<div class="line">input.output-&gt;dataInject(data);  <span class="comment">// Copy data to the input tensor</span></div>
<div class="line"> </div>
<div class="line"><a class="code hl_function" href="#a78a8e2e5cf13d31956e2367d923efa53">ReLUNode</a> relu_node(&amp;input);  <span class="comment">// Apply ReLU activation</span></div>
<div class="line">relu_node.forward();  <span class="comment">// Perform the forward pass</span></div>
<div class="line">relu_node.backward();  <span class="comment">// Propagate gradients in the backward pass</span></div>
<div class="line"> </div>
<div class="line">std::cout &lt;&lt; <span class="stringliteral">&quot;Output: &quot;</span> &lt;&lt; *relu_node.output &lt;&lt; std::endl;  <span class="comment">// Print the result</span></div>
<div class="ttc" id="aclassnz_1_1nodes_1_1calc_1_1_re_l_u_node_html_a78a8e2e5cf13d31956e2367d923efa53"><div class="ttname"><a href="#a78a8e2e5cf13d31956e2367d923efa53">nz::nodes::calc::ReLUNode::ReLUNode</a></div><div class="ttdeci">ReLUNode(Node *input)</div><div class="ttdoc">Constructor to initialize a ReLUNode for applying the ReLU activation function.</div><div class="ttdef"><b>Definition</b> <a href="_nodes_8cu_source.html#l00344">Nodes.cu:344</a></div></div>
</div><!-- fragment --><dl class="section see"><dt>See also</dt><dd><a class="el" href="#ae8e78ee766f1e4845c6a110464e077f7" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward()</a> for the <a class="el" href="namespacenz_1_1data.html#a4706224f5e7c9a0cfe4c74983aaef1bd" title="Apply the Rectified Linear Unit (ReLU) activation function element-wise to an input tensor.">ReLU</a> activation computation in the <a class="el" href="#ae8e78ee766f1e4845c6a110464e077f7" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward</a> pass. </dd>
<dd>
<a class="el" href="#af06dd1eadec4b3616c7c9e655820b1b8" title="Backward pass for the ReLUNode to compute gradients.">backward()</a> for gradient computation in the <a class="el" href="#af06dd1eadec4b3616c7c9e655820b1b8" title="Backward pass for the ReLUNode to compute gradients.">backward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_nodes_8cuh_source.html#l01929">1929</a> of file <a class="el" href="_nodes_8cuh_source.html">Nodes.cuh</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a78a8e2e5cf13d31956e2367d923efa53" name="a78a8e2e5cf13d31956e2367d923efa53"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a78a8e2e5cf13d31956e2367d923efa53">&#9670;&#160;</a></span>ReLUNode()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">nz::nodes::calc::ReLUNode::ReLUNode </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classnz_1_1nodes_1_1_node.html">Node</a> *</td>          <td class="paramname"><span class="paramname"><em>input</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">explicit</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Constructor to initialize a <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> for applying the ReLU activation function. </p>
<p>The constructor initializes a <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code>, which applies the Rectified Linear Unit (ReLU) activation function to an input tensor. It establishes a connection to the input node, initializes the output tensor, and sets the type of the node to "ReLU".</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>A pointer to the input node. Its <code>output</code> tensor will have the ReLU activation applied.</td></tr>
  </table>
  </dd>
</dl>
<ul>
<li>The input node is added to the <code>inputs</code> vector to establish the connection in the computational graph.</li>
<li>The <code>output</code> tensor is initialized with the same shape as the input tensor, and its gradient tracking is determined based on the input tensor's requirements.</li>
<li>The node's type is set to "ReLU" to reflect its operation.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The ReLU activation is defined as <code>ReLU(x) = max(0, x)</code>. This will be applied during the forward pass.</li>
<li>This node supports automatic gradient tracking if the input tensor requires gradients.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#ae8e78ee766f1e4845c6a110464e077f7" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward()</a> for the <a class="el" href="#ae8e78ee766f1e4845c6a110464e077f7" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward</a> pass implementation. </dd>
<dd>
<a class="el" href="#af06dd1eadec4b3616c7c9e655820b1b8" title="Backward pass for the ReLUNode to compute gradients.">backward()</a> for gradient propagation in the <a class="el" href="#af06dd1eadec4b3616c7c9e655820b1b8" title="Backward pass for the ReLUNode to compute gradients.">backward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00344">344</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="af06dd1eadec4b3616c7c9e655820b1b8" name="af06dd1eadec4b3616c7c9e655820b1b8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af06dd1eadec4b3616c7c9e655820b1b8">&#9670;&#160;</a></span>backward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nz::nodes::calc::ReLUNode::backward </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Backward pass for the <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> to compute gradients. </p>
<p>The <code><a class="el" href="#af06dd1eadec4b3616c7c9e655820b1b8" title="Backward pass for the ReLUNode to compute gradients.">backward()</a></code> method computes the gradient of the loss with respect to the input tensor by applying the derivative of the ReLU activation function. Gradients are propagated only for elements where the input tensor values are positive; otherwise, the gradients are set to zero.</p>
<ul>
<li>A CUDA kernel (<code>ReLUBackward</code>) is launched to compute the gradients in parallel on the GPU.</li>
<li>The grid and block dimensions are calculated dynamically based on the size of the <code>output</code> tensor.</li>
<li>The derivative of ReLU is defined as: <div class="fragment"><div class="line"><a class="code hl_function" href="namespacenz_1_1data.html#a4706224f5e7c9a0cfe4c74983aaef1bd">ReLU</a><span class="stringliteral">&#39;(x) = 1, if x &gt; 0</span></div>
<div class="line"><span class="stringliteral">          0, if x &lt;= 0</span></div>
<div class="ttc" id="anamespacenz_1_1data_html_a4706224f5e7c9a0cfe4c74983aaef1bd"><div class="ttname"><a href="namespacenz_1_1data.html#a4706224f5e7c9a0cfe4c74983aaef1bd">nz::data::ReLU</a></div><div class="ttdeci">std::enable_if_t&lt; is_valid_tensor_type&lt; T &gt;::value, T &gt; ReLU(T &amp;input)</div><div class="ttdoc">Apply the Rectified Linear Unit (ReLU) activation function element-wise to an input tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_operations_8cuh_source.html#l00050">TensorOperations.cuh:50</a></div></div>
</div><!-- fragment --> This derivative is applied element-wise to propagate gradients through the ReLU operation.</li>
<li>Gradients are accumulated in the gradient tensor of the input node.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Gradients are only propagated if the input tensor's <code>requiresGrad</code> property is true.</li>
<li>The shape of the gradient tensor matches the shape of the input tensor.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#ae8e78ee766f1e4845c6a110464e077f7" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward()</a> for the <a class="el" href="namespacenz_1_1data.html#a4706224f5e7c9a0cfe4c74983aaef1bd" title="Apply the Rectified Linear Unit (ReLU) activation function element-wise to an input tensor.">ReLU</a> activation computation in the <a class="el" href="#ae8e78ee766f1e4845c6a110464e077f7" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p>Implements <a class="el" href="classnz_1_1nodes_1_1_node.html#a0a9ecbaa3d790ba38e8218aca7837fd0">nz::nodes::Node</a>.</p>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00357">357</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1nodes_1_1calc_1_1_re_l_u_node_af06dd1eadec4b3616c7c9e655820b1b8_cgraph.png" border="0" usemap="#aclassnz_1_1nodes_1_1calc_1_1_re_l_u_node_af06dd1eadec4b3616c7c9e655820b1b8_cgraph" alt=""/></div>
<map name="aclassnz_1_1nodes_1_1calc_1_1_re_l_u_node_af06dd1eadec4b3616c7c9e655820b1b8_cgraph" id="aclassnz_1_1nodes_1_1calc_1_1_re_l_u_node_af06dd1eadec4b3616c7c9e655820b1b8_cgraph">
<area shape="rect" title="Backward pass for the ReLUNode to compute gradients." alt="" coords="5,39,189,81"/>
<area shape="rect" href="namespacenz_1_1krnl.html#a4ddfc808de99fe831e74a3bd3f9bbdaf" title="Kernel function to compute the gradient of the ReLU activation during backpropagation." alt="" coords="237,47,401,73"/>
<area shape="poly" title=" " alt="" coords="189,57,221,57,221,63,189,63"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#ab4b2eb422e0e1ee44bdfdc0eb94457ce" title="Returns a reference to the singleton instance of the StreamManager." alt="" coords="449,5,634,48"/>
<area shape="poly" title=" " alt="" coords="401,45,433,40,434,45,402,50"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#a46ce59b45de432842454aadf00b93791" title="Asynchronously submits a CUDA kernel with stream&#45;ordered dependency management." alt="" coords="449,72,634,115"/>
<area shape="poly" title=" " alt="" coords="402,70,434,75,433,80,401,75"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#a1de1cf3aadea137faf90a2f9b4b7abe2" title="Acquires CUDA stream from pool using round&#45;robin scheduling." alt="" coords="682,39,868,81"/>
<area shape="poly" title=" " alt="" coords="634,77,666,73,667,78,635,83"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#adb1078a67c6e38932d7d58c2adb05ec0" title="Synchronizes CUDA stream execution until data writes complete." alt="" coords="682,105,868,148"/>
<area shape="poly" title=" " alt="" coords="635,104,667,109,666,114,634,109"/>
</map>
</div>

</div>
</div>
<a id="ae8e78ee766f1e4845c6a110464e077f7" name="ae8e78ee766f1e4845c6a110464e077f7"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae8e78ee766f1e4845c6a110464e077f7">&#9670;&#160;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nz::nodes::calc::ReLUNode::forward </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Forward pass for the <code><a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph.">ReLUNode</a></code> to apply the ReLU activation function. </p>
<p>The <code><a class="el" href="#ae8e78ee766f1e4845c6a110464e077f7" title="Forward pass for the ReLUNode to apply the ReLU activation function.">forward()</a></code> method applies the Rectified Linear Unit (ReLU) activation function element-wise to the input tensor. Values less than zero are set to zero, while non-negative values remain unchanged. The results are stored in the <code>output</code> tensor.</p>
<ul>
<li>A CUDA kernel (<code>RectifiedLinearUnit</code>) is launched to compute the ReLU activation in parallel on the GPU.</li>
<li>The grid and block dimensions are calculated dynamically based on the size of the <code>output</code> tensor to optimize GPU performance.</li>
<li>The output tensor stores the result of applying <code>ReLU(x) = max(0, x)</code> for each element of the input tensor.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The shape of the output tensor matches that of the input tensor.</li>
<li>Ensure the input tensor is properly initialized before calling this method.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#af06dd1eadec4b3616c7c9e655820b1b8" title="Backward pass for the ReLUNode to compute gradients.">backward()</a> for gradient computation in the <a class="el" href="#af06dd1eadec4b3616c7c9e655820b1b8" title="Backward pass for the ReLUNode to compute gradients.">backward</a> pass.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/05 </dd></dl>

<p>Implements <a class="el" href="classnz_1_1nodes_1_1_node.html#a8a828c2e91a4aa2a9ab7b94554e4685b">nz::nodes::Node</a>.</p>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00351">351</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1nodes_1_1calc_1_1_re_l_u_node_ae8e78ee766f1e4845c6a110464e077f7_cgraph.png" border="0" usemap="#aclassnz_1_1nodes_1_1calc_1_1_re_l_u_node_ae8e78ee766f1e4845c6a110464e077f7_cgraph" alt=""/></div>
<map name="aclassnz_1_1nodes_1_1calc_1_1_re_l_u_node_ae8e78ee766f1e4845c6a110464e077f7_cgraph" id="aclassnz_1_1nodes_1_1calc_1_1_re_l_u_node_ae8e78ee766f1e4845c6a110464e077f7_cgraph">
<area shape="rect" title="Forward pass for the ReLUNode to apply the ReLU activation function." alt="" coords="5,39,189,81"/>
<area shape="rect" href="namespacenz_1_1krnl.html#a8855f411733f7de29d013f4ad40096c9" title="Kernel function to apply the Rectified Linear Unit (ReLU) activation on the GPU." alt="" coords="237,47,420,73"/>
<area shape="poly" title=" " alt="" coords="189,57,221,57,221,63,189,63"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#ab4b2eb422e0e1ee44bdfdc0eb94457ce" title="Returns a reference to the singleton instance of the StreamManager." alt="" coords="468,5,653,48"/>
<area shape="poly" title=" " alt="" coords="420,44,452,40,453,45,421,49"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#a46ce59b45de432842454aadf00b93791" title="Asynchronously submits a CUDA kernel with stream&#45;ordered dependency management." alt="" coords="468,72,653,115"/>
<area shape="poly" title=" " alt="" coords="421,71,453,75,452,80,420,76"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#a1de1cf3aadea137faf90a2f9b4b7abe2" title="Acquires CUDA stream from pool using round&#45;robin scheduling." alt="" coords="701,39,887,81"/>
<area shape="poly" title=" " alt="" coords="653,77,685,73,686,78,654,83"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#adb1078a67c6e38932d7d58c2adb05ec0" title="Synchronizes CUDA stream execution until data writes complete." alt="" coords="701,105,887,148"/>
<area shape="poly" title=" " alt="" coords="654,104,686,109,685,114,653,109"/>
</map>
</div>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/include/NeuZephyr/<a class="el" href="_nodes_8cuh_source.html">Nodes.cuh</a></li>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/src/<a class="el" href="_nodes_8cu_source.html">Nodes.cu</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
