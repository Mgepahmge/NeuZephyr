<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NeuZephyr: nz::opt::Adam Class Reference</title>
<link rel="icon" href="NZ_logo2.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="NZ_logo2.png"/></td>
  <td id="projectalign">
   <div id="projectname">NeuZephyr
   </div>
   <div id="projectbrief">Simple DL Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="inherits.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>nz</b></li><li class="navelem"><a class="el" href="namespacenz_1_1opt.html">opt</a></li><li class="navelem"><a class="el" href="classnz_1_1opt_1_1_adam.html">Adam</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classnz_1_1opt_1_1_adam-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">nz::opt::Adam Class Reference</div></div>
</div><!--header-->
<div class="contents">

<p><a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimizer for deep learning models.  
 <a href="#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for nz::opt::Adam:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1opt_1_1_adam__inherit__graph.png" border="0" usemap="#anz_1_1opt_1_1_adam_inherit__map" alt="Inheritance graph"/></div>
<map name="anz_1_1opt_1_1_adam_inherit__map" id="anz_1_1opt_1_1_adam_inherit__map">
<area shape="rect" title="Adam optimizer for deep learning models." alt="" coords="16,80,120,107"/>
<area shape="rect" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning." alt="" coords="5,5,131,32"/>
<area shape="poly" title=" " alt="" coords="71,48,71,80,65,80,65,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<div class="dynheader">
Collaboration diagram for nz::opt::Adam:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1opt_1_1_adam__coll__graph.png" border="0" usemap="#anz_1_1opt_1_1_adam_coll__map" alt="Collaboration graph"/></div>
<map name="anz_1_1opt_1_1_adam_coll__map" id="anz_1_1opt_1_1_adam_coll__map">
<area shape="rect" title="Adam optimizer for deep learning models." alt="" coords="16,80,120,107"/>
<area shape="rect" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning." alt="" coords="5,5,131,32"/>
<area shape="poly" title=" " alt="" coords="71,48,71,80,65,80,65,48"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a7514c0e41db88f3996bc677c654db438" id="r_a7514c0e41db88f3996bc677c654db438"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a7514c0e41db88f3996bc677c654db438">Adam</a> (Tensor::value_type learning_rate, Tensor::value_type beta1, Tensor::value_type beta2)</td></tr>
<tr class="memdesc:a7514c0e41db88f3996bc677c654db438"><td class="mdescLeft">&#160;</td><td class="mdescRight">Constructs an <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimizer with the specified hyperparameters.  <br /></td></tr>
<tr class="separator:a7514c0e41db88f3996bc677c654db438"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa7fc73a17f092e104d5284d2556a1a98" id="r_aa7fc73a17f092e104d5284d2556a1a98"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aa7fc73a17f092e104d5284d2556a1a98">step</a> (<a class="el" href="classnz_1_1nodes_1_1_node.html">Node</a> *input) override</td></tr>
<tr class="memdesc:aa7fc73a17f092e104d5284d2556a1a98"><td class="mdescLeft">&#160;</td><td class="mdescRight">Performs a single optimization step using the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> algorithm.  <br /></td></tr>
<tr class="separator:aa7fc73a17f092e104d5284d2556a1a98"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="inherit_header pub_methods_classnz_1_1opt_1_1_optimizer"><td colspan="2" onclick="javascript:dynsection.toggleInherit('pub_methods_classnz_1_1opt_1_1_optimizer')"><img src="closed.png" alt="-"/>&#160;Public Member Functions inherited from <a class="el" href="classnz_1_1opt_1_1_optimizer.html">nz::opt::Optimizer</a></td></tr>
<tr class="memitem:aaf8d92566a815254dbb0ace9af9cb1ae inherit pub_methods_classnz_1_1opt_1_1_optimizer" id="r_aaf8d92566a815254dbb0ace9af9cb1ae"><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1opt_1_1_optimizer.html#aaf8d92566a815254dbb0ace9af9cb1ae">Optimizer</a> ()=default</td></tr>
<tr class="memdesc:aaf8d92566a815254dbb0ace9af9cb1ae inherit pub_methods_classnz_1_1opt_1_1_optimizer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Default constructor for the <a class="el" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning.">Optimizer</a> class.  <br /></td></tr>
<tr class="separator:aaf8d92566a815254dbb0ace9af9cb1ae inherit pub_methods_classnz_1_1opt_1_1_optimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9262983ef3bd11e6f548862b2f58e1d inherit pub_methods_classnz_1_1opt_1_1_optimizer" id="r_ab9262983ef3bd11e6f548862b2f58e1d"><td class="memItemLeft" align="right" valign="top">virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1opt_1_1_optimizer.html#ab9262983ef3bd11e6f548862b2f58e1d">~Optimizer</a> ()=default</td></tr>
<tr class="memdesc:ab9262983ef3bd11e6f548862b2f58e1d inherit pub_methods_classnz_1_1opt_1_1_optimizer"><td class="mdescLeft">&#160;</td><td class="mdescRight">Default destructor for the <a class="el" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning.">Optimizer</a> class.  <br /></td></tr>
<tr class="separator:ab9262983ef3bd11e6f548862b2f58e1d inherit pub_methods_classnz_1_1opt_1_1_optimizer"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p><a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimizer for deep learning models. </p>
<p>The <code><a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a></code> class implements the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimization algorithm, which is an adaptive learning rate optimization method designed for training deep learning models. <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> combines the advantages of two popular optimization techniques: Adaptive Gradient Algorithm (<a class="el" href="classnz_1_1opt_1_1_ada_grad.html" title="AdaGrad optimizer for deep learning models.">AdaGrad</a>) and Root Mean Square Propagation (RMSProp). It uses estimates of first and second moments of gradients to adaptively adjust the learning rate for each parameter.</p>
<p>This class extends the <code><a class="el" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning.">Optimizer</a></code> base class and provides a concrete implementation of the <code>step</code> method, which updates the model's parameters (represented as <code>Node</code> objects) using the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> algorithm.</p>
<ul>
<li>The optimizer maintains two moment estimates for each parameter (<code>Node</code>):<ul>
<li>( m_t ): The first moment estimate, which is the exponentially decaying average of past gradients.</li>
<li>( v_t ): The second moment estimate, which is the exponentially decaying average of past squared gradients.</li>
</ul>
</li>
<li>The moment estimates are updated using the following formulas: [ m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t ] [ v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 ] where ( g_t ) is the current gradient, ( \beta_1 ) and ( \beta_2 ) are the decay rates for the first and second moments.</li>
<li>The model parameters are then updated using the bias-corrected moment estimates: [ \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t} ] [ \theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} ] where ( \eta ) is the learning rate, ( \epsilon ) is a small constant to prevent division by zero.</li>
<li>The optimizer uses GPU-accelerated computations through CUDA to efficiently update parameters, making it suitable for large-scale models.</li>
</ul>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The optimizer assumes that the model parameters are represented by <code>Node</code> objects, and each node must have associated gradients.</li>
<li>The first and second moment estimates (<code>m</code> and <code>v</code>) are stored per <code>Node</code> object. If a <code>Node</code> does not have existing moments, they are initialized to zero tensors.</li>
<li>The optimizer utilizes GPU memory for moment storage and gradient computation, requiring CUDA support.</li>
<li>Ensure that the model parameters have been properly initialized, and gradients are computed before calling this method.</li>
</ul>
</dd></dl>
<h3><a class="anchor" id="autotoc_md72"></a>
Usage Example:</h3>
<div class="fragment"><div class="line"><a class="code hl_class" href="classnz_1_1opt_1_1_adam.html">Adam</a> optimizer(0.001, 0.9, 0.999);</div>
<div class="line">graph.update(&amp;optimizer); <span class="comment">// Suppose &quot;graph&quot; is a computation graph waiting for gradient updates.</span></div>
<div class="ttc" id="aclassnz_1_1opt_1_1_adam_html"><div class="ttname"><a href="classnz_1_1opt_1_1_adam.html">nz::opt::Adam</a></div><div class="ttdoc">Adam optimizer for deep learning models.</div><div class="ttdef"><b>Definition</b> <a href="_optimizer_8cuh_source.html#l00707">Optimizer.cuh:707</a></div></div>
</div><!-- fragment --><dl class="section see"><dt>See also</dt><dd><a class="el" href="classnz_1_1opt_1_1_optimizer.html" title="Base class for optimization algorithms in deep learning.">Optimizer</a> for the base class that defines the interface for all optimizers. </dd>
<dd>
Nodes::Node for the class representing model parameters.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/07 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_optimizer_8cuh_source.html#l00707">707</a> of file <a class="el" href="_optimizer_8cuh_source.html">Optimizer.cuh</a>.</p>
</div><h2 class="groupheader">Constructor &amp; Destructor Documentation</h2>
<a id="a7514c0e41db88f3996bc677c654db438" name="a7514c0e41db88f3996bc677c654db438"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7514c0e41db88f3996bc677c654db438">&#9670;&#160;</a></span>Adam()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">nz::opt::Adam::Adam </td>
          <td>(</td>
          <td class="paramtype">Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>learning_rate</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>beta1</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>beta2</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">explicit</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Constructs an <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimizer with the specified hyperparameters. </p>
<p>The <code><a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a></code> constructor initializes an instance of the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimizer with the given learning rate, beta1, and beta2 values. These hyperparameters control the behavior of the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimization algorithm:</p><ul>
<li>The learning rate determines the step size for parameter updates.</li>
<li>Beta1 controls the decay rate for the first moment estimate (moving average of gradients).</li>
<li>Beta2 controls the decay rate for the second moment estimate (moving average of squared gradients).</li>
</ul>
<p>The constructor also initializes the internal iteration counter (<code>it</code>) to zero, which is used for bias correction during the parameter updates.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">learning_rate</td><td>The learning rate (( \eta )) used for parameter updates. It controls the step size. </td></tr>
    <tr><td class="paramname">beta1</td><td>The exponential decay rate for the first moment estimate (( \beta_1 )). Typical values are in the range [0.9, 0.99]. </td></tr>
    <tr><td class="paramname">beta2</td><td>The exponential decay rate for the second moment estimate (( \beta_2 )). Typical values are in the range [0.99, 0.999].</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The learning rate, beta1, and beta2 values should be chosen carefully based on the specific task and dataset.</li>
<li>The default values for beta1 (0.9) and beta2 (0.999) are commonly used in practice.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#aa7fc73a17f092e104d5284d2556a1a98" title="Performs a single optimization step using the Adam algorithm.">Adam::step</a> for the method that performs parameter updates using the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimizer.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/07 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_optimizer_8cu_source.html#l00079">79</a> of file <a class="el" href="_optimizer_8cu_source.html">Optimizer.cu</a>.</p>

</div>
</div>
<h2 class="groupheader">Member Function Documentation</h2>
<a id="aa7fc73a17f092e104d5284d2556a1a98" name="aa7fc73a17f092e104d5284d2556a1a98"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa7fc73a17f092e104d5284d2556a1a98">&#9670;&#160;</a></span>step()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nz::opt::Adam::step </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="classnz_1_1nodes_1_1_node.html">Node</a> *</td>          <td class="paramname"><span class="paramname"><em>input</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">override</span><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Performs a single optimization step using the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> algorithm. </p>
<p>The <code>step</code> method updates the model parameters based on the gradients computed during the forward pass. It applies the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> optimization algorithm, which uses moving averages of the gradients and their squared values to adaptively adjust the learning rate for each parameter. This helps achieve stable and efficient parameter updates.</p>
<p>The method performs the following steps:</p><ol type="1">
<li>Increments the internal iteration counter (<code>it</code>), which is used for bias correction.</li>
<li>Checks if the first moment estimate (<code>m</code>) and second moment estimate (<code>v</code>) for the given input node exist. If not, it initializes them to zero tensors with the same shape as the node's output.</li>
<li>Launches a CUDA kernel to compute the <a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> updates for the parameters, using the current gradient, the moving averages of the gradients (<code>m</code>), and their squared values (<code>v</code>), along with the specified hyperparameters (learning rate, beta1, beta2, epsilon).</li>
</ol>
<p>This method is designed to be used with a model parameter represented as a <code>Node</code> object and assumes that the node has an associated output tensor and gradient.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">input</td><td>A pointer to the <code>Node</code> object representing the model parameter to be updated. The node should have an output tensor and its gradient already computed.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This method operates on the GPU using CUDA to accelerate the parameter update process.</li>
<li>It assumes that the <code>input</code> node has a valid gradient stored in its <code>output</code> object.</li>
<li>The first moment estimate (<code>m</code>) and second moment estimate (<code>v</code>) are maintained for each node individually.</li>
<li>The <code>epsilon</code> value is used to prevent division by zero during the parameter update.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="classnz_1_1opt_1_1_adam.html" title="Adam optimizer for deep learning models.">Adam</a> for the class definition and constructor.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/12/07 </dd></dl>

<p>Implements <a class="el" href="classnz_1_1opt_1_1_optimizer.html#a826381abaaf29dbebade7cfd38b266e4">nz::opt::Optimizer</a>.</p>

<p class="definition">Definition at line <a class="el" href="_optimizer_8cu_source.html#l00086">86</a> of file <a class="el" href="_optimizer_8cu_source.html">Optimizer.cu</a>.</p>
<div class="dynheader">
Here is the call graph for this function:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1opt_1_1_adam_aa7fc73a17f092e104d5284d2556a1a98_cgraph.png" border="0" usemap="#aclassnz_1_1opt_1_1_adam_aa7fc73a17f092e104d5284d2556a1a98_cgraph" alt=""/></div>
<map name="aclassnz_1_1opt_1_1_adam_aa7fc73a17f092e104d5284d2556a1a98_cgraph" id="aclassnz_1_1opt_1_1_adam_aa7fc73a17f092e104d5284d2556a1a98_cgraph">
<area shape="rect" title="Performs a single optimization step using the Adam algorithm." alt="" coords="5,39,142,65"/>
<area shape="rect" href="namespacenz_1_1krnl.html#a2b9ab840eeb0e74f4b78277a046b3a07" title="Kernel function to apply Adam optimization." alt="" coords="374,13,480,40"/>
<area shape="poly" title=" " alt="" coords="142,44,358,29,359,34,142,50"/>
<area shape="rect" href="classnz_1_1data_1_1_tensor.html#ad220de56b18c404611f07f2290cd7e9d" title="Fills the tensor&#39;s data with a specified value." alt="" coords="190,64,326,91"/>
<area shape="poly" title=" " alt="" coords="142,59,174,63,174,68,142,64"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#ab4b2eb422e0e1ee44bdfdc0eb94457ce" title="Returns a reference to the singleton instance of the StreamManager." alt="" coords="528,5,714,48"/>
<area shape="poly" title=" " alt="" coords="481,24,513,24,513,29,481,29"/>
<area shape="rect" href="namespacenz_1_1krnl.html#ad136c8a6560a5305984ce0a31bea71bf" title="Kernel function to fill a data array with a given value." alt="" coords="382,64,472,91"/>
<area shape="poly" title=" " alt="" coords="326,75,367,75,367,80,326,80"/>
<area shape="poly" title=" " alt="" coords="472,63,522,50,523,55,473,68"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#a46ce59b45de432842454aadf00b93791" title="Asynchronously submits a CUDA kernel with stream&#45;ordered dependency management." alt="" coords="528,72,714,115"/>
<area shape="poly" title=" " alt="" coords="473,78,513,82,513,87,472,84"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#a1de1cf3aadea137faf90a2f9b4b7abe2" title="Acquires CUDA stream from pool using round&#45;robin scheduling." alt="" coords="762,39,947,81"/>
<area shape="poly" title=" " alt="" coords="714,77,746,73,746,78,715,83"/>
<area shape="rect" href="classnz_1_1cu_strm_1_1_stream_manager.html#adb1078a67c6e38932d7d58c2adb05ec0" title="Synchronizes CUDA stream execution until data writes complete." alt="" coords="762,105,947,148"/>
<area shape="poly" title=" " alt="" coords="715,104,746,109,746,114,714,109"/>
</map>
</div>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/include/NeuZephyr/<a class="el" href="_optimizer_8cuh_source.html">Optimizer.cuh</a></li>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/src/<a class="el" href="_optimizer_8cu_source.html">Optimizer.cu</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
