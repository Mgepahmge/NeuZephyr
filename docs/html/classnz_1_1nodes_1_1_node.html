<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NeuZephyr: nz::nodes::Node Class Reference</title>
<link rel="icon" href="NZ_logo2.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="NZ_logo2.png"/></td>
  <td id="projectalign">
   <div id="projectname">NeuZephyr
   </div>
   <div id="projectbrief">Simple DL Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="classes.html"><span>Class&#160;Index</span></a></li>
      <li><a href="inherits.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>nz</b></li><li class="navelem"><a class="el" href="namespacenz_1_1nodes.html">nodes</a></li><li class="navelem"><a class="el" href="classnz_1_1nodes_1_1_node.html">Node</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="classnz_1_1nodes_1_1_node-members.html">List of all members</a>  </div>
  <div class="headertitle"><div class="title">nz::nodes::Node Class Reference<span class="mlabels"><span class="mlabel">abstract</span></span></div></div>
</div><!--header-->
<div class="contents">

<p>Base class for nodes in a neural network or computational graph.  
 <a href="#details">More...</a></p>
<div class="dynheader">
Inheritance diagram for nz::nodes::Node:</div>
<div class="dyncontent">
<div class="center"><img src="classnz_1_1nodes_1_1_node__inherit__graph.png" border="0" usemap="#anz_1_1nodes_1_1_node_inherit__map" alt="Inheritance graph"/></div>
<map name="anz_1_1nodes_1_1_node_inherit__map" id="anz_1_1nodes_1_1_node_inherit__map">
<area shape="rect" title="Base class for nodes in a neural network or computational graph." alt="" coords="5,452,123,479"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_add_node.html" title="Represents a node that performs element&#45;wise addition between two input tensors." alt="" coords="197,5,369,32"/>
<area shape="poly" title=" " alt="" coords="66,436,78,346,100,228,130,116,148,72,169,42,195,25,198,30,173,46,153,75,135,118,105,230,84,347,71,437"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_e_l_u_node.html" title="Represents an Exponential Linear Unit (ELU) activation function node in a computational graph." alt="" coords="195,56,370,83"/>
<area shape="poly" title=" " alt="" coords="62,436,64,366,78,274,91,224,110,176,136,132,169,93,193,77,196,82,172,97,140,135,115,179,96,226,83,275,69,367,67,436"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_hard_sigmoid_node.html" title="Represents a Hard Sigmoid activation function node in a computational graph." alt="" coords="171,107,395,133"/>
<area shape="poly" title=" " alt="" coords="63,436,69,375,85,296,98,254,116,214,139,176,169,143,186,132,189,136,172,147,144,179,121,216,103,256,90,297,74,376,69,437"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html" title="Represents a Hard Swish activation function node in a computational graph." alt="" coords="176,157,390,184"/>
<area shape="poly" title=" " alt="" coords="65,436,74,384,92,318,105,284,122,251,143,221,169,194,187,182,189,187,172,198,147,224,127,254,110,286,97,320,79,385,71,437"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_leaky_re_l_u_node.html" title="Represents a Leaky Rectified Linear Unit (LeakyReLU) activation function node in a computational grap..." alt="" coords="198,208,368,251"/>
<area shape="poly" title=" " alt="" coords="70,436,83,397,103,350,131,302,169,261,196,244,199,249,172,265,136,305,108,352,88,399,75,438"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_mat_mul_node.html" title="Represents a matrix multiplication operation node in a computational graph." alt="" coords="187,275,379,301"/>
<area shape="poly" title=" " alt="" coords="74,436,110,373,137,339,169,311,191,300,194,304,172,316,141,343,115,376,79,439"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html" title="Represents a Rectified Linear Unit (ReLU) operation node in a computational graph." alt="" coords="191,325,374,352"/>
<area shape="poly" title=" " alt="" coords="83,438,120,398,144,378,169,362,197,350,199,355,172,366,147,383,124,402,87,442"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_scalar_add_node.html" title="Represents a scalar addition operation node in a computational graph." alt="" coords="178,376,388,403"/>
<area shape="poly" title=" " alt="" coords="102,442,170,412,209,401,211,406,172,417,105,447"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_scalar_div_node.html" title="Represents a scalar division operation node in a computational graph." alt="" coords="180,427,386,453"/>
<area shape="poly" title=" " alt="" coords="138,454,179,449,179,455,139,459"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_scalar_mul_node.html" title="Represents a scalar multiplication operation node in a computational graph." alt="" coords="179,477,386,504"/>
<area shape="poly" title=" " alt="" coords="138,471,179,476,178,481,138,477"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_scalar_sub_node.html" title="Represents a scalar subtraction operation node in a computational graph." alt="" coords="178,528,388,555"/>
<area shape="poly" title=" " alt="" coords="105,484,172,513,211,525,209,530,170,519,102,489"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_sigmoid_node.html" title="Represents a Sigmoid activation function node in a computational graph." alt="" coords="185,579,381,605"/>
<area shape="poly" title=" " alt="" coords="87,489,124,529,147,548,172,564,199,576,197,581,169,569,144,552,120,532,83,493"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_softmax_node.html" title="Implements the Softmax activation function as a node in a neural network computational graph." alt="" coords="184,629,382,656"/>
<area shape="poly" title=" " alt="" coords="79,492,115,555,141,588,172,615,194,626,191,631,169,620,137,591,110,558,74,494"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_sub_node.html" title="Represents a subtraction operation node in a computational graph." alt="" coords="197,680,369,707"/>
<area shape="poly" title=" " alt="" coords="74,493,87,532,106,579,135,627,152,648,172,666,198,680,195,684,169,670,148,651,130,630,102,582,82,534,69,494"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_swish_node.html" title="Represents a Swish activation function node in a computational graph." alt="" coords="190,731,376,757"/>
<area shape="poly" title=" " alt="" coords="71,494,81,543,99,603,129,665,149,693,172,717,191,728,188,733,169,721,145,696,124,668,94,605,76,544,66,495"/>
<area shape="rect" href="classnz_1_1nodes_1_1calc_1_1_tanh_node.html" title="Represents a hyperbolic tangent (tanh) activation function node in a computational graph." alt="" coords="194,781,372,808"/>
<area shape="poly" title=" " alt="" coords="69,493,75,551,92,626,105,665,123,702,145,737,172,767,195,781,192,786,169,771,141,740,118,705,100,667,87,627,70,552,64,494"/>
<area shape="rect" href="classnz_1_1nodes_1_1io_1_1_input_node.html" title="Represents an input node in a computational graph." alt="" coords="201,832,365,859"/>
<area shape="poly" title=" " alt="" coords="67,494,71,561,85,649,98,695,117,740,141,782,172,818,201,835,199,840,169,822,137,785,112,742,93,697,80,650,66,561,62,494"/>
<area shape="rect" href="classnz_1_1nodes_1_1io_1_1_output_node.html" title="Base class for loss function nodes in a computational graph." alt="" coords="196,883,370,909"/>
<area shape="poly" title=" " alt="" coords="72,494,84,580,106,693,136,800,153,841,173,869,197,884,194,889,169,873,149,844,131,802,101,694,79,581,66,495"/>
<area shape="rect" href="classnz_1_1nodes_1_1loss_1_1_binary_cross_entropy_node.html" title="Represents the Binary Cross&#45;Entropy (BCE) loss function node in a computational graph." alt="" coords="447,841,637,884"/>
<area shape="poly" title=" " alt="" coords="385,880,446,872,447,878,386,885"/>
<area shape="rect" href="classnz_1_1nodes_1_1loss_1_1_mean_squared_error_node.html" title="Represents the Mean Squared Error (MSE) loss function node in a computational graph." alt="" coords="443,908,641,951"/>
<area shape="poly" title=" " alt="" coords="386,906,443,914,442,919,385,912"/>
</map>
<center><span class="legend">[<a href="graph_legend.html">legend</a>]</span></center></div>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="pub-methods" name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr class="memitem:a8a828c2e91a4aa2a9ab7b94554e4685b" id="r_a8a828c2e91a4aa2a9ab7b94554e4685b"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a8a828c2e91a4aa2a9ab7b94554e4685b">forward</a> ()=0</td></tr>
<tr class="memdesc:a8a828c2e91a4aa2a9ab7b94554e4685b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Abstract method for the forward pass computation.  <br /></td></tr>
<tr class="separator:a8a828c2e91a4aa2a9ab7b94554e4685b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0a9ecbaa3d790ba38e8218aca7837fd0" id="r_a0a9ecbaa3d790ba38e8218aca7837fd0"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a0a9ecbaa3d790ba38e8218aca7837fd0">backward</a> ()=0</td></tr>
<tr class="memdesc:a0a9ecbaa3d790ba38e8218aca7837fd0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Abstract method for the backward pass (gradient computation).  <br /></td></tr>
<tr class="separator:a0a9ecbaa3d790ba38e8218aca7837fd0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a687ee9c34eb61f8f28caa201ca42696e" id="r_a687ee9c34eb61f8f28caa201ca42696e"><td class="memItemLeft" align="right" valign="top">virtual void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a687ee9c34eb61f8f28caa201ca42696e">print</a> (std::ostream &amp;os) const</td></tr>
<tr class="memdesc:a687ee9c34eb61f8f28caa201ca42696e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Prints the type, data, and gradient of the node.  <br /></td></tr>
<tr class="separator:a687ee9c34eb61f8f28caa201ca42696e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7d2c36560f68c28e655ca242da5b2097" id="r_a7d2c36560f68c28e655ca242da5b2097"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a7d2c36560f68c28e655ca242da5b2097">dataInject</a> (const Tensor::value_type *data, bool grad=false) const</td></tr>
<tr class="memdesc:a7d2c36560f68c28e655ca242da5b2097"><td class="mdescLeft">&#160;</td><td class="mdescRight">Injects data into a relevant tensor object, optionally setting its gradient requirement.  <br /></td></tr>
<tr class="separator:a7d2c36560f68c28e655ca242da5b2097"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a609f1730085dd1d31e0ddcbbae48a065" id="r_a609f1730085dd1d31e0ddcbbae48a065"><td class="memTemplParams" colspan="2">template&lt;typename Iterator &gt; </td></tr>
<tr class="memitem:a609f1730085dd1d31e0ddcbbae48a065"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a609f1730085dd1d31e0ddcbbae48a065">dataInject</a> (Iterator begin, Iterator end, const bool grad=false) const</td></tr>
<tr class="memdesc:a609f1730085dd1d31e0ddcbbae48a065"><td class="mdescLeft">&#160;</td><td class="mdescRight">Injects data from an iterator range into the output tensor of the InputNode, optionally setting its gradient requirement.  <br /></td></tr>
<tr class="separator:a609f1730085dd1d31e0ddcbbae48a065"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af8b4bab3271df92ca1f0914f7a97b1e8" id="r_af8b4bab3271df92ca1f0914f7a97b1e8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af8b4bab3271df92ca1f0914f7a97b1e8">dataInject</a> (const std::initializer_list&lt; Tensor::value_type &gt; &amp;data, bool grad=false) const</td></tr>
<tr class="memdesc:af8b4bab3271df92ca1f0914f7a97b1e8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Injects data from a std::initializer_list into the output tensor of the <a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a>, optionally setting its gradient requirement.  <br /></td></tr>
<tr class="separator:af8b4bab3271df92ca1f0914f7a97b1e8"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Base class for nodes in a neural network or computational graph. </p>
<p>The <code><a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a></code> class serves as an abstract base class for all types of nodes in a computational graph, commonly used in neural networks. Each node represents an operation or a layer in the graph, with input and output connections that allow data to flow through the network. The <code><a class="el" href="#a8a828c2e91a4aa2a9ab7b94554e4685b" title="Abstract method for the forward pass computation.">forward()</a></code> and <code><a class="el" href="#a0a9ecbaa3d790ba38e8218aca7837fd0" title="Abstract method for the backward pass (gradient computation).">backward()</a></code> methods define the computations to be performed during the forward and backward passes of the network, respectively.</p>
<p>This class is designed to be subclassed and extended for specific layers or operations. Derived classes are required to implement the <code><a class="el" href="#a8a828c2e91a4aa2a9ab7b94554e4685b" title="Abstract method for the forward pass computation.">forward()</a></code> and <code><a class="el" href="#a0a9ecbaa3d790ba38e8218aca7837fd0" title="Abstract method for the backward pass (gradient computation).">backward()</a></code> methods to define the specific computations for each node.</p>
<p>Key features:</p><ul>
<li><b>Inputs</b>: A vector of pointers to other nodes that provide input data to this node.</li>
<li><b>Output</b>: A shared pointer to a <code>Tensor</code> object that stores the result of this node's computation.</li>
<li><b>Type</b>: A string indicating the type of the node (e.g., "Basic", "Input", "MatMul").</li>
<li><b>Forward and Backward Passes</b>: The pure virtual functions <code><a class="el" href="#a8a828c2e91a4aa2a9ab7b94554e4685b" title="Abstract method for the forward pass computation.">forward()</a></code> and <code><a class="el" href="#a0a9ecbaa3d790ba38e8218aca7837fd0" title="Abstract method for the backward pass (gradient computation).">backward()</a></code> that must be implemented by derived classes to perform the forward and backward propagation steps of the neural network.</li>
</ul>
<p>This class is part of the <code><a class="el" href="namespacenz_1_1nodes.html" title="Contains classes and functionality for nodes in a neural network or computational graph.">nz::nodes</a></code> namespace, and is intended to be used as a base class for defining custom layers or operations in a neural network.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Derived classes must implement the <code><a class="el" href="#a8a828c2e91a4aa2a9ab7b94554e4685b" title="Abstract method for the forward pass computation.">forward()</a></code> and <code><a class="el" href="#a0a9ecbaa3d790ba38e8218aca7837fd0" title="Abstract method for the backward pass (gradient computation).">backward()</a></code> functions to define the specific computations for the node.</li>
<li>This class is designed to be used within a larger computational graph, where nodes are connected to form a complete neural network.</li>
</ul>
</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/11/29 </dd></dl>

<p class="definition">Definition at line <a class="el" href="_nodes_8cuh_source.html#l00114">114</a> of file <a class="el" href="_nodes_8cuh_source.html">Nodes.cuh</a>.</p>
</div><h2 class="groupheader">Member Function Documentation</h2>
<a id="a0a9ecbaa3d790ba38e8218aca7837fd0" name="a0a9ecbaa3d790ba38e8218aca7837fd0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0a9ecbaa3d790ba38e8218aca7837fd0">&#9670;&#160;</a></span>backward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void nz::nodes::Node::backward </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">pure virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Abstract method for the backward pass (gradient computation). </p>
<p>The <code><a class="el" href="#a0a9ecbaa3d790ba38e8218aca7837fd0" title="Abstract method for the backward pass (gradient computation).">backward()</a></code> method is a pure virtual function in the <code><a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a></code> class, which must be implemented by derived classes. It is responsible for computing the gradients during the backward pass of the neural network or computational graph, which is used for backpropagation in training.</p>
<p>During the backward pass, the error gradients are propagated backward through the network, from the output nodes to the input nodes. Each node computes the gradient of its output with respect to its input, using the chain rule of calculus, to update the weights or parameters of the network.</p>
<p>Derived classes that represent specific layers or operations must implement this method to define how gradients are calculated for that particular layer or operation.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The <code><a class="el" href="#a0a9ecbaa3d790ba38e8218aca7837fd0" title="Abstract method for the backward pass (gradient computation).">backward()</a></code> method must be implemented by any class derived from <code><a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a></code>. It should compute the gradient of the output with respect to the node's input and store it in the node's <code>grad</code> tensor.</li>
<li>This method is essential for the backpropagation process during training, allowing the model to adjust its parameters based on the computed gradients.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#a8a828c2e91a4aa2a9ab7b94554e4685b" title="Abstract method for the forward pass computation.">forward()</a> for the <a class="el" href="#a8a828c2e91a4aa2a9ab7b94554e4685b" title="Abstract method for the forward pass computation.">forward</a> propagation (computation) method.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/11/29 </dd></dl>

<p>Implemented in <a class="el" href="classnz_1_1nodes_1_1calc_1_1_add_node.html#aacd0de4600132791c8da7860dba3e43c">nz::nodes::calc::AddNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_e_l_u_node.html#a167cd647d2a6e3f273c250f15562a317">nz::nodes::calc::ELUNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_sigmoid_node.html#ad977c6a8c49252de4038f8ac08beed3c">nz::nodes::calc::HardSigmoidNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html#ace2cc92220449c5425a75d8e7f35ff42">nz::nodes::calc::HardSwishNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_leaky_re_l_u_node.html#aa3fe1c9e74733c8c5e15e71ec14143f0">nz::nodes::calc::LeakyReLUNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_mat_mul_node.html#ab644a874feb6a620ad31d37ca20525fd">nz::nodes::calc::MatMulNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html#af06dd1eadec4b3616c7c9e655820b1b8">nz::nodes::calc::ReLUNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_scalar_add_node.html#a453eed787a8161b36410bef2ba8b0a75">nz::nodes::calc::ScalarAddNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_scalar_div_node.html#a9b72dc5618e8e11790756c91116719e4">nz::nodes::calc::ScalarDivNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_scalar_mul_node.html#a074fe78c03e0b62c5e69b6a25b6b4c24">nz::nodes::calc::ScalarMulNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_scalar_sub_node.html#a7880d18811e20c3ec34b1417a28d697e">nz::nodes::calc::ScalarSubNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_sigmoid_node.html#a05fe16b3ecde344d9463efabf1318115">nz::nodes::calc::SigmoidNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_softmax_node.html#aa991e3bde7a3a5edbee62fab1cabba23">nz::nodes::calc::SoftmaxNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_sub_node.html#abb396a092ac9fb09c3a656329132842d">nz::nodes::calc::SubNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_swish_node.html#a55758c143f9a941d24abc58a43ae5e9d">nz::nodes::calc::SwishNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_tanh_node.html#ac75b18193ae5de920c0060ad83d1542a">nz::nodes::calc::TanhNode</a>, <a class="el" href="classnz_1_1nodes_1_1io_1_1_input_node.html#a3cde8af9401a117601dcdb0c9063516a">nz::nodes::io::InputNode</a>, <a class="el" href="classnz_1_1nodes_1_1io_1_1_output_node.html#a2f76355b646a9c9f1a0972ad87f6a260">nz::nodes::io::OutputNode</a>, <a class="el" href="classnz_1_1nodes_1_1loss_1_1_binary_cross_entropy_node.html#a868b2f5886b2e2adee12439ad50ca91a">nz::nodes::loss::BinaryCrossEntropyNode</a>, and <a class="el" href="classnz_1_1nodes_1_1loss_1_1_mean_squared_error_node.html#a8ccbbad9b8bb2111d24af789020337ce">nz::nodes::loss::MeanSquaredErrorNode</a>.</p>

</div>
</div>
<a id="af8b4bab3271df92ca1f0914f7a97b1e8" name="af8b4bab3271df92ca1f0914f7a97b1e8"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af8b4bab3271df92ca1f0914f7a97b1e8">&#9670;&#160;</a></span>dataInject() <span class="overload">[1/3]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void nz::nodes::Node::dataInject </td>
          <td>(</td>
          <td class="paramtype">const std::initializer_list&lt; Tensor::value_type &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>data</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>grad</em></span><span class="paramdefsep"> = </span><span class="paramdefval">false</span>&#160;) const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Injects data from a std::initializer_list into the output tensor of the <a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a>, optionally setting its gradient requirement. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">data</td><td>A std::initializer_list containing the data to be injected into the output tensor (host-to-device). </td></tr>
    <tr><td class="paramname">grad</td><td>A boolean indicating whether the output tensor should require gradient computation after data injection.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None.</dd></dl>
<p>This function is responsible for injecting data from a std::initializer_list into the output tensor of the <a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a>. Memory management is handled by the underlying <code>dataInject</code> method of the <code>Tensor</code> class. The <code>output</code> tensor is assumed to have already allocated enough memory to accommodate the data in the <code>std::initializer_list</code>.</p>
<p>Regarding exception handling, this function does not explicitly catch any exceptions. Exceptions that might occur during data injection, such as memory allocation errors in the <code>Tensor</code> class, will propagate to the caller.</p>
<p>This function acts as a bridge between the <a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a> and its output tensor, allowing data to be easily provided using a <code>std::initializer_list</code>.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">None</td><td>explicitly, but the <code>dataInject</code> method of the <code>Tensor</code> class may throw exceptions, such as <code>std::bad_alloc</code> if memory allocation fails during the injection process.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Ensure that the <code>std::initializer_list</code> contains enough elements to fill the output tensor according to its shape.</li>
<li>The CUDA runtime environment should be properly initialized before calling this function if the tensor is using CUDA memory.</li>
<li>The time complexity of this function is O(n), where n is the number of elements in the <code>std::initializer_list</code>, as it involves copying data from the list into the tensor.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"> </div>
<div class="line">InputNode node({2, 2}, <span class="keyword">true</span>);</div>
<div class="line">node.dataInject({1.0f, 2.0f, 3.0f, 4.0f});</div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00016">16</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>

</div>
</div>
<a id="a7d2c36560f68c28e655ca242da5b2097" name="a7d2c36560f68c28e655ca242da5b2097"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7d2c36560f68c28e655ca242da5b2097">&#9670;&#160;</a></span>dataInject() <span class="overload">[2/3]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void nz::nodes::Node::dataInject </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type *</td>          <td class="paramname"><span class="paramname"><em>data</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>grad</em></span><span class="paramdefsep"> = </span><span class="paramdefval">false</span>&#160;) const</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Injects data into a relevant tensor object, optionally setting its gradient requirement. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">data</td><td>A pointer to the data to be injected into the tensor (host-to-device). This data will be used to populate the tensor. </td></tr>
    <tr><td class="paramname">grad</td><td>A boolean indicating whether the tensor should require gradient computation after data injection. Defaults to false.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None.</dd></dl>
<p>This function is designed to inject data into a tensor object. Memory management within this function is handled by the underlying tensor operations. It is assumed that the tensor object has already allocated the necessary memory to hold the data pointed to by <code>data</code>.</p>
<p>Regarding exception handling, this function does not explicitly catch any exceptions. Exceptions that might occur during data injection, such as memory access errors or CUDA errors (if applicable), will propagate to the caller.</p>
<p>This function likely interacts with other components related to the tensor, such as the computation graph or the gradient computation system, depending on the value of <code>grad</code>.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">None</td><td>explicitly, but underlying tensor operations may throw exceptions, such as std::bad_alloc if memory allocation fails during the injection process.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Ensure that the <code>data</code> pointer is valid and points to enough data to fill the target tensor.</li>
<li>The CUDA runtime environment should be properly initialized before calling this function if the tensor is using CUDA memory.</li>
<li>The time complexity of this function is O(n), where n is the number of elements in the tensor, as it involves copying data into the tensor.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"> </div>
<div class="line">value_type data[] = {1.0f, 2.0f, 3.0f, 4.0f};</div>
<div class="line"><span class="comment">// Assume there is an object that has the dataInject method</span></div>
<div class="line">InputNode input({2, 2}), <span class="keyword">true</span>);</div>
<div class="line">obj.dataInject(data);</div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00012">12</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>

</div>
</div>
<a id="a609f1730085dd1d31e0ddcbbae48a065" name="a609f1730085dd1d31e0ddcbbae48a065"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a609f1730085dd1d31e0ddcbbae48a065">&#9670;&#160;</a></span>dataInject() <span class="overload">[3/3]</span></h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename Iterator &gt; </div>
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nz::nodes::Node::dataInject </td>
          <td>(</td>
          <td class="paramtype">Iterator</td>          <td class="paramname"><span class="paramname"><em>begin</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Iterator</td>          <td class="paramname"><span class="paramname"><em>end</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool</td>          <td class="paramname"><span class="paramname"><em>grad</em></span><span class="paramdefsep"> = </span><span class="paramdefval">false</span>&#160;) const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Injects data from an iterator range into the output tensor of the InputNode, optionally setting its gradient requirement. </p>
<dl class="tparams"><dt>Template Parameters</dt><dd>
  <table class="tparams">
    <tr><td class="paramname">Iterator</td><td>The type of the iterators used to define the data range. It should support the standard iterator operations like dereferencing and incrementing. </td></tr>
  </table>
  </dd>
</dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">begin</td><td>An iterator pointing to the beginning of the data range (host-to-device). The data in this range will be injected into the output tensor. </td></tr>
    <tr><td class="paramname">end</td><td>An iterator pointing to the end of the data range (host-to-device). </td></tr>
    <tr><td class="paramname">grad</td><td>A boolean indicating whether the output tensor should require gradient computation after data injection. Defaults to false.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None.</dd></dl>
<p>This template function is used to inject data from an iterator range into the output tensor of the InputNode. Memory management is handled by the underlying <code>dataInject</code> method of the <code>Tensor</code> class. It is assumed that the <code>output</code> tensor has already allocated sufficient memory to hold the data from the iterator range.</p>
<p>Regarding exception handling, this function does not explicitly catch any exceptions. Exceptions that might occur during data injection, such as iterator invalidation or memory allocation errors in the <code>Tensor</code> class, will propagate to the caller.</p>
<p>This function serves as a wrapper around the <code>dataInject</code> method of the <code>output</code> tensor, facilitating the use of iterators to provide data for injection.</p>
<dl class="exception"><dt>Exceptions</dt><dd>
  <table class="exception">
    <tr><td class="paramname">None</td><td>explicitly, but the <code>dataInject</code> method of the <code>Tensor</code> class may throw exceptions, such as <code>std::bad_alloc</code> if memory allocation fails during the injection process.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>Ensure that the iterator range <code>[begin, end)</code> is valid and that the data type pointed to by the iterators is compatible with the <code>Tensor::value_type</code>.</li>
<li>The CUDA runtime environment should be properly initialized before calling this function if the tensor is using CUDA memory.</li>
<li>The time complexity of this function is O(n), where n is the number of elements in the iterator range, as it involves copying data from the range into the tensor.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><span class="preprocessor">#include &lt;vector&gt;</span></div>
<div class="line"> </div>
<div class="line">std::vector&lt;value_type&gt; data = {1.0f, 2.0f, 3.0f, 4.0f};</div>
<div class="line">InputNode inputNode({2, 2}, <span class="keyword">true</span>);</div>
<div class="line">inputNode.dataInject(data.begin(), data.end());</div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_nodes_8cuh_source.html#l00278">278</a> of file <a class="el" href="_nodes_8cuh_source.html">Nodes.cuh</a>.</p>

</div>
</div>
<a id="a8a828c2e91a4aa2a9ab7b94554e4685b" name="a8a828c2e91a4aa2a9ab7b94554e4685b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8a828c2e91a4aa2a9ab7b94554e4685b">&#9670;&#160;</a></span>forward()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">virtual void nz::nodes::Node::forward </td>
          <td>(</td>
          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">pure virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Abstract method for the forward pass computation. </p>
<p>The <code><a class="el" href="#a8a828c2e91a4aa2a9ab7b94554e4685b" title="Abstract method for the forward pass computation.">forward()</a></code> method is a pure virtual function in the <code><a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a></code> class, which must be implemented by derived classes. It is responsible for performing the computation during the forward pass of the neural network or computational graph.</p>
<p>In the forward pass, data flows through the network from input nodes to output nodes, and each node performs its specific computation (e.g., activation, matrix multiplication, etc.) based on the data it receives as input.</p>
<p>Derived classes that represent specific layers or operations (such as activation functions, convolution layers, etc.) must implement this method to define the exact computation to be performed for that layer.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The <code><a class="el" href="#a8a828c2e91a4aa2a9ab7b94554e4685b" title="Abstract method for the forward pass computation.">forward()</a></code> method must be implemented by any class derived from <code><a class="el" href="classnz_1_1nodes_1_1_node.html" title="Base class for nodes in a neural network or computational graph.">Node</a></code>. It should modify the output of the node based on its inputs and computation.</li>
<li>This method does not return any value, as it updates the node's output tensor directly.</li>
</ul>
</dd></dl>
<dl class="section see"><dt>See also</dt><dd><a class="el" href="#a0a9ecbaa3d790ba38e8218aca7837fd0" title="Abstract method for the backward pass (gradient computation).">backward()</a> for the reverse propagation (gradient calculation) method.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/11/29 </dd></dl>

<p>Implemented in <a class="el" href="classnz_1_1nodes_1_1calc_1_1_add_node.html#adcbcffc97ede105ec64c7360377b9af3">nz::nodes::calc::AddNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_e_l_u_node.html#acd8b07d5fbd5a920d58d55a72a9ff092">nz::nodes::calc::ELUNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_sigmoid_node.html#a97590995aa192807d96a856ee2bcd71f">nz::nodes::calc::HardSigmoidNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_hard_swish_node.html#afd48643b49c76a9c38b0bfc0c4a502f1">nz::nodes::calc::HardSwishNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_leaky_re_l_u_node.html#a287aebe2ce3437d393fb1ac1b1119d25">nz::nodes::calc::LeakyReLUNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_mat_mul_node.html#a4d1ec1a90036ff16358c5f83123bac67">nz::nodes::calc::MatMulNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_re_l_u_node.html#ae8e78ee766f1e4845c6a110464e077f7">nz::nodes::calc::ReLUNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_scalar_add_node.html#ad1162e693cd13ee6e9e4f7cab27e4a31">nz::nodes::calc::ScalarAddNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_scalar_div_node.html#a4728d1f10d35d7e71b11acd32ee1a26d">nz::nodes::calc::ScalarDivNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_scalar_mul_node.html#af96c94d5a91e2ee3bd97113992c829ca">nz::nodes::calc::ScalarMulNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_scalar_sub_node.html#ac5d375db4c17885e597c4dcca9d0a318">nz::nodes::calc::ScalarSubNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_sigmoid_node.html#aa24b02f6d79fda31e6ad150879ed2bbb">nz::nodes::calc::SigmoidNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_softmax_node.html#a93f7d936ff487db8e7dceb6ee0cdc38e">nz::nodes::calc::SoftmaxNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_sub_node.html#a6ba6a63da4e869f8f0004896d01fe3f1">nz::nodes::calc::SubNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_swish_node.html#ab0fbf5a4d05c0df96b8aaffab36d92db">nz::nodes::calc::SwishNode</a>, <a class="el" href="classnz_1_1nodes_1_1calc_1_1_tanh_node.html#a451ff464932275955fbec1c33abdba97">nz::nodes::calc::TanhNode</a>, <a class="el" href="classnz_1_1nodes_1_1io_1_1_input_node.html#a4ba34603676c094723409d9e6b770976">nz::nodes::io::InputNode</a>, <a class="el" href="classnz_1_1nodes_1_1io_1_1_output_node.html#a1c05ec6cdbddef105a20c400d0515471">nz::nodes::io::OutputNode</a>, <a class="el" href="classnz_1_1nodes_1_1loss_1_1_binary_cross_entropy_node.html#afccd1a1a1207379dbfb648d0cbc3aab4">nz::nodes::loss::BinaryCrossEntropyNode</a>, and <a class="el" href="classnz_1_1nodes_1_1loss_1_1_mean_squared_error_node.html#ae81d6afb059f76617ea034032c12ec13">nz::nodes::loss::MeanSquaredErrorNode</a>.</p>

</div>
</div>
<a id="a687ee9c34eb61f8f28caa201ca42696e" name="a687ee9c34eb61f8f28caa201ca42696e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a687ee9c34eb61f8f28caa201ca42696e">&#9670;&#160;</a></span>print()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">void nz::nodes::Node::print </td>
          <td>(</td>
          <td class="paramtype">std::ostream &amp;</td>          <td class="paramname"><span class="paramname"><em>os</em></span></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">virtual</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>Prints the type, data, and gradient of the node. </p>
<p>The <code><a class="el" href="#a687ee9c34eb61f8f28caa201ca42696e" title="Prints the type, data, and gradient of the node.">print()</a></code> method outputs the information about the node, including its type, the tensor data stored in the node's output, and the corresponding gradient. This is useful for debugging and inspecting the state of nodes in a computational graph or during training, allowing for easy visualization of the node's content and gradients.</p>
<p>The method outputs the following details:</p><ul>
<li><b>Type</b>: The type of the node (e.g., the operation it represents, such as "MatrixMul", "ReLU", etc.).</li>
<li><b>Data</b>: The tensor data stored in the node's <code>output</code> tensor.</li>
<li><b>Gradient</b>: If the node has a computed gradient, it is also displayed, providing insights into the gradient values that are being backpropagated through the network during training.</li>
</ul>
<p>This method is primarily used for debugging and monitoring the state of tensors and gradients, making it easier to inspect how the data and gradients flow through the network.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>The <code>output</code> tensor should contain both the data and the gradient information, and both are printed when this method is called.</li>
<li>This method is typically used during development or debugging phases and should not be used in performance-critical code as it involves printing potentially large amounts of data.</li>
</ul>
</dd></dl>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">os</td><td>The output stream (e.g., <code>std::cout</code>) to which the node's information will be printed.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge (<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/11/29 </dd></dl>

<p>Reimplemented in <a class="el" href="classnz_1_1nodes_1_1io_1_1_output_node.html#ac340bd5a932808333e08e8bf24d53039">nz::nodes::io::OutputNode</a>.</p>

<p class="definition">Definition at line <a class="el" href="_nodes_8cu_source.html#l00007">7</a> of file <a class="el" href="_nodes_8cu_source.html">Nodes.cu</a>.</p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/include/NeuZephyr/<a class="el" href="_nodes_8cuh_source.html">Nodes.cuh</a></li>
<li>D:/Users/Mgepahmge/Documents/C Program/NeuZephyr/src/<a class="el" href="_nodes_8cu_source.html">Nodes.cu</a></li>
</ul>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
