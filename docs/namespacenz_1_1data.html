<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.12.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>NeuZephyr: nz::data Namespace Reference</title>
<link rel="icon" href="NZ_logo2.png" type="image/x-icon" />
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectlogo"><img alt="Logo" src="NZ_logo2.png"/></td>
  <td id="projectalign">
   <div id="projectname">NeuZephyr
   </div>
   <div id="projectbrief">Simple DL Framework</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.12.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li class="current"><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="namespaces.html"><span>Namespace&#160;List</span></a></li>
      <li><a href="namespacemembers.html"><span>Namespace&#160;Members</span></a></li>
    </ul>
  </div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<div id="nav-path" class="navpath">
  <ul>
<li class="navelem"><b>nz</b></li><li class="navelem"><a class="el" href="namespacenz_1_1data.html">data</a></li>  </ul>
</div>
</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">nz::data Namespace Reference</div></div>
</div><!--header-->
<div class="contents">

<p>Contains data structures and utilities for tensor operations in machine learning workflows.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a></td></tr>
<tr class="memdesc:"><td class="mdescLeft">&#160;</td><td class="mdescRight">A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.  <a href="classnz_1_1data_1_1_tensor.html#details">More...</a><br /></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a2907370af84a6c5bdc4b72803c9edc68" id="r_a2907370af84a6c5bdc4b72803c9edc68"><td class="memItemLeft" align="right" valign="top">std::ostream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2907370af84a6c5bdc4b72803c9edc68">operator&lt;&lt;</a> (std::ostream &amp;os, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a2907370af84a6c5bdc4b72803c9edc68"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overloads the <code>&lt;&lt;</code> operator to print the tensor's data to an output stream.  <br /></td></tr>
<tr class="separator:a2907370af84a6c5bdc4b72803c9edc68"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a40134aba93013e1b0d43c6fd5158d400" id="r_a40134aba93013e1b0d43c6fd5158d400"><td class="memItemLeft" align="right" valign="top">std::istream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a40134aba93013e1b0d43c6fd5158d400">operator&gt;&gt;</a> (std::istream &amp;is, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a40134aba93013e1b0d43c6fd5158d400"><td class="mdescLeft">&#160;</td><td class="mdescRight">Overloads the <code>&gt;&gt;</code> operator to read a tensor's data from an input stream.  <br /></td></tr>
<tr class="separator:a40134aba93013e1b0d43c6fd5158d400"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac65cd4ed45d91da297a90d4381411064" id="r_ac65cd4ed45d91da297a90d4381411064"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ac65cd4ed45d91da297a90d4381411064">operator*</a> (const Tensor::value_type lhs, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;rhs)</td></tr>
<tr class="memdesc:ac65cd4ed45d91da297a90d4381411064"><td class="mdescLeft">&#160;</td><td class="mdescRight">Multiplies a tensor by a scalar (element-wise multiplication).  <br /></td></tr>
<tr class="separator:ac65cd4ed45d91da297a90d4381411064"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab7386d5f0c567a1bed5a78eab4bf2d6b" id="r_ab7386d5f0c567a1bed5a78eab4bf2d6b"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab7386d5f0c567a1bed5a78eab4bf2d6b">operator*</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:ab7386d5f0c567a1bed5a78eab4bf2d6b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Multiplies a tensor by a scalar (element-wise multiplication).  <br /></td></tr>
<tr class="separator:ab7386d5f0c567a1bed5a78eab4bf2d6b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3b044ce5a34b4724830f073efb954743" id="r_a3b044ce5a34b4724830f073efb954743"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a3b044ce5a34b4724830f073efb954743">operator/</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:a3b044ce5a34b4724830f073efb954743"><td class="mdescLeft">&#160;</td><td class="mdescRight">Divides a tensor by a scalar (element-wise division).  <br /></td></tr>
<tr class="separator:a3b044ce5a34b4724830f073efb954743"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5c3243e28d8132d9514c8d37d597a7f" id="r_ab5c3243e28d8132d9514c8d37d597a7f"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab5c3243e28d8132d9514c8d37d597a7f">operator+</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:ab5c3243e28d8132d9514c8d37d597a7f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a scalar to a tensor (element-wise addition).  <br /></td></tr>
<tr class="separator:ab5c3243e28d8132d9514c8d37d597a7f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0f75427702f8732bc1833b272baa7f6" id="r_aa0f75427702f8732bc1833b272baa7f6"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aa0f75427702f8732bc1833b272baa7f6">operator+</a> (const Tensor::value_type lhs, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;rhs)</td></tr>
<tr class="memdesc:aa0f75427702f8732bc1833b272baa7f6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Adds a scalar to a tensor (element-wise addition).  <br /></td></tr>
<tr class="separator:aa0f75427702f8732bc1833b272baa7f6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a402c3fdfb410d9351a43087834982bbe" id="r_a402c3fdfb410d9351a43087834982bbe"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a402c3fdfb410d9351a43087834982bbe">operator-</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;lhs, const Tensor::value_type rhs)</td></tr>
<tr class="memdesc:a402c3fdfb410d9351a43087834982bbe"><td class="mdescLeft">&#160;</td><td class="mdescRight">Subtracts a scalar from a tensor (element-wise subtraction).  <br /></td></tr>
<tr class="separator:a402c3fdfb410d9351a43087834982bbe"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a00d2b2ae594fa907461c12045de8f470" id="r_a00d2b2ae594fa907461c12045de8f470"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a00d2b2ae594fa907461c12045de8f470">operator-</a> (const Tensor::value_type lhs, const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;rhs)</td></tr>
<tr class="memdesc:a00d2b2ae594fa907461c12045de8f470"><td class="mdescLeft">&#160;</td><td class="mdescRight">Subtracts a tensor from a scalar (element-wise subtraction).  <br /></td></tr>
<tr class="separator:a00d2b2ae594fa907461c12045de8f470"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2cd35db5c2bc0fc68cb0461d2f600089" id="r_a2cd35db5c2bc0fc68cb0461d2f600089"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2cd35db5c2bc0fc68cb0461d2f600089">ReLU</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a2cd35db5c2bc0fc68cb0461d2f600089"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Rectified Linear Unit (ReLU) activation function to a tensor.  <br /></td></tr>
<tr class="separator:a2cd35db5c2bc0fc68cb0461d2f600089"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a370bede45854c04c8222e9971cedc075" id="r_a370bede45854c04c8222e9971cedc075"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a370bede45854c04c8222e9971cedc075">Sigmoid</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a370bede45854c04c8222e9971cedc075"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Sigmoid activation function to a tensor.  <br /></td></tr>
<tr class="separator:a370bede45854c04c8222e9971cedc075"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2037c2f13444847ea57d7110d8c48a51" id="r_a2037c2f13444847ea57d7110d8c48a51"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a2037c2f13444847ea57d7110d8c48a51">Tanh</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a2037c2f13444847ea57d7110d8c48a51"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Tanh (hyperbolic tangent) activation function to a tensor.  <br /></td></tr>
<tr class="separator:a2037c2f13444847ea57d7110d8c48a51"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae638e4a70745936b4542263a5fef9b9d" id="r_ae638e4a70745936b4542263a5fef9b9d"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ae638e4a70745936b4542263a5fef9b9d">LeakyReLU</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor, float alpha=0.01f)</td></tr>
<tr class="memdesc:ae638e4a70745936b4542263a5fef9b9d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Tanh (hyperbolic tangent) activation function to a tensor.  <br /></td></tr>
<tr class="separator:ae638e4a70745936b4542263a5fef9b9d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a271473f387bde9059ad5afa03851184a" id="r_a271473f387bde9059ad5afa03851184a"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a271473f387bde9059ad5afa03851184a">Swish</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a271473f387bde9059ad5afa03851184a"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Swish activation function to a tensor.  <br /></td></tr>
<tr class="separator:a271473f387bde9059ad5afa03851184a"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a463a6acd4af10f53a8f57e30fba2e660" id="r_a463a6acd4af10f53a8f57e30fba2e660"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a463a6acd4af10f53a8f57e30fba2e660">ELU</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor, float alpha=1.0f)</td></tr>
<tr class="memdesc:a463a6acd4af10f53a8f57e30fba2e660"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Exponential Linear Unit (ELU) activation function to a tensor.  <br /></td></tr>
<tr class="separator:a463a6acd4af10f53a8f57e30fba2e660"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab9821ac15d218a07161a211b04db5528" id="r_ab9821ac15d218a07161a211b04db5528"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ab9821ac15d218a07161a211b04db5528">HardSigmoid</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor, float alpha=0.2f, float beta=0.5f)</td></tr>
<tr class="memdesc:ab9821ac15d218a07161a211b04db5528"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Hard Sigmoid activation function to a tensor.  <br /></td></tr>
<tr class="separator:ab9821ac15d218a07161a211b04db5528"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a68859b30e6833b954f27f89eb8592235" id="r_a68859b30e6833b954f27f89eb8592235"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a68859b30e6833b954f27f89eb8592235">HardSwish</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor, float alpha=0.2f, float beta=0.5f)</td></tr>
<tr class="memdesc:a68859b30e6833b954f27f89eb8592235"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Hard Swish activation function to a tensor.  <br /></td></tr>
<tr class="separator:a68859b30e6833b954f27f89eb8592235"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a09849b685c23664bf02b0f824006f695" id="r_a09849b685c23664bf02b0f824006f695"><td class="memItemLeft" align="right" valign="top"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a09849b685c23664bf02b0f824006f695">Softmax</a> (const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;tensor)</td></tr>
<tr class="memdesc:a09849b685c23664bf02b0f824006f695"><td class="mdescLeft">&#160;</td><td class="mdescRight">Applies the Softmax activation function to a tensor.  <br /></td></tr>
<tr class="separator:a09849b685c23664bf02b0f824006f695"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Contains data structures and utilities for tensor operations in machine learning workflows. </p>
<p>The <code><a class="el" href="namespacenz_1_1data.html" title="Contains data structures and utilities for tensor operations in machine learning workflows.">nz::data</a></code> namespace provides foundational classes and functions for managing and manipulating tensors in GPU-based computations. It is designed for use in deep learning frameworks and other numerical computing applications.</p>
<p>Key components within this namespace include:</p><ul>
<li><b><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></b>: A class representing multidimensional arrays (tensors) stored in GPU memory.</li>
<li><b>Utilities</b>: Functions and operators for performing mathematical operations, memory management, and activation functions.</li>
</ul>
<p>The namespace is intended to encapsulate all tensor-related functionality to ensure modularity and maintainability in the larger nz project.</p>
<dl class="section note"><dt>Note</dt><dd>The components in this namespace rely on CUDA for GPU-based operations. Ensure that CUDA-compatible hardware and software are properly configured.</dd></dl>
<dl class="section author"><dt>Author</dt><dd>Mgepahmge(<a href="https://github.com/Mgepahmge">https://github.com/Mgepahmge</a>)</dd></dl>
<dl class="section date"><dt>Date</dt><dd>2024/11/29 </dd></dl>
</div><h2 class="groupheader">Function Documentation</h2>
<a id="a463a6acd4af10f53a8f57e30fba2e660" name="a463a6acd4af10f53a8f57e30fba2e660"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a463a6acd4af10f53a8f57e30fba2e660">&#9670;&#160;</a></span>ELU()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::ELU </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">1.0f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Exponential Linear Unit (ELU) activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the ELU activation function element-wise to the given tensor. ELU is a smooth, differentiable activation function defined as: </p><pre class="fragment">ELU(x) = x, if x &gt; 0
ELU(x) = alpha * (exp(x) - 1), if x &lt;= 0
</pre><p>where <code>alpha</code> is a hyperparameter that controls the slope of the negative part of the function. ELU has been shown to perform better than ReLU in certain situations, especially in deep networks.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the ELU activation function will be applied. </td></tr>
    <tr><td class="paramname">alpha</td><td>A parameter that controls the value for negative inputs. It determines the slope of the negative part. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the ELU activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>ExponentialLinearUnit</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise ELU activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(-1.0f);  <span class="comment">// Fill the tensor with negative values</span></div>
<div class="line"><span class="keywordtype">float</span> alpha = 1.0f;</div>
<div class="line">Tensor result = <a class="code hl_function" href="#a463a6acd4af10f53a8f57e30fba2e660">ELU</a>(tensor, alpha);  <span class="comment">// Apply the ELU activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="aclassnz_1_1data_1_1_tensor_html"><div class="ttname"><a href="classnz_1_1data_1_1_tensor.html">nz::data::Tensor</a></div><div class="ttdoc">A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cuh_source.html#l00133">Tensor.cuh:133</a></div></div>
<div class="ttc" id="aclassnz_1_1data_1_1_tensor_html_ac273bf64f5d447bce9b91f08ab312ba1"><div class="ttname"><a href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">nz::data::Tensor::fill</a></div><div class="ttdeci">void fill(const value_type value) const</div><div class="ttdoc">Fills the tensor's data with a specified value.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00919">Tensor.cu:919</a></div></div>
<div class="ttc" id="anamespacenz_1_1data_html_a463a6acd4af10f53a8f57e30fba2e660"><div class="ttname"><a href="#a463a6acd4af10f53a8f57e30fba2e660">nz::data::ELU</a></div><div class="ttdeci">Tensor ELU(const Tensor &amp;tensor, float alpha)</div><div class="ttdoc">Applies the Exponential Linear Unit (ELU) activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00579">Tensor.cu:579</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00579">579</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ab9821ac15d218a07161a211b04db5528" name="ab9821ac15d218a07161a211b04db5528"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab9821ac15d218a07161a211b04db5528">&#9670;&#160;</a></span>HardSigmoid()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::HardSigmoid </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.2f</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.5f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Hard Sigmoid activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Hard Sigmoid activation function element-wise to the given tensor. Hard Sigmoid is a piecewise linear approximation of the Sigmoid function, defined as: </p><pre class="fragment">HardSigmoid(x) = alpha * x + beta, if x is between -2.5 and 2.5
HardSigmoid(x) = 0, if x &lt; -2.5
HardSigmoid(x) = 1, if x &gt; 2.5
</pre><p>where <code>alpha</code> and <code>beta</code> are parameters that control the slope and the shift of the function. Hard Sigmoid is often used in neural networks as a computationally efficient approximation to Sigmoid.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Hard Sigmoid activation function will be applied. </td></tr>
    <tr><td class="paramname">alpha</td><td>A parameter that controls the slope of the activation function. </td></tr>
    <tr><td class="paramname">beta</td><td>A parameter that controls the shift of the activation function. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Hard Sigmoid activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>HardSigmoid</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Hard Sigmoid activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(1.0f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line"><span class="keywordtype">float</span> alpha = 0.2f;</div>
<div class="line"><span class="keywordtype">float</span> beta = 0.5f;</div>
<div class="line">Tensor result = <a class="code hl_function" href="#ab9821ac15d218a07161a211b04db5528">HardSigmoid</a>(tensor, alpha, beta);  <span class="comment">// Apply the Hard Sigmoid activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_ab9821ac15d218a07161a211b04db5528"><div class="ttname"><a href="#ab9821ac15d218a07161a211b04db5528">nz::data::HardSigmoid</a></div><div class="ttdeci">Tensor HardSigmoid(const Tensor &amp;tensor, float alpha, float beta)</div><div class="ttdoc">Applies the Hard Sigmoid activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00624">Tensor.cu:624</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00624">624</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a68859b30e6833b954f27f89eb8592235" name="a68859b30e6833b954f27f89eb8592235"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a68859b30e6833b954f27f89eb8592235">&#9670;&#160;</a></span>HardSwish()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::HardSwish </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.2f</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>beta</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.5f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Hard Swish activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Hard Swish activation function element-wise to the given tensor. Hard Swish is a variant of the Swish activation function, which is defined as: </p><pre class="fragment">HardSwish(x) = x * (ReLU6(x + 3) / 6)
</pre><p>where <code>ReLU6(x)</code> is the Rectified Linear Unit function with a maximum value of 6. The Hard Swish function is computationally more efficient than the original Swish function while maintaining similar properties.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Hard Swish activation function will be applied. </td></tr>
    <tr><td class="paramname">alpha</td><td>A parameter that controls the scaling of the activation. Typically, this is set to 1.0. </td></tr>
    <tr><td class="paramname">beta</td><td>A parameter that controls the shift of the activation. Typically, this is set to 3.0. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Hard Swish activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>HardSwish</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Hard Swish activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(1.0f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line"><span class="keywordtype">float</span> alpha = 1.0f;</div>
<div class="line"><span class="keywordtype">float</span> beta = 3.0f;</div>
<div class="line">Tensor result = <a class="code hl_function" href="#a68859b30e6833b954f27f89eb8592235">HardSwish</a>(tensor, alpha, beta);  <span class="comment">// Apply the Hard Swish activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a68859b30e6833b954f27f89eb8592235"><div class="ttname"><a href="#a68859b30e6833b954f27f89eb8592235">nz::data::HardSwish</a></div><div class="ttdeci">Tensor HardSwish(const Tensor &amp;tensor, float alpha, float beta)</div><div class="ttdoc">Applies the Hard Swish activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00667">Tensor.cu:667</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00667">667</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ae638e4a70745936b4542263a5fef9b9d" name="ae638e4a70745936b4542263a5fef9b9d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae638e4a70745936b4542263a5fef9b9d">&#9670;&#160;</a></span>LeakyReLU()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::LeakyReLU </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">float</td>          <td class="paramname"><span class="paramname"><em>alpha</em></span><span class="paramdefsep"> = </span><span class="paramdefval">0.01f</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Tanh (hyperbolic tangent) activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Tanh activation function element-wise to the given tensor. The Tanh function squashes each element of the tensor to a value between -1 and 1. The Tanh function is often used in neural networks and is similar to the Sigmoid function but has a wider output range.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Tanh activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Tanh activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>Tanh</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Tanh activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(0.5f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a2037c2f13444847ea57d7110d8c48a51">Tanh</a>(tensor);  <span class="comment">// Apply the Tanh activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a2037c2f13444847ea57d7110d8c48a51"><div class="ttname"><a href="#a2037c2f13444847ea57d7110d8c48a51">nz::data::Tanh</a></div><div class="ttdeci">Tensor Tanh(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Tanh (hyperbolic tangent) activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00464">Tensor.cu:464</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00499">499</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ab7386d5f0c567a1bed5a78eab4bf2d6b" name="ab7386d5f0c567a1bed5a78eab4bf2d6b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab7386d5f0c567a1bed5a78eab4bf2d6b">&#9670;&#160;</a></span>operator*() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator* </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Multiplies a tensor by a scalar (element-wise multiplication). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the multiplication operator (<code>*</code>) to multiply each element of the tensor by a scalar value. It performs element-wise multiplication, where every element in the tensor is multiplied by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will be multiplied by the scalar. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value to multiply each element of the tensor by. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise multiplication.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarMul</code>) to perform the element-wise multiplication in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise multiplication.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor * scalar;  <span class="comment">// Multiply each element of the tensor by 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00170">170</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ac65cd4ed45d91da297a90d4381411064" name="ac65cd4ed45d91da297a90d4381411064"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac65cd4ed45d91da297a90d4381411064">&#9670;&#160;</a></span>operator*() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator* </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Multiplies a tensor by a scalar (element-wise multiplication). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the multiplication operator (<code>*</code>) to multiply each element of the tensor by a scalar value. It performs element-wise multiplication, where every element in the tensor is multiplied by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The scalar value to multiply each element of the tensor by. </td></tr>
    <tr><td class="paramname">rhs</td><td>The tensor whose elements will be multiplied by the scalar. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise multiplication.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarMul</code>) to perform the element-wise multiplication in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise multiplication.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = scalar * tensor;  <span class="comment">// Multiply each element of the tensor by 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00132">132</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="ab5c3243e28d8132d9514c8d37d597a7f" name="ab5c3243e28d8132d9514c8d37d597a7f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ab5c3243e28d8132d9514c8d37d597a7f">&#9670;&#160;</a></span>operator+() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator+ </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds a scalar to a tensor (element-wise addition). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the addition operator (<code>+</code>) to add a scalar value to each element of the tensor. It performs element-wise addition, where every element in the tensor is increased by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will be added by the scalar. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value to add to each element of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise addition.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise addition in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise addition.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor + scalar;  <span class="comment">// Add 2.0f to each element of the tensor</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00245">245</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="aa0f75427702f8732bc1833b272baa7f6" name="aa0f75427702f8732bc1833b272baa7f6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aa0f75427702f8732bc1833b272baa7f6">&#9670;&#160;</a></span>operator+() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator+ </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Adds a scalar to a tensor (element-wise addition). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the addition operator (<code>+</code>) to add a scalar value to each element of the tensor. It performs element-wise addition, where every element in the tensor is increased by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The scalar value to add to each element of the tensor. </td></tr>
    <tr><td class="paramname">rhs</td><td>The tensor whose elements will be added by the scalar. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise addition.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise addition in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise addition.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = scalar + tensor;  <span class="comment">// Add 2.0f to each element of the tensor</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00282">282</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a402c3fdfb410d9351a43087834982bbe" name="a402c3fdfb410d9351a43087834982bbe"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a402c3fdfb410d9351a43087834982bbe">&#9670;&#160;</a></span>operator-() <span class="overload">[1/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator- </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Subtracts a scalar from a tensor (element-wise subtraction). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the subtraction operator (<code>-</code>) to subtract a scalar value from each element of the tensor. It performs element-wise subtraction, where every element in the tensor is decreased by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will have the scalar subtracted from them. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value to subtract from each element of the tensor. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise subtraction.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise subtraction in parallel on the GPU. The result is stored in a new tensor, which is returned. The scalar is negated during the operation to achieve subtraction.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise subtraction.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(10.0f);  <span class="comment">// Fill the tensor with 10.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor - scalar;  <span class="comment">// Subtract 2.0f from each element of the tensor</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00320">320</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a00d2b2ae594fa907461c12045de8f470" name="a00d2b2ae594fa907461c12045de8f470"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a00d2b2ae594fa907461c12045de8f470">&#9670;&#160;</a></span>operator-() <span class="overload">[2/2]</span></h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator- </td>
          <td>(</td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Subtracts a tensor from a scalar (element-wise subtraction). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the subtraction operator (<code>-</code>) to subtract each element of the tensor from a scalar value. It performs element-wise subtraction, where every element in the tensor is subtracted from the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The scalar value from which each element of the tensor will be subtracted. </td></tr>
    <tr><td class="paramname">rhs</td><td>The tensor whose elements will be subtracted from the scalar. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise subtraction.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarAdd</code>) to perform the element-wise subtraction in parallel on the GPU. The result is stored in a new tensor, which is returned. The scalar is negated during the operation to achieve subtraction.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise subtraction.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
<li>The scalar is negated to perform subtraction, which results in <code>lhs - rhs</code> for each element in the tensor.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(10.0f);  <span class="comment">// Fill the tensor with 10.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = scalar - tensor;  <span class="comment">// Subtract each element of the tensor from 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00359">359</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a3b044ce5a34b4724830f073efb954743" name="a3b044ce5a34b4724830f073efb954743"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a3b044ce5a34b4724830f073efb954743">&#9670;&#160;</a></span>operator/()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::operator/ </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>lhs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor::value_type</td>          <td class="paramname"><span class="paramname"><em>rhs</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Divides a tensor by a scalar (element-wise division). </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the division operator (<code>/</code>) to divide each element of the tensor by a scalar value. It performs element-wise division, where every element in the tensor is divided by the given scalar.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lhs</td><td>The tensor whose elements will be divided by the scalar. </td></tr>
    <tr><td class="paramname">rhs</td><td>The scalar value by which each element of the tensor will be divided. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the element-wise division.</dd></dl>
<p>This function uses a CUDA kernel (<code>ScalarDiv</code>) to perform the element-wise division in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise division.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
<li>Division by zero should be handled appropriately, and input tensors should be checked to ensure no element is zero.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(10.0f);  <span class="comment">// Fill the tensor with 10.0f</span></div>
<div class="line"><span class="keywordtype">float</span> scalar = 2.0f;</div>
<div class="line">Tensor result = tensor / scalar;  <span class="comment">// Divide each element of the tensor by 2.0f</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00208">208</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a2907370af84a6c5bdc4b72803c9edc68" name="a2907370af84a6c5bdc4b72803c9edc68"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2907370af84a6c5bdc4b72803c9edc68">&#9670;&#160;</a></span>operator&lt;&lt;()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::ostream &amp; nz::data::operator&lt;&lt; </td>
          <td>(</td>
          <td class="paramtype">std::ostream &amp;</td>          <td class="paramname"><span class="paramname"><em>os</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overloads the <code>&lt;&lt;</code> operator to print the tensor's data to an output stream. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the output stream operator (<code>&lt;&lt;</code>) to print the contents of a tensor to the specified output stream (e.g., <code>std::cout</code> or a file stream).</p>
<p>The tensor's data is first copied from GPU memory to host memory for printing, and then the data is printed in a 2D matrix format. Each row of the tensor is printed on a new line, and each element in a row is separated by a space. Each row is enclosed in square brackets.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">os</td><td>The output stream to which the tensor will be printed. </td></tr>
    <tr><td class="paramname">tensor</td><td>The tensor whose contents will be printed. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The output stream (<code>os</code>) after the tensor has been printed, allowing for chaining of operations.</dd></dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator works by accessing the tensor's private data members (e.g., <code>_data</code>) directly.</li>
<li>The tensor's data is assumed to be in a valid state (i.e., properly allocated in GPU memory) before printing.</li>
<li>The function copies the tensor's data from device (GPU) memory to host (CPU) memory using <code>cudaMemcpy</code>, which may introduce performance overhead for large tensors.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(1.0f);  <span class="comment">// Fill the tensor with 1.0f</span></div>
<div class="line">std::cout &lt;&lt; tensor &lt;&lt; std::endl;  <span class="comment">// Prints the tensor to standard output in matrix format</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00034">34</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a40134aba93013e1b0d43c6fd5158d400" name="a40134aba93013e1b0d43c6fd5158d400"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a40134aba93013e1b0d43c6fd5158d400">&#9670;&#160;</a></span>operator&gt;&gt;()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::istream &amp; nz::data::operator&gt;&gt; </td>
          <td>(</td>
          <td class="paramtype">std::istream &amp;</td>          <td class="paramname"><span class="paramname"><em>is</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Overloads the <code>&gt;&gt;</code> operator to read a tensor's data from an input stream. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and provides an overloaded version of the input stream operator (<code>&gt;&gt;</code>) to read the contents of a tensor from the specified input stream (e.g., <code>std::cin</code> or a file stream).</p>
<p>The function reads the tensor's data element by element from the input stream and stores the values in a temporary buffer. Once all the data has been read, it is copied from the host memory back into the tensor's GPU memory using <code>cudaMemcpy</code>.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">is</td><td>The input stream from which the tensor's data will be read. </td></tr>
    <tr><td class="paramname">tensor</td><td>The tensor to which the data will be read. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The input stream (<code>is</code>) after reading the tensor's data, allowing for chaining of operations.</dd></dl>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator works by reading data from the input stream and storing it in a temporary buffer on the host.</li>
<li>The function assumes that the input data matches the size of the tensor. If the data is malformed or does not match, the behavior may be undefined.</li>
<li>After reading, the data is copied from host memory back into the tensor's GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">std::cin &gt;&gt; tensor;  <span class="comment">// Reads the tensor&#39;s data from standard input</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00092">92</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a2cd35db5c2bc0fc68cb0461d2f600089" name="a2cd35db5c2bc0fc68cb0461d2f600089"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2cd35db5c2bc0fc68cb0461d2f600089">&#9670;&#160;</a></span>ReLU()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::ReLU </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Rectified Linear Unit (ReLU) activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the ReLU activation function element-wise to the given tensor. ReLU is a popular activation function in neural networks that replaces all negative values in the tensor with zero, leaving positive values unchanged.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the ReLU activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the ReLU activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>RectifiedLinearUnit</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise ReLU activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(-1.0f);  <span class="comment">// Fill the tensor with negative values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a2cd35db5c2bc0fc68cb0461d2f600089">ReLU</a>(tensor);  <span class="comment">// Apply the ReLU activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a2cd35db5c2bc0fc68cb0461d2f600089"><div class="ttname"><a href="#a2cd35db5c2bc0fc68cb0461d2f600089">nz::data::ReLU</a></div><div class="ttdeci">Tensor ReLU(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Rectified Linear Unit (ReLU) activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00394">Tensor.cu:394</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00394">394</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a370bede45854c04c8222e9971cedc075" name="a370bede45854c04c8222e9971cedc075"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a370bede45854c04c8222e9971cedc075">&#9670;&#160;</a></span>Sigmoid()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::Sigmoid </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Sigmoid activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Sigmoid activation function element-wise to the given tensor. The Sigmoid function squashes each element of the tensor to a value between 0 and 1. The Sigmoid function is commonly used in neural networks, especially for binary classification tasks.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Sigmoid activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Sigmoid activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>Sigmoid</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Sigmoid activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(0.5f);  <span class="comment">// Fill the tensor with values between 0 and 1</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a370bede45854c04c8222e9971cedc075">Sigmoid</a>(tensor);  <span class="comment">// Apply the Sigmoid activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a370bede45854c04c8222e9971cedc075"><div class="ttname"><a href="#a370bede45854c04c8222e9971cedc075">nz::data::Sigmoid</a></div><div class="ttdeci">Tensor Sigmoid(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Sigmoid activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00429">Tensor.cu:429</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00429">429</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a09849b685c23664bf02b0f824006f695" name="a09849b685c23664bf02b0f824006f695"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a09849b685c23664bf02b0f824006f695">&#9670;&#160;</a></span>Softmax()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::Softmax </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Softmax activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Softmax activation function element-wise to the given tensor. The Softmax function converts the tensor into a probability distribution, where each element is transformed into a value between 0 and 1, and the sum of all elements in the tensor equals 1. The Softmax function is commonly used in the output layer of neural networks for multi-class classification tasks.</p>
<p>The Softmax function for each element <code>x_i</code> in the tensor is computed as: </p><pre class="fragment">Softmax(x_i) = exp(x_i) / sum(exp(x_j) for all j)
</pre><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Softmax activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The input tensor with the Softmax function applied element-wise.</dd></dl>
<p>This function uses a CUDA kernel (<code>SummationExp</code>) to compute the sum of the exponentiated values in parallel on the GPU, and then another kernel (<code>Softmax</code>) to apply the Softmax transformation. The result is stored in the original tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator modifies the original tensor by applying the Softmax transformation in-place.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
<li>The Softmax computation is performed in two stages: first by calculating the exponentiated values' sum, then applying the Softmax transformation.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(1.0f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a09849b685c23664bf02b0f824006f695">Softmax</a>(tensor);  <span class="comment">// Apply the Softmax activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a09849b685c23664bf02b0f824006f695"><div class="ttname"><a href="#a09849b685c23664bf02b0f824006f695">nz::data::Softmax</a></div><div class="ttdeci">Tensor Softmax(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Softmax activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00709">Tensor.cu:709</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00709">709</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a271473f387bde9059ad5afa03851184a" name="a271473f387bde9059ad5afa03851184a"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a271473f387bde9059ad5afa03851184a">&#9670;&#160;</a></span>Swish()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::Swish </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Swish activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Swish activation function element-wise to the given tensor. Swish is a smooth, non-monotonic activation function defined as: </p><pre class="fragment">Swish(x) = x / (1 + exp(-x))
</pre><p>Swish has been shown to work well in deep neural networks, offering benefits over ReLU in certain tasks.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Swish activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Swish activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>Swish</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Swish activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(0.5f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a271473f387bde9059ad5afa03851184a">Swish</a>(tensor);  <span class="comment">// Apply the Swish activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
<div class="ttc" id="anamespacenz_1_1data_html_a271473f387bde9059ad5afa03851184a"><div class="ttname"><a href="#a271473f387bde9059ad5afa03851184a">nz::data::Swish</a></div><div class="ttdeci">Tensor Swish(const Tensor &amp;tensor)</div><div class="ttdoc">Applies the Swish activation function to a tensor.</div><div class="ttdef"><b>Definition</b> <a href="_tensor_8cu_source.html#l00537">Tensor.cu:537</a></div></div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00537">537</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
<a id="a2037c2f13444847ea57d7110d8c48a51" name="a2037c2f13444847ea57d7110d8c48a51"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a2037c2f13444847ea57d7110d8c48a51">&#9670;&#160;</a></span>Tanh()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> nz::data::Tanh </td>
          <td>(</td>
          <td class="paramtype">const <a class="el" href="classnz_1_1data_1_1_tensor.html">Tensor</a> &amp;</td>          <td class="paramname"><span class="paramname"><em>tensor</em></span></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Applies the Tanh (hyperbolic tangent) activation function to a tensor. </p>
<p>This function is a friend of the <code><a class="el" href="classnz_1_1data_1_1_tensor.html" title="A class for representing and manipulating multidimensional arrays (tensors) in GPU memory.">Tensor</a></code> class and applies the Tanh activation function element-wise to the given tensor. The Tanh function squashes each element of the tensor to a value between -1 and 1. The Tanh function is often used in neural networks and is similar to the Sigmoid function but has a wider output range.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">tensor</td><td>The tensor to which the Tanh activation function will be applied. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A new tensor containing the result of the Tanh activation applied element-wise to the input tensor.</dd></dl>
<p>This function uses a CUDA kernel (<code>Tanh</code>) to perform the element-wise activation in parallel on the GPU. The result is stored in a new tensor, which is returned.</p>
<dl class="section note"><dt>Note</dt><dd><ul>
<li>This operator does not modify the original tensor. Instead, it returns a new tensor that contains the result of the element-wise Tanh activation.</li>
<li>The function assumes that the tensor is already in a valid state and that the tensor's data is in GPU memory.</li>
</ul>
</dd></dl>
<div class="fragment"><div class="line">```cpp</div>
<div class="line"><a class="code hl_class" href="classnz_1_1data_1_1_tensor.html">Tensor</a> tensor({2, 3});</div>
<div class="line">tensor.<a class="code hl_function" href="classnz_1_1data_1_1_tensor.html#ac273bf64f5d447bce9b91f08ab312ba1">fill</a>(0.5f);  <span class="comment">// Fill the tensor with values</span></div>
<div class="line">Tensor result = <a class="code hl_function" href="#a2037c2f13444847ea57d7110d8c48a51">Tanh</a>(tensor);  <span class="comment">// Apply the Tanh activation</span></div>
<div class="line">std::cout &lt;&lt; result &lt;&lt; std::endl;  <span class="comment">// Print the resulting tensor</span></div>
<div class="line">```</div>
</div><!-- fragment --> 
<p class="definition">Definition at line <a class="el" href="_tensor_8cu_source.html#l00464">464</a> of file <a class="el" href="_tensor_8cu_source.html">Tensor.cu</a>.</p>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.12.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
